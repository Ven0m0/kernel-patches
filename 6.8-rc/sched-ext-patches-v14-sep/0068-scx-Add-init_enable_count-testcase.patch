From 36080fb898081a39c70a63dbcbfa6be2eede303e Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Fri, 15 Dec 2023 10:53:46 -0600
Subject: [PATCH 068/130] scx: Add init_enable_count testcase

We expect to have some sched_ext_ops callbacks be called differently
depending on the scheduler, and the tasks running on the system.  Let's
add a testcase that verifies that the init_task(), exit_task(),
enable(), and disable() callbacks are all invoked correctly and as
expected.

Signed-off-by: David Vernet <void@manifault.com>
---
 tools/testing/selftests/scx/.gitignore        |   1 +
 tools/testing/selftests/scx/Makefile          |  27 +++--
 .../selftests/scx/init_enable_count.bpf.c     |  57 +++++++++
 .../testing/selftests/scx/init_enable_count.c | 114 ++++++++++++++++++
 .../scx/select_cpu_dfl_nodispatch.bpf.c       |   6 +-
 5 files changed, 189 insertions(+), 16 deletions(-)
 create mode 100644 tools/testing/selftests/scx/init_enable_count.bpf.c
 create mode 100644 tools/testing/selftests/scx/init_enable_count.c

diff --git a/tools/testing/selftests/scx/.gitignore b/tools/testing/selftests/scx/.gitignore
index ab806b18d..991721c50 100644
--- a/tools/testing/selftests/scx/.gitignore
+++ b/tools/testing/selftests/scx/.gitignore
@@ -2,6 +2,7 @@ dsp_fallbackdsq_fail
 dsp_localdsq_fail
 enq_last_no_enq_fails
 enqueue_select_cpu_fails
+init_enable_count
 minimal
 select_cpu_dfl
 select_cpu_dfl_nodispatch
diff --git a/tools/testing/selftests/scx/Makefile b/tools/testing/selftests/scx/Makefile
index e993335a2..ae713d614 100644
--- a/tools/testing/selftests/scx/Makefile
+++ b/tools/testing/selftests/scx/Makefile
@@ -96,8 +96,8 @@ BPF_CFLAGS = -g -D__TARGET_ARCH_$(SRCARCH)					\
 	     -O2 -mcpu=v3
 
 # sort removes libbpf duplicates when not cross-building
-MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(HOST_BUILD_DIR)/libbpf			\
-	       $(HOST_BUILD_DIR)/bpftool $(HOST_BUILD_DIR)/resolve_btfids	\
+MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(OBJ_DIR)/libbpf				\
+	       $(OBJ_DIR)/bpftool $(OBJ_DIR)/resolve_btfids			\
 	       $(INCLUDE_DIR) $(SCXOBJ_DIR))
 
 $(MAKE_DIRS):
@@ -112,14 +112,14 @@ $(BPFOBJ): $(wildcard $(BPFDIR)/*.[ch] $(BPFDIR)/Makefile)			\
 		    DESTDIR=$(OUTPUT_DIR) prefix= all install_headers
 
 $(DEFAULT_BPFTOOL): $(wildcard $(BPFTOOLDIR)/*.[ch] $(BPFTOOLDIR)/Makefile)	\
-		    $(LIBBPF_OUTPUT) | $(HOST_BUILD_DIR)/bpftool
+		    $(LIBBPF_OUTPUT) | $(OBJ_DIR)/bpftool
 	$(Q)$(MAKE) $(submake_extras)  -C $(BPFTOOLDIR)				\
 		    ARCH= CROSS_COMPILE= CC=$(HOSTCC) LD=$(HOSTLD)		\
 		    EXTRA_CFLAGS='-g -O0'					\
-		    OUTPUT=$(HOST_BUILD_DIR)/bpftool/				\
-		    LIBBPF_OUTPUT=$(HOST_BUILD_DIR)/libbpf/			\
-		    LIBBPF_DESTDIR=$(HOST_OUTPUT_DIR)/				\
-		    prefix= DESTDIR=$(HOST_OUTPUT_DIR)/ install-bin
+		    OUTPUT=$(OBJ_DIR)/bpftool/					\
+		    LIBBPF_OUTPUT=$(OBJ_DIR)/libbpf/				\
+		    LIBBPF_DESTDIR=$(OUTPUT_DIR)/				\
+		    prefix= DESTDIR=$(OUTPUT_DIR)/ install-bin
 
 $(INCLUDE_DIR)/vmlinux.h: $(VMLINUX_BTF) $(BPFTOOL) | $(INCLUDE_DIR)
 ifeq ($(VMLINUX_H),)
@@ -148,16 +148,17 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
 # C schedulers #
 ################
 c-sched-targets :=			\
+	dsp_fallbackdsq_fail		\
+	dsp_localdsq_fail		\
+	enq_last_no_enq_fails		\
+	enqueue_select_cpu_fails	\
+	init_enable_count		\
 	minimal				\
 	select_cpu_dfl			\
 	select_cpu_dfl_nodispatch	\
 	select_cpu_dispatch		\
-	select_cpu_dispatch_dbl_dsp	\
 	select_cpu_dispatch_bad_dsq	\
-	enqueue_select_cpu_fails	\
-	enq_last_no_enq_fails		\
-	dsp_localdsq_fail		\
-	dsp_fallbackdsq_fail
+	select_cpu_dispatch_dbl_dsp
 
 $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
 	$(eval sched=$(notdir $@))
@@ -167,7 +168,7 @@ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
 TEST_GEN_PROGS := $(c-sched-targets)
 
 override define CLEAN
-	rm -rf $(OUTPUT_DIR) $(HOST_OUTPUT_DIR)
+	rm -rf $(OUTPUT_DIR)
 	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
 	rm -f $(TEST_GEN_PROGS)
 endef
diff --git a/tools/testing/selftests/scx/init_enable_count.bpf.c b/tools/testing/selftests/scx/init_enable_count.bpf.c
new file mode 100644
index 000000000..8ad8fdf4a
--- /dev/null
+++ b/tools/testing/selftests/scx/init_enable_count.bpf.c
@@ -0,0 +1,57 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that verifies that we do proper counting of init, enable, etc
+ * callbacks.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+u64 init_task_cnt, exit_task_cnt, enable_cnt, disable_cnt;
+volatile const bool switch_all;
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(cnt_init_task, struct task_struct *p,
+			     struct scx_init_task_args *args)
+{
+	__sync_fetch_and_add(&init_task_cnt, 1);
+
+	return 0;
+}
+
+void BPF_STRUCT_OPS(cnt_exit_task, struct task_struct *p)
+{
+	__sync_fetch_and_add(&exit_task_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(cnt_enable, struct task_struct *p)
+{
+	__sync_fetch_and_add(&enable_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(cnt_disable, struct task_struct *p)
+{
+	__sync_fetch_and_add(&disable_cnt, 1);
+}
+
+s32 BPF_STRUCT_OPS(cnt_init)
+{
+	if (switch_all)
+		scx_bpf_switch_all();
+
+	return 0;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops init_enable_count_ops = {
+	.init_task	= cnt_init_task,
+	.exit_task	= cnt_exit_task,
+	.enable		= cnt_enable,
+	.disable	= cnt_disable,
+	.init		= cnt_init,
+	.name		= "init_enable_count",
+};
diff --git a/tools/testing/selftests/scx/init_enable_count.c b/tools/testing/selftests/scx/init_enable_count.c
new file mode 100644
index 000000000..413bf0656
--- /dev/null
+++ b/tools/testing/selftests/scx/init_enable_count.c
@@ -0,0 +1,114 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <stdio.h>
+#include <unistd.h>
+#include <sched.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include "scx_test.h"
+#include "init_enable_count.bpf.skel.h"
+
+#define SCHED_EXT 7
+
+static struct init_enable_count *
+open_load_prog(bool global)
+{
+	struct init_enable_count *skel;
+
+	skel = init_enable_count__open();
+	SCX_BUG_ON(!skel, "Failed to open skel");
+
+	if (global)
+		skel->rodata->switch_all = global;
+
+	SCX_BUG_ON(init_enable_count__load(skel), "Failed to load skel");
+
+	return skel;
+}
+
+static void run_test(bool global)
+{
+	struct init_enable_count *skel;
+	struct bpf_link *link;
+	const u32 num_children = 5;
+	int ret, i, status;
+	struct sched_param param = {};
+	pid_t pids[num_children];
+
+	skel = open_load_prog(global);
+	link = bpf_map__attach_struct_ops(skel->maps.init_enable_count_ops);
+	SCX_BUG_ON(!link, "Failed to attach struct_ops");
+
+	/* SCHED_EXT children */
+	for (i = 0; i < num_children; i++) {
+		pids[i] = fork();
+		SCX_BUG_ON(pids[i] < 0, "Failed to fork child");
+
+		if (pids[i] == 0) {
+			ret = sched_setscheduler(0, SCHED_EXT, &param);
+			SCX_BUG_ON(ret, "Failed to set sched to sched_ext");
+
+			/*
+			 * Reset to SCHED_OTHER for half of them. Counts for
+			 * everything should still be the same regardless, as
+			 * ops.disable() is invoked even if a task is still on
+			 * SCHED_EXT before it exits.
+			 */
+			if (i % 2 == 0) {
+				ret = sched_setscheduler(0, SCHED_OTHER, &param);
+				SCX_BUG_ON(ret, "Failed to reset sched to normal");
+			}
+			exit(0);
+		}
+	}
+	for (i = 0; i < num_children; i++) {
+		SCX_BUG_ON(waitpid(pids[i], &status, 0) != pids[i],
+			   "Failed to wait for SCX child");
+		SCX_BUG_ON(status != 0, "SCX child %d exited with status %d",
+			   i, status);
+	}
+
+	/* SCHED_OTHER children */
+	for (i = 0; i < num_children; i++) {
+		pids[i] = fork();
+		if (pids[i] == 0)
+			exit(0);
+	}
+	for (i = 0; i < num_children; i++) {
+		SCX_BUG_ON(waitpid(pids[i], &status, 0) != pids[i],
+			   "Failed to wait for normal child");
+		SCX_BUG_ON(status != 0,
+			   "Normal child %d exited with status %d", i, status);
+	}
+
+	sleep(1);
+
+	SCX_GE(skel->bss->init_task_cnt, 2 * num_children);
+	SCX_GE(skel->bss->exit_task_cnt, 2 * num_children);
+
+	if (global) {
+		SCX_GE(skel->bss->enable_cnt, 2 * num_children);
+		SCX_GE(skel->bss->disable_cnt, 2 * num_children);
+	} else {
+		SCX_EQ(skel->bss->enable_cnt, num_children);
+		SCX_EQ(skel->bss->disable_cnt, num_children);
+	}
+
+	bpf_link__destroy(link);
+	init_enable_count__destroy(skel);
+}
+
+int main(int argc, char **argv)
+{
+	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+
+	run_test(true);
+	run_test(false);
+
+	return 0;
+}
diff --git a/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c
index 9d026e0cb..636ea1de1 100644
--- a/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c
+++ b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c
@@ -70,8 +70,8 @@ void BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_enqueue, struct task_struct *p,
 	scx_bpf_dispatch(p, dsq_id, SCX_SLICE_DFL, enq_flags);
 }
 
-s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_prep_enable,
-		   struct task_struct *p, struct scx_enable_args *args)
+s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_init_task,
+		   struct task_struct *p, struct scx_init_task_args *args)
 {
 	if (bpf_task_storage_get(&task_ctx_stor, p, 0,
 				 BPF_LOCAL_STORAGE_GET_F_CREATE))
@@ -91,7 +91,7 @@ SEC(".struct_ops.link")
 struct sched_ext_ops select_cpu_dfl_nodispatch_ops = {
 	.select_cpu		= select_cpu_dfl_nodispatch_select_cpu,
 	.enqueue		= select_cpu_dfl_nodispatch_enqueue,
-	.prep_enable		= select_cpu_dfl_nodispatch_prep_enable,
+	.init_task		= select_cpu_dfl_nodispatch_init_task,
 	.init			= select_cpu_dfl_nodispatch_init,
 	.name			= "select_cpu_dfl_nodispatch",
 };
-- 
2.43.0.232.ge79552d197

