From 79653de42d80a02c250598286cd969ca4bf57fe6 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 5 Feb 2024 11:55:05 +0100
Subject: [PATCH] amd-6.8: merge changes from dev tree

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 .../admin-guide/kernel-parameters.txt         |   5 +
 Documentation/admin-guide/pm/amd-pstate.rst   |  70 ++-
 arch/x86/Kconfig                              |   5 +-
 arch/x86/kernel/acpi/cppc.c                   |   2 +-
 drivers/acpi/cppc_acpi.c                      |  17 +-
 drivers/acpi/processor_driver.c               |   6 +
 drivers/cpufreq/amd-pstate-ut.c               |  16 -
 drivers/cpufreq/amd-pstate.c                  | 445 ++++++++++++++++--
 include/acpi/cppc_acpi.h                      |   5 +
 include/linux/amd-pstate.h                    |  18 +-
 include/linux/cpufreq.h                       |   1 +
 tools/testing/selftests/scx/Makefile.rej      | 307 ++++++++++++
 .../scx/ddsp_bogus_dsq_fail.bpf.c.rej         |  65 +++
 .../selftests/scx/ddsp_bogus_dsq_fail.c.rej   |  11 +
 .../selftests/scx/ddsp_vtimelocal_fail.c.rej  |  11 +
 .../selftests/scx/enq_last_no_enq_fails.c.rej |  72 +++
 .../scx/enq_select_cpu_fails.bpf.c.rej        |  52 ++
 .../selftests/scx/init_enable_count.bpf.c.rej |  22 +
 .../selftests/scx/init_enable_count.c.rej     | 150 ++++++
 tools/testing/selftests/scx/maximal.bpf.c.rej |  10 +
 tools/testing/selftests/scx/maximal.c.rej     |  10 +
 .../selftests/scx/maybe_null.bpf.c.rej        |  29 ++
 tools/testing/selftests/scx/maybe_null.c.rej  |  51 ++
 .../selftests/scx/maybe_null_fail.bpf.c.rej   |  27 ++
 tools/testing/selftests/scx/minimal.c.rej     |  85 ++++
 tools/testing/selftests/scx/runner.c.rej      |  68 +++
 tools/testing/selftests/scx/scx_test.h.rej    | 140 ++++++
 .../selftests/scx/select_cpu_dfl.bpf.c.rej    |  25 +
 .../selftests/scx/select_cpu_dfl.c.rej        |  77 +++
 .../scx/select_cpu_dfl_nodispatch.bpf.c.rej   |  22 +
 .../scx/select_cpu_dfl_nodispatch.c.rej       |  81 ++++
 .../selftests/scx/select_cpu_dispatch.c.rej   |  77 +++
 .../scx/select_cpu_dispatch_bad_dsq.c.rej     | 100 ++++
 .../scx/select_cpu_dispatch_dbl_dsp.c.rej     | 100 ++++
 .../selftests/scx/select_cpu_vtime.c.rej      |  70 +++
 35 files changed, 2176 insertions(+), 76 deletions(-)
 create mode 100644 tools/testing/selftests/scx/Makefile.rej
 create mode 100644 tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c.rej
 create mode 100644 tools/testing/selftests/scx/ddsp_vtimelocal_fail.c.rej
 create mode 100644 tools/testing/selftests/scx/enq_last_no_enq_fails.c.rej
 create mode 100644 tools/testing/selftests/scx/enq_select_cpu_fails.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/init_enable_count.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/init_enable_count.c.rej
 create mode 100644 tools/testing/selftests/scx/maximal.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/maximal.c.rej
 create mode 100644 tools/testing/selftests/scx/maybe_null.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/maybe_null.c.rej
 create mode 100644 tools/testing/selftests/scx/maybe_null_fail.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/minimal.c.rej
 create mode 100644 tools/testing/selftests/scx/runner.c.rej
 create mode 100644 tools/testing/selftests/scx/scx_test.h.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dfl.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dfl.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dispatch.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c.rej
 create mode 100644 tools/testing/selftests/scx/select_cpu_vtime.c.rej

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 31b3a2568..522530432 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -374,6 +374,11 @@
 			  selects a performance level in this range and appropriate
 			  to the current workload.
 
+	amd_prefcore=
+			[X86]
+			disable
+			  Disable amd-pstate preferred core.
+
 	amijoy.map=	[HW,JOY] Amiga joystick support
 			Map of devices attached to JOY0DAT and JOY1DAT
 			Format: <a>,<b>
diff --git a/Documentation/admin-guide/pm/amd-pstate.rst b/Documentation/admin-guide/pm/amd-pstate.rst
index 9eb26014d..4fc000352 100644
--- a/Documentation/admin-guide/pm/amd-pstate.rst
+++ b/Documentation/admin-guide/pm/amd-pstate.rst
@@ -300,8 +300,8 @@ platforms. The AMD P-States mechanism is the more performance and energy
 efficiency frequency management method on AMD processors.
 
 
-AMD Pstate Driver Operation Modes
-=================================
+``amd-pstate`` Driver Operation Modes
+======================================
 
 ``amd_pstate`` CPPC has 3 operation modes: autonomous (active) mode,
 non-autonomous (passive) mode and guided autonomous (guided) mode.
@@ -353,6 +353,48 @@ is activated.  In this mode, driver requests minimum and maximum performance
 level and the platform autonomously selects a performance level in this range
 and appropriate to the current workload.
 
+``amd-pstate`` Preferred Core
+=================================
+
+The core frequency is subjected to the process variation in semiconductors.
+Not all cores are able to reach the maximum frequency respecting the
+infrastructure limits. Consequently, AMD has redefined the concept of
+maximum frequency of a part. This means that a fraction of cores can reach
+maximum frequency. To find the best process scheduling policy for a given
+scenario, OS needs to know the core ordering informed by the platform through
+highest performance capability register of the CPPC interface.
+
+``amd-pstate`` preferred core enables the scheduler to prefer scheduling on
+cores that can achieve a higher frequency with lower voltage. The preferred
+core rankings can dynamically change based on the workload, platform conditions,
+thermals and ageing.
+
+The priority metric will be initialized by the ``amd-pstate`` driver. The ``amd-pstate``
+driver will also determine whether or not ``amd-pstate`` preferred core is
+supported by the platform.
+
+``amd-pstate`` driver will provide an initial core ordering when the system boots.
+The platform uses the CPPC interfaces to communicate the core ranking to the
+operating system and scheduler to make sure that OS is choosing the cores
+with highest performance firstly for scheduling the process. When ``amd-pstate``
+driver receives a message with the highest performance change, it will
+update the core ranking and set the cpu's priority.
+
+``amd-pstate`` Preferred Core Switch
+=====================================
+Kernel Parameters
+-----------------
+
+``amd-pstate`` peferred core`` has two states: enable and disable.
+Enable/disable states can be chosen by different kernel parameters.
+Default enable ``amd-pstate`` preferred core.
+
+``amd_prefcore=disable``
+
+For systems that support ``amd-pstate`` preferred core, the core rankings will
+always be advertised by the platform. But OS can choose to ignore that via the
+kernel parameter ``amd_prefcore=disable``.
+
 User Space Interface in ``sysfs`` - General
 ===========================================
 
@@ -385,6 +427,30 @@ control its functionality at the system level.  They are located in the
         to the operation mode represented by that string - or to be
         unregistered in the "disable" case.
 
+``prefcore``
+	Preferred core state of the driver: "enabled" or "disabled".
+
+	"enabled"
+		Enable the ``amd-pstate`` preferred core.
+
+	"disabled"
+		Disable the ``amd-pstate`` preferred core
+
+
+        This attribute is read-only to check the state of preferred core set
+        by the kernel parameter.
+
+``cpb_boost``
+        Specifies whether core performance boost is requested to be enabled or disabled
+        If core performance boost is disabled while a core is in a boosted P-state, the
+        core automatically transitions to the highest performance non-boosted P-state.
+        AMD Core Performance Boost(CPB) is controlled by this new attribute file which
+        allow user to change all cores frequency boosting state. It supports both
+        ``active``, ``passive`` and ``guided`` mode control with below value write to it.
+
+        "0" Disable Core performance Boosting
+        "1" Enable  Core performance Boosting
+
 ``cpupower`` tool support for ``amd-pstate``
 ===============================================
 
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 5edec175b..29d110285 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1054,8 +1054,9 @@ config SCHED_MC
 
 config SCHED_MC_PRIO
 	bool "CPU core priorities scheduler support"
-	depends on SCHED_MC && CPU_SUP_INTEL
-	select X86_INTEL_PSTATE
+	depends on SCHED_MC
+	select X86_INTEL_PSTATE if CPU_SUP_INTEL
+	select X86_AMD_PSTATE if CPU_SUP_AMD && ACPI
 	select CPU_FREQ
 	default y
 	help
diff --git a/arch/x86/kernel/acpi/cppc.c b/arch/x86/kernel/acpi/cppc.c
index 8d8752b44..ff8f25fac 100644
--- a/arch/x86/kernel/acpi/cppc.c
+++ b/arch/x86/kernel/acpi/cppc.c
@@ -20,7 +20,7 @@ bool cpc_supported_by_cpu(void)
 		    (boot_cpu_data.x86_model >= 0x20 && boot_cpu_data.x86_model <= 0x2f)))
 			return true;
 		else if (boot_cpu_data.x86 == 0x17 &&
-			 boot_cpu_data.x86_model >= 0x70 && boot_cpu_data.x86_model <= 0x7f)
+			 boot_cpu_data.x86_model >= 0x30 && boot_cpu_data.x86_model <= 0x7f)
 			return true;
 		return boot_cpu_has(X86_FEATURE_CPPC);
 	}
diff --git a/drivers/acpi/cppc_acpi.c b/drivers/acpi/cppc_acpi.c
index d155a86a8..e23a84f4a 100644
--- a/drivers/acpi/cppc_acpi.c
+++ b/drivers/acpi/cppc_acpi.c
@@ -679,8 +679,10 @@ int acpi_cppc_processor_probe(struct acpi_processor *pr)
 
 	if (!osc_sb_cppc2_support_acked) {
 		pr_debug("CPPC v2 _OSC not acked\n");
-		if (!cpc_supported_by_cpu())
+		if (!cpc_supported_by_cpu()) {
+			pr_debug("CPPC is not supported by the CPU\n");
 			return -ENODEV;
+		}
 	}
 
 	/* Parse the ACPI _CPC table for this CPU. */
@@ -1157,6 +1159,19 @@ int cppc_get_nominal_perf(int cpunum, u64 *nominal_perf)
 	return cppc_get_perf(cpunum, NOMINAL_PERF, nominal_perf);
 }
 
+/**
+ * cppc_get_highest_perf - Get the highest performance register value.
+ * @cpunum: CPU from which to get highest performance.
+ * @highest_perf: Return address.
+ *
+ * Return: 0 for success, -EIO otherwise.
+ */
+int cppc_get_highest_perf(int cpunum, u64 *highest_perf)
+{
+	return cppc_get_perf(cpunum, HIGHEST_PERF, highest_perf);
+}
+EXPORT_SYMBOL_GPL(cppc_get_highest_perf);
+
 /**
  * cppc_get_epp_perf - Get the epp register value.
  * @cpunum: CPU from which to get epp preference value.
diff --git a/drivers/acpi/processor_driver.c b/drivers/acpi/processor_driver.c
index 4bd16b3f0..67db60eda 100644
--- a/drivers/acpi/processor_driver.c
+++ b/drivers/acpi/processor_driver.c
@@ -27,6 +27,7 @@
 #define ACPI_PROCESSOR_NOTIFY_PERFORMANCE 0x80
 #define ACPI_PROCESSOR_NOTIFY_POWER	0x81
 #define ACPI_PROCESSOR_NOTIFY_THROTTLING	0x82
+#define ACPI_PROCESSOR_NOTIFY_HIGEST_PERF_CHANGED	0x85
 
 MODULE_AUTHOR("Paul Diefenbaugh");
 MODULE_DESCRIPTION("ACPI Processor Driver");
@@ -83,6 +84,11 @@ static void acpi_processor_notify(acpi_handle handle, u32 event, void *data)
 		acpi_bus_generate_netlink_event(device->pnp.device_class,
 						  dev_name(&device->dev), event, 0);
 		break;
+	case ACPI_PROCESSOR_NOTIFY_HIGEST_PERF_CHANGED:
+		cpufreq_update_limits(pr->id);
+		acpi_bus_generate_netlink_event(device->pnp.device_class,
+						  dev_name(&device->dev), event, 0);
+		break;
 	default:
 		acpi_handle_debug(handle, "Unsupported event [0x%x]\n", event);
 		break;
diff --git a/drivers/cpufreq/amd-pstate-ut.c b/drivers/cpufreq/amd-pstate-ut.c
index f04ae67dd..e59b4aa52 100644
--- a/drivers/cpufreq/amd-pstate-ut.c
+++ b/drivers/cpufreq/amd-pstate-ut.c
@@ -226,22 +226,6 @@ static void amd_pstate_ut_check_freq(u32 index)
 			goto skip_test;
 		}
 
-		if (cpudata->boost_supported) {
-			if ((policy->max == cpudata->max_freq) ||
-					(policy->max == cpudata->nominal_freq))
-				amd_pstate_ut_cases[index].result = AMD_PSTATE_UT_RESULT_PASS;
-			else {
-				amd_pstate_ut_cases[index].result = AMD_PSTATE_UT_RESULT_FAIL;
-				pr_err("%s cpu%d policy_max=%d should be equal cpu_max=%d or cpu_nominal=%d !\n",
-					__func__, cpu, policy->max, cpudata->max_freq,
-					cpudata->nominal_freq);
-				goto skip_test;
-			}
-		} else {
-			amd_pstate_ut_cases[index].result = AMD_PSTATE_UT_RESULT_FAIL;
-			pr_err("%s cpu%d must support boost!\n", __func__, cpu);
-			goto skip_test;
-		}
 		cpufreq_cpu_put(policy);
 	}
 
diff --git a/drivers/cpufreq/amd-pstate.c b/drivers/cpufreq/amd-pstate.c
index 1791d37fb..eb9f5d9b2 100644
--- a/drivers/cpufreq/amd-pstate.c
+++ b/drivers/cpufreq/amd-pstate.c
@@ -37,6 +37,7 @@
 #include <linux/uaccess.h>
 #include <linux/static_call.h>
 #include <linux/amd-pstate.h>
+#include <linux/topology.h>
 
 #include <acpi/processor.h>
 #include <acpi/cppc_acpi.h>
@@ -49,6 +50,7 @@
 
 #define AMD_PSTATE_TRANSITION_LATENCY	20000
 #define AMD_PSTATE_TRANSITION_DELAY	1000
+#define AMD_PSTATE_PREFCORE_THRESHOLD	166
 
 /*
  * TODO: We need more time to fine tune processors with shared memory solution
@@ -64,6 +66,21 @@ static struct cpufreq_driver amd_pstate_driver;
 static struct cpufreq_driver amd_pstate_epp_driver;
 static int cppc_state = AMD_PSTATE_UNDEFINED;
 static bool cppc_enabled;
+static bool amd_pstate_prefcore = true;
+static struct quirk_entry *quirks;
+
+/**
+ * struct global_params - Global parameters, mostly tunable via sysfs.
+ * @cpb_boost:		Whether or not to use boost CPU P-states.
+ * @cpb_supported:	Whether or not CPU boost P-states are available
+ *			based on the MSR_K7_HWCR bit[25] state
+ */
+struct global_params {
+	bool cpb_boost;
+	bool cpb_supported;
+};
+
+static struct global_params global;
 
 /*
  * AMD Energy Preference Performance (EPP)
@@ -108,6 +125,33 @@ static unsigned int epp_values[] = {
 
 typedef int (*cppc_mode_transition_fn)(int);
 
+static struct quirk_entry quirk_amd_7k62 = {
+	.nominal_freq = 2600,
+	.lowest_freq = 550,
+};
+
+static int __init dmi_matched(const struct dmi_system_id *dmi)
+{
+	quirks = dmi->driver_data;
+	pr_info("hardware type %s found\n", dmi->ident);
+
+	return 1;
+}
+
+static const struct dmi_system_id amd_pstate_quirks_table[] __initconst = {
+	{
+		.callback = dmi_matched,
+		.ident = "AMD EPYC 7K62",
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VERSION, "5.14"),
+			DMI_MATCH(DMI_BIOS_RELEASE, "12/12/2019"),
+		},
+		.driver_data = &quirk_amd_7k62,
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(dmi, amd_pstate_quirks_table);
+
 static inline int get_mode_idx_from_str(const char *str, size_t size)
 {
 	int i;
@@ -297,13 +341,14 @@ static int pstate_init_perf(struct amd_cpudata *cpudata)
 	if (ret)
 		return ret;
 
-	/*
-	 * TODO: Introduce AMD specific power feature.
-	 *
-	 * CPPC entry doesn't indicate the highest performance in some ASICs.
+	/* For platforms that do not support the preferred core feature, the
+	 * highest_pef may be configured with 166 or 255, to avoid max frequency
+	 * calculated wrongly. we take the AMD_CPPC_HIGHEST_PERF(cap1) value as
+	 * the default max perf.
 	 */
-	highest_perf = amd_get_highest_perf();
-	if (highest_perf > AMD_CPPC_HIGHEST_PERF(cap1))
+	if (cpudata->hw_prefcore)
+		highest_perf = AMD_PSTATE_PREFCORE_THRESHOLD;
+	else
 		highest_perf = AMD_CPPC_HIGHEST_PERF(cap1);
 
 	WRITE_ONCE(cpudata->highest_perf, highest_perf);
@@ -311,6 +356,7 @@ static int pstate_init_perf(struct amd_cpudata *cpudata)
 	WRITE_ONCE(cpudata->nominal_perf, AMD_CPPC_NOMINAL_PERF(cap1));
 	WRITE_ONCE(cpudata->lowest_nonlinear_perf, AMD_CPPC_LOWNONLIN_PERF(cap1));
 	WRITE_ONCE(cpudata->lowest_perf, AMD_CPPC_LOWEST_PERF(cap1));
+	WRITE_ONCE(cpudata->prefcore_ranking, AMD_CPPC_HIGHEST_PERF(cap1));
 	WRITE_ONCE(cpudata->min_limit_perf, AMD_CPPC_LOWEST_PERF(cap1));
 	return 0;
 }
@@ -324,8 +370,9 @@ static int cppc_init_perf(struct amd_cpudata *cpudata)
 	if (ret)
 		return ret;
 
-	highest_perf = amd_get_highest_perf();
-	if (highest_perf > cppc_perf.highest_perf)
+	if (cpudata->hw_prefcore)
+		highest_perf = AMD_PSTATE_PREFCORE_THRESHOLD;
+	else
 		highest_perf = cppc_perf.highest_perf;
 
 	WRITE_ONCE(cpudata->highest_perf, highest_perf);
@@ -334,6 +381,7 @@ static int cppc_init_perf(struct amd_cpudata *cpudata)
 	WRITE_ONCE(cpudata->lowest_nonlinear_perf,
 		   cppc_perf.lowest_nonlinear_perf);
 	WRITE_ONCE(cpudata->lowest_perf, cppc_perf.lowest_perf);
+	WRITE_ONCE(cpudata->prefcore_ranking, cppc_perf.highest_perf);
 	WRITE_ONCE(cpudata->min_limit_perf, cppc_perf.lowest_perf);
 
 	if (cppc_state == AMD_PSTATE_ACTIVE)
@@ -431,6 +479,7 @@ static void amd_pstate_update(struct amd_cpudata *cpudata, u32 min_perf,
 			      u32 des_perf, u32 max_perf, bool fast_switch, int gov_flags)
 {
 	u64 prev = READ_ONCE(cpudata->cppc_req_cached);
+	u32 nominal_perf = READ_ONCE(cpudata->nominal_perf);
 	u64 value = prev;
 
 	min_perf = clamp_t(unsigned long, min_perf, cpudata->min_limit_perf,
@@ -450,6 +499,10 @@ static void amd_pstate_update(struct amd_cpudata *cpudata, u32 min_perf,
 	value &= ~AMD_CPPC_DES_PERF(~0L);
 	value |= AMD_CPPC_DES_PERF(des_perf);
 
+	/* limit the max perf when core performance boost feature is disabled */
+	if (!global.cpb_boost)
+		max_perf = min_t(unsigned long, nominal_perf, max_perf);
+
 	value &= ~AMD_CPPC_MAX_PERF(~0L);
 	value |= AMD_CPPC_MAX_PERF(max_perf);
 
@@ -593,13 +646,19 @@ static void amd_pstate_adjust_perf(unsigned int cpu,
 static int amd_get_min_freq(struct amd_cpudata *cpudata)
 {
 	struct cppc_perf_caps cppc_perf;
+	u32 lowest_freq;
 
 	int ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);
 	if (ret)
 		return ret;
 
+	if (quirks && quirks->lowest_freq)
+		lowest_freq = quirks->lowest_freq;
+	else
+		lowest_freq = cppc_perf.lowest_freq;
+
 	/* Switch to khz */
-	return cppc_perf.lowest_freq * 1000;
+	return lowest_freq * 1000;
 }
 
 static int amd_get_max_freq(struct amd_cpudata *cpudata)
@@ -612,10 +671,14 @@ static int amd_get_max_freq(struct amd_cpudata *cpudata)
 	if (ret)
 		return ret;
 
-	nominal_freq = cppc_perf.nominal_freq;
+	nominal_freq = READ_ONCE(cpudata->nominal_freq);
 	nominal_perf = READ_ONCE(cpudata->nominal_perf);
 	max_perf = READ_ONCE(cpudata->highest_perf);
 
+	/* when boost is off, the highest perf will be limited to nominal_perf */
+	if (!global.cpb_boost)
+		max_perf = nominal_perf;
+
 	boost_ratio = div_u64(max_perf << SCHED_CAPACITY_SHIFT,
 			      nominal_perf);
 
@@ -628,13 +691,18 @@ static int amd_get_max_freq(struct amd_cpudata *cpudata)
 static int amd_get_nominal_freq(struct amd_cpudata *cpudata)
 {
 	struct cppc_perf_caps cppc_perf;
+	u32 nominal_freq;
 
 	int ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);
 	if (ret)
 		return ret;
 
-	/* Switch to khz */
-	return cppc_perf.nominal_freq * 1000;
+	if (quirks && quirks->nominal_freq)
+		nominal_freq = quirks->nominal_freq;
+	else
+		nominal_freq = cppc_perf.nominal_freq;
+
+	return nominal_freq;
 }
 
 static int amd_get_lowest_nonlinear_freq(struct amd_cpudata *cpudata)
@@ -648,7 +716,7 @@ static int amd_get_lowest_nonlinear_freq(struct amd_cpudata *cpudata)
 	if (ret)
 		return ret;
 
-	nominal_freq = cppc_perf.nominal_freq;
+	nominal_freq = READ_ONCE(cpudata->nominal_freq);
 	nominal_perf = READ_ONCE(cpudata->nominal_perf);
 
 	lowest_nonlinear_perf = cppc_perf.lowest_nonlinear_perf;
@@ -662,48 +730,164 @@ static int amd_get_lowest_nonlinear_freq(struct amd_cpudata *cpudata)
 	return lowest_nonlinear_freq * 1000;
 }
 
-static int amd_pstate_set_boost(struct cpufreq_policy *policy, int state)
+static int amd_pstate_boost_init(struct amd_cpudata *cpudata)
 {
-	struct amd_cpudata *cpudata = policy->driver_data;
+	u64 boost_val;
 	int ret;
 
-	if (!cpudata->boost_supported) {
-		pr_err("Boost mode is not supported by this processor or SBIOS\n");
-		return -EINVAL;
+	ret = rdmsrl_on_cpu(cpudata->cpu, MSR_K7_HWCR, &boost_val);
+	if (ret) {
+		pr_err_once("failed to read initial CPU boost state!\n");
+		return ret;
 	}
 
-	if (state)
-		policy->cpuinfo.max_freq = cpudata->max_freq;
-	else
-		policy->cpuinfo.max_freq = cpudata->nominal_freq;
+	global.cpb_supported = !((boost_val >> 25) & 0x1);
+	global.cpb_boost = global.cpb_supported;
 
-	policy->max = policy->cpuinfo.max_freq;
+	return ret;
+}
 
-	ret = freq_qos_update_request(&cpudata->req[1],
-				      policy->cpuinfo.max_freq);
-	if (ret < 0)
-		return ret;
+static void amd_perf_ctl_reset(unsigned int cpu)
+{
+	wrmsrl_on_cpu(cpu, MSR_AMD_PERF_CTL, 0);
+}
 
-	return 0;
+/*
+ * Set amd-pstate preferred core enable can't be done directly from cpufreq callbacks
+ * due to locking, so queue the work for later.
+ */
+static void amd_pstste_sched_prefcore_workfn(struct work_struct *work)
+{
+	sched_set_itmt_support();
 }
+static DECLARE_WORK(sched_prefcore_work, amd_pstste_sched_prefcore_workfn);
 
-static void amd_pstate_boost_init(struct amd_cpudata *cpudata)
+/*
+ * Get the highest performance register value.
+ * @cpu: CPU from which to get highest performance.
+ * @highest_perf: Return address.
+ *
+ * Return: 0 for success, -EIO otherwise.
+ */
+static int amd_pstate_get_highest_perf(int cpu, u32 *highest_perf)
 {
-	u32 highest_perf, nominal_perf;
+	int ret;
 
-	highest_perf = READ_ONCE(cpudata->highest_perf);
-	nominal_perf = READ_ONCE(cpudata->nominal_perf);
+	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+		u64 cap1;
 
-	if (highest_perf <= nominal_perf)
+		ret = rdmsrl_safe_on_cpu(cpu, MSR_AMD_CPPC_CAP1, &cap1);
+		if (ret)
+			return ret;
+		WRITE_ONCE(*highest_perf, AMD_CPPC_HIGHEST_PERF(cap1));
+	} else {
+		u64 cppc_highest_perf;
+
+		ret = cppc_get_highest_perf(cpu, &cppc_highest_perf);
+		if (ret)
+			return ret;
+		WRITE_ONCE(*highest_perf, cppc_highest_perf);
+	}
+
+	return (ret);
+}
+
+#define CPPC_MAX_PERF	U8_MAX
+
+static void amd_pstate_init_prefcore(struct amd_cpudata *cpudata)
+{
+	int ret, prio;
+	u32 highest_perf;
+
+	ret = amd_pstate_get_highest_perf(cpudata->cpu, &highest_perf);
+	if (ret)
+		return;
+
+	cpudata->hw_prefcore = true;
+	/* check if CPPC preferred core feature is enabled*/
+	if (highest_perf < CPPC_MAX_PERF)
+		prio = (int)highest_perf;
+	else {
+		pr_debug("AMD CPPC preferred core is unsupported!\n");
+		cpudata->hw_prefcore = false;
 		return;
+	}
 
-	cpudata->boost_supported = true;
-	current_pstate_driver->boost_enabled = true;
+	if (!amd_pstate_prefcore)
+		return;
+
+	/*
+	 * The priorities can be set regardless of whether or not
+	 * sched_set_itmt_support(true) has been called and it is valid to
+	 * update them at any time after it has been called.
+	 */
+	sched_set_itmt_core_prio(prio, cpudata->cpu);
+
+	schedule_work(&sched_prefcore_work);
 }
 
-static void amd_perf_ctl_reset(unsigned int cpu)
+static void amd_pstate_update_limits(unsigned int cpu)
 {
-	wrmsrl_on_cpu(cpu, MSR_AMD_PERF_CTL, 0);
+	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+	struct amd_cpudata *cpudata = policy->driver_data;
+	u32 prev_high = 0, cur_high = 0;
+	int ret;
+	bool highest_perf_changed = false;
+
+	mutex_lock(&amd_pstate_driver_lock);
+	if ((!amd_pstate_prefcore) || (!cpudata->hw_prefcore))
+		goto free_cpufreq_put;
+
+	ret = amd_pstate_get_highest_perf(cpu, &cur_high);
+	if (ret)
+		goto free_cpufreq_put;
+
+	prev_high = READ_ONCE(cpudata->prefcore_ranking);
+	if (prev_high != cur_high) {
+		highest_perf_changed = true;
+		WRITE_ONCE(cpudata->prefcore_ranking, cur_high);
+
+		if (cur_high < CPPC_MAX_PERF)
+			sched_set_itmt_core_prio((int)cur_high, cpu);
+	}
+
+free_cpufreq_put:
+	cpufreq_cpu_put(policy);
+
+	if (!highest_perf_changed)
+		cpufreq_update_policy(cpu);
+
+	mutex_unlock(&amd_pstate_driver_lock);
+}
+
+/**
+ * Get pstate transition delay time from ACPI tables that firmware set
+ * instead of using hardcode value directly.
+ */
+static u32 amd_pstate_get_transition_delay_us(unsigned int cpu)
+{
+	u32 transition_delay_ns;
+
+	transition_delay_ns = cppc_get_transition_latency(cpu);
+	if (transition_delay_ns == CPUFREQ_ETERNAL)
+		return AMD_PSTATE_TRANSITION_DELAY;
+
+	return transition_delay_ns / NSEC_PER_USEC;
+}
+
+/**
+ * Get pstate transition latency value from ACPI tables that firmware set
+ * instead of using hardcode value directly.
+ */
+static u32 amd_pstate_get_transition_latency(unsigned int cpu)
+{
+	u32 transition_latency;
+
+	transition_latency = cppc_get_transition_latency(cpu);
+	if (transition_latency  == CPUFREQ_ETERNAL)
+		return AMD_PSTATE_TRANSITION_LATENCY;
+
+	return transition_latency;
 }
 
 static int amd_pstate_cpu_init(struct cpufreq_policy *policy)
@@ -727,24 +911,30 @@ static int amd_pstate_cpu_init(struct cpufreq_policy *policy)
 
 	cpudata->cpu = policy->cpu;
 
+	amd_pstate_init_prefcore(cpudata);
+
 	ret = amd_pstate_init_perf(cpudata);
 	if (ret)
 		goto free_cpudata1;
 
+	/* initialize cpu cores boot state */
+	amd_pstate_boost_init(cpudata);
+
 	min_freq = amd_get_min_freq(cpudata);
-	max_freq = amd_get_max_freq(cpudata);
 	nominal_freq = amd_get_nominal_freq(cpudata);
+	cpudata->nominal_freq = nominal_freq;
+	max_freq = amd_get_max_freq(cpudata);
 	lowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);
 
-	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {
-		dev_err(dev, "min_freq(%d) or max_freq(%d) value is incorrect\n",
-			min_freq, max_freq);
+	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq || nominal_freq == 0) {
+		dev_err(dev, "min_freq(%d) or max_freq(%d) or nominal_freq(%d) is incorrect\n",
+			min_freq, max_freq, nominal_freq);
 		ret = -EINVAL;
 		goto free_cpudata1;
 	}
 
-	policy->cpuinfo.transition_latency = AMD_PSTATE_TRANSITION_LATENCY;
-	policy->transition_delay_us = AMD_PSTATE_TRANSITION_DELAY;
+	policy->cpuinfo.transition_latency = amd_pstate_get_transition_latency(policy->cpu);
+	policy->transition_delay_us = amd_pstate_get_transition_delay_us(policy->cpu);
 
 	policy->min = min_freq;
 	policy->max = max_freq;
@@ -777,12 +967,10 @@ static int amd_pstate_cpu_init(struct cpufreq_policy *policy)
 	cpudata->min_freq = min_freq;
 	cpudata->max_limit_freq = max_freq;
 	cpudata->min_limit_freq = min_freq;
-	cpudata->nominal_freq = nominal_freq;
 	cpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;
 
 	policy->driver_data = cpudata;
 
-	amd_pstate_boost_init(cpudata);
 	if (!current_pstate_driver->adjust_perf)
 		current_pstate_driver->adjust_perf = amd_pstate_adjust_perf;
 
@@ -877,6 +1065,28 @@ static ssize_t show_amd_pstate_highest_perf(struct cpufreq_policy *policy,
 	return sysfs_emit(buf, "%u\n", perf);
 }
 
+static ssize_t show_amd_pstate_prefcore_ranking(struct cpufreq_policy *policy,
+						char *buf)
+{
+	u32 perf;
+	struct amd_cpudata *cpudata = policy->driver_data;
+
+	perf = READ_ONCE(cpudata->prefcore_ranking);
+
+	return sysfs_emit(buf, "%u\n", perf);
+}
+
+static ssize_t show_amd_pstate_hw_prefcore(struct cpufreq_policy *policy,
+					   char *buf)
+{
+	bool hw_prefcore;
+	struct amd_cpudata *cpudata = policy->driver_data;
+
+	hw_prefcore = READ_ONCE(cpudata->hw_prefcore);
+
+	return sysfs_emit(buf, "%s\n", str_enabled_disabled(hw_prefcore));
+}
+
 static ssize_t show_energy_performance_available_preferences(
 				struct cpufreq_policy *policy, char *buf)
 {
@@ -1074,18 +1284,125 @@ static ssize_t status_store(struct device *a, struct device_attribute *b,
 	return ret < 0 ? ret : count;
 }
 
+static ssize_t prefcore_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%s\n", str_enabled_disabled(amd_pstate_prefcore));
+}
+
+static int amd_cpu_boost_update(struct amd_cpudata *cpudata, u32 on)
+{
+	struct cpufreq_policy *policy = cpufreq_cpu_acquire(cpudata->cpu);
+	struct cppc_perf_ctrls perf_ctrls;
+	u32 highest_perf, nominal_perf;
+	int ret;
+
+	if (!policy)
+		return -ENODATA;
+
+	highest_perf = READ_ONCE(cpudata->highest_perf);
+	nominal_perf = READ_ONCE(cpudata->nominal_perf);
+
+	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+		u64 value = READ_ONCE(cpudata->cppc_req_cached);
+
+		value &= ~GENMASK_ULL(7, 0);
+		value |= on ? highest_perf : nominal_perf;
+		WRITE_ONCE(cpudata->cppc_req_cached, value);
+
+		wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+
+	} else {
+		perf_ctrls.max_perf = on ? highest_perf : nominal_perf;
+		ret = cppc_set_epp_perf(cpudata->cpu, &perf_ctrls, 1);
+		if (ret) {
+			pr_debug("failed to set energy perf value (%d)\n", ret);
+			return ret;
+		}
+	}
+
+	if (on)
+		policy->cpuinfo.max_freq = cpudata->max_freq;
+	else
+		policy->cpuinfo.max_freq = cpudata->nominal_freq;
+
+	policy->max = policy->cpuinfo.max_freq;
+
+	if (cppc_state == AMD_PSTATE_PASSIVE) {
+		ret = freq_qos_update_request(&cpudata->req[1],
+				      policy->cpuinfo.max_freq);
+	}
+
+	cpufreq_cpu_release(policy);
+
+	return ret;
+}
+
+static ssize_t cpb_boost_show(struct device *dev,
+			   struct device_attribute *attr, char *buf)
+{
+	return sysfs_emit(buf, "%u\n", global.cpb_boost);
+}
+
+static ssize_t cpb_boost_store(struct device *dev, struct device_attribute *b,
+			    const char *buf, size_t count)
+{
+	bool new_state;
+	ssize_t ret;
+	int cpu;
+
+	mutex_lock(&amd_pstate_driver_lock);
+	if (!global.cpb_supported) {
+		pr_err("Boost mode is not supported by this processor or SBIOS\n");
+		return -EINVAL;
+	}
+
+	ret = kstrtobool(buf, &new_state);
+	if (ret)
+		return -EINVAL;
+
+	global.cpb_boost = !!new_state;
+
+	for_each_possible_cpu(cpu) {
+
+		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+		struct amd_cpudata *cpudata = policy->driver_data;
+
+		if (!cpudata) {
+			pr_err("cpudata is NULL\n");
+			ret = -ENODATA;
+			cpufreq_cpu_put(policy);
+			goto err_exit;
+		}
+
+		amd_cpu_boost_update(cpudata, global.cpb_boost);
+		refresh_frequency_limits(policy);
+		cpufreq_cpu_put(policy);
+	}
+
+err_exit:
+	mutex_unlock(&amd_pstate_driver_lock);
+	return ret < 0 ? ret : count;
+}
+
 cpufreq_freq_attr_ro(amd_pstate_max_freq);
 cpufreq_freq_attr_ro(amd_pstate_lowest_nonlinear_freq);
 
 cpufreq_freq_attr_ro(amd_pstate_highest_perf);
+cpufreq_freq_attr_ro(amd_pstate_prefcore_ranking);
+cpufreq_freq_attr_ro(amd_pstate_hw_prefcore);
 cpufreq_freq_attr_rw(energy_performance_preference);
 cpufreq_freq_attr_ro(energy_performance_available_preferences);
 static DEVICE_ATTR_RW(status);
+static DEVICE_ATTR_RO(prefcore);
+static DEVICE_ATTR_RW(cpb_boost);
 
 static struct freq_attr *amd_pstate_attr[] = {
 	&amd_pstate_max_freq,
 	&amd_pstate_lowest_nonlinear_freq,
 	&amd_pstate_highest_perf,
+	&amd_pstate_prefcore_ranking,
+	&amd_pstate_hw_prefcore,
 	NULL,
 };
 
@@ -1093,6 +1410,8 @@ static struct freq_attr *amd_pstate_epp_attr[] = {
 	&amd_pstate_max_freq,
 	&amd_pstate_lowest_nonlinear_freq,
 	&amd_pstate_highest_perf,
+	&amd_pstate_prefcore_ranking,
+	&amd_pstate_hw_prefcore,
 	&energy_performance_preference,
 	&energy_performance_available_preferences,
 	NULL,
@@ -1100,6 +1419,8 @@ static struct freq_attr *amd_pstate_epp_attr[] = {
 
 static struct attribute *pstate_global_attributes[] = {
 	&dev_attr_status.attr,
+	&dev_attr_prefcore.attr,
+	&dev_attr_cpb_boost.attr,
 	NULL
 };
 
@@ -1151,17 +1472,23 @@ static int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)
 	cpudata->cpu = policy->cpu;
 	cpudata->epp_policy = 0;
 
+	amd_pstate_init_prefcore(cpudata);
+
 	ret = amd_pstate_init_perf(cpudata);
 	if (ret)
 		goto free_cpudata1;
 
+	/* initialize cpu cores boot state */
+	amd_pstate_boost_init(cpudata);
+
 	min_freq = amd_get_min_freq(cpudata);
-	max_freq = amd_get_max_freq(cpudata);
 	nominal_freq = amd_get_nominal_freq(cpudata);
+	cpudata->nominal_freq = nominal_freq;
+	max_freq = amd_get_max_freq(cpudata);
 	lowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);
-	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {
-		dev_err(dev, "min_freq(%d) or max_freq(%d) value is incorrect\n",
-				min_freq, max_freq);
+	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq || nominal_freq == 0) {
+		dev_err(dev, "min_freq(%d) or max_freq(%d) or nominal_freq(%d) is incorrect\n",
+				min_freq, max_freq, nominal_freq);
 		ret = -EINVAL;
 		goto free_cpudata1;
 	}
@@ -1174,7 +1501,6 @@ static int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)
 	/* Initial processor data capability frequencies */
 	cpudata->max_freq = max_freq;
 	cpudata->min_freq = min_freq;
-	cpudata->nominal_freq = nominal_freq;
 	cpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;
 
 	policy->driver_data = cpudata;
@@ -1205,7 +1531,6 @@ static int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)
 			return ret;
 		WRITE_ONCE(cpudata->cppc_cap1_cached, value);
 	}
-	amd_pstate_boost_init(cpudata);
 
 	return 0;
 
@@ -1431,7 +1756,7 @@ static struct cpufreq_driver amd_pstate_driver = {
 	.exit		= amd_pstate_cpu_exit,
 	.suspend	= amd_pstate_cpu_suspend,
 	.resume		= amd_pstate_cpu_resume,
-	.set_boost	= amd_pstate_set_boost,
+	.update_limits	= amd_pstate_update_limits,
 	.name		= "amd-pstate",
 	.attr		= amd_pstate_attr,
 };
@@ -1446,6 +1771,7 @@ static struct cpufreq_driver amd_pstate_epp_driver = {
 	.online		= amd_pstate_epp_cpu_online,
 	.suspend	= amd_pstate_epp_suspend,
 	.resume		= amd_pstate_epp_resume,
+	.update_limits	= amd_pstate_update_limits,
 	.name		= "amd-pstate-epp",
 	.attr		= amd_pstate_epp_attr,
 };
@@ -1486,6 +1812,11 @@ static int __init amd_pstate_init(void)
 	if (cpufreq_get_current_driver())
 		return -EEXIST;
 
+	quirks = NULL;
+
+	/* check if this machine need CPPC quirks */
+	dmi_check_system(amd_pstate_quirks_table);
+
 	switch (cppc_state) {
 	case AMD_PSTATE_UNDEFINED:
 		/* Disable on the following configs by default:
@@ -1567,7 +1898,17 @@ static int __init amd_pstate_param(char *str)
 
 	return amd_pstate_set_driver(mode_idx);
 }
+
+static int __init amd_prefcore_param(char *str)
+{
+	if (!strcmp(str, "disable"))
+		amd_pstate_prefcore = false;
+
+	return 0;
+}
+
 early_param("amd_pstate", amd_pstate_param);
+early_param("amd_prefcore", amd_prefcore_param);
 
 MODULE_AUTHOR("Huang Rui <ray.huang@amd.com>");
 MODULE_DESCRIPTION("AMD Processor P-state Frequency Driver");
diff --git a/include/acpi/cppc_acpi.h b/include/acpi/cppc_acpi.h
index 3a0995f8b..930b6afba 100644
--- a/include/acpi/cppc_acpi.h
+++ b/include/acpi/cppc_acpi.h
@@ -139,6 +139,7 @@ struct cppc_cpudata {
 #ifdef CONFIG_ACPI_CPPC_LIB
 extern int cppc_get_desired_perf(int cpunum, u64 *desired_perf);
 extern int cppc_get_nominal_perf(int cpunum, u64 *nominal_perf);
+extern int cppc_get_highest_perf(int cpunum, u64 *highest_perf);
 extern int cppc_get_perf_ctrs(int cpu, struct cppc_perf_fb_ctrs *perf_fb_ctrs);
 extern int cppc_set_perf(int cpu, struct cppc_perf_ctrls *perf_ctrls);
 extern int cppc_set_enable(int cpu, bool enable);
@@ -167,6 +168,10 @@ static inline int cppc_get_nominal_perf(int cpunum, u64 *nominal_perf)
 {
 	return -ENOTSUPP;
 }
+static inline int cppc_get_highest_perf(int cpunum, u64 *highest_perf)
+{
+	return -ENOTSUPP;
+}
 static inline int cppc_get_perf_ctrs(int cpu, struct cppc_perf_fb_ctrs *perf_fb_ctrs)
 {
 	return -ENOTSUPP;
diff --git a/include/linux/amd-pstate.h b/include/linux/amd-pstate.h
index 6ad02ad9c..db4462774 100644
--- a/include/linux/amd-pstate.h
+++ b/include/linux/amd-pstate.h
@@ -39,11 +39,16 @@ struct amd_aperf_mperf {
  * @cppc_req_cached: cached performance request hints
  * @highest_perf: the maximum performance an individual processor may reach,
  *		  assuming ideal conditions
+ *		  For platforms that do not support the preferred core feature, the
+ *		  highest_pef may be configured with 166 or 255, to avoid max frequency
+ *		  calculated wrongly. we take the fixed value as the highest_perf.
  * @nominal_perf: the maximum sustained performance level of the processor,
  *		  assuming ideal operating conditions
  * @lowest_nonlinear_perf: the lowest performance level at which nonlinear power
  *			   savings are achieved
  * @lowest_perf: the absolute lowest performance level of the processor
+ * @prefcore_ranking: the preferred core ranking, the higher value indicates a higher
+ * 		  priority.
  * @max_freq: the frequency that mapped to highest_perf
  * @min_freq: the frequency that mapped to lowest_perf
  * @nominal_freq: the frequency that mapped to nominal_perf
@@ -51,7 +56,9 @@ struct amd_aperf_mperf {
  * @cur: Difference of Aperf/Mperf/tsc count between last and current sample
  * @prev: Last Aperf/Mperf/tsc count value read from register
  * @freq: current cpu frequency value
- * @boost_supported: check whether the Processor or SBIOS supports boost mode
+ * @hw_prefcore: check whether HW supports preferred core featue.
+ * 		  Only when hw_prefcore and early prefcore param are true,
+ * 		  AMD P-State driver supports preferred core featue.
  * @epp_policy: Last saved policy used to set energy-performance preference
  * @epp_cached: Cached CPPC energy-performance preference value
  * @policy: Cpufreq policy value
@@ -70,6 +77,7 @@ struct amd_cpudata {
 	u32	nominal_perf;
 	u32	lowest_nonlinear_perf;
 	u32	lowest_perf;
+	u32     prefcore_ranking;
 	u32     min_limit_perf;
 	u32     max_limit_perf;
 	u32     min_limit_freq;
@@ -84,7 +92,7 @@ struct amd_cpudata {
 	struct amd_aperf_mperf prev;
 
 	u64	freq;
-	bool	boost_supported;
+	bool	hw_prefcore;
 
 	/* EPP feature related attributes*/
 	s16	epp_policy;
@@ -114,4 +122,10 @@ static const char * const amd_pstate_mode_string[] = {
 	[AMD_PSTATE_GUIDED]      = "guided",
 	NULL,
 };
+
+struct quirk_entry {
+	u32 nominal_freq;
+	u32 lowest_freq;
+};
+
 #endif /* _LINUX_AMD_PSTATE_H */
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index afda5f24d..9bebeec24 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -263,6 +263,7 @@ static inline bool cpufreq_supports_freq_invariance(void)
 	return false;
 }
 static inline void disable_cpufreq(void) { }
+static inline void cpufreq_update_limits(unsigned int cpu) { }
 #endif
 
 #ifdef CONFIG_CPU_FREQ_STAT
diff --git a/tools/testing/selftests/scx/Makefile.rej b/tools/testing/selftests/scx/Makefile.rej
new file mode 100644
index 000000000..e319ebe88
--- /dev/null
+++ b/tools/testing/selftests/scx/Makefile.rej
@@ -0,0 +1,307 @@
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -42,6 +42,8 @@ SCXOBJ_DIR := $(OBJ_DIR)/sched_ext
+ BPFOBJ := $(BPFOBJ_DIR)/libbpf.a
+ LIBBPF_OUTPUT := $(OBJ_DIR)/libbpf/libbpf.a
+ DEFAULT_BPFTOOL := $(OUTPUT_DIR)/sbin/bpftool
++HOST_BUILD_DIR := $(OBJ_DIR)
++HOST_OUTPUT_DIR := $(OUTPUT_DIR)
+ 
+ VMLINUX_BTF_PATHS ?= ../../../../vmlinux					\
+ 		     /sys/kernel/btf/vmlinux					\
+@@ -145,7 +147,15 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
+ ################
+ # C schedulers #
+ ################
+-c-sched-targets := minimal
++c-sched-targets :=			\
++	minimal				\
++	select_cpu_dfl			\
++	select_cpu_dfl_nodispatch	\
++	select_cpu_dispatch		\
++	select_cpu_dispatch_dbl_dsp	\
++	select_cpu_dispatch_bad_dsq	\
++	enqueue_select_cpu_fails	\
++	enq_last_no_enq_fails
+ 
+ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+ 	$(eval sched=$(notdir $@))
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -155,7 +155,9 @@ c-sched-targets :=			\
+ 	select_cpu_dispatch_dbl_dsp	\
+ 	select_cpu_dispatch_bad_dsq	\
+ 	enqueue_select_cpu_fails	\
+-	enq_last_no_enq_fails
++	enq_last_no_enq_fails		\
++	dsp_localdsq_fail		\
++	dsp_fallbackdsq_fail
+ 
+ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+ 	$(eval sched=$(notdir $@))
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -96,8 +96,8 @@ BPF_CFLAGS = -g -D__TARGET_ARCH_$(SRCARCH)					\
+ 	     -O2 -mcpu=v3
+ 
+ # sort removes libbpf duplicates when not cross-building
+-MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(HOST_BUILD_DIR)/libbpf			\
+-	       $(HOST_BUILD_DIR)/bpftool $(HOST_BUILD_DIR)/resolve_btfids	\
++MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(OBJ_DIR)/libbpf				\
++	       $(OBJ_DIR)/bpftool $(OBJ_DIR)/resolve_btfids			\
+ 	       $(INCLUDE_DIR) $(SCXOBJ_DIR))
+ 
+ $(MAKE_DIRS):
+@@ -112,14 +112,14 @@ $(BPFOBJ): $(wildcard $(BPFDIR)/*.[ch] $(BPFDIR)/Makefile)			\
+ 		    DESTDIR=$(OUTPUT_DIR) prefix= all install_headers
+ 
+ $(DEFAULT_BPFTOOL): $(wildcard $(BPFTOOLDIR)/*.[ch] $(BPFTOOLDIR)/Makefile)	\
+-		    $(LIBBPF_OUTPUT) | $(HOST_BUILD_DIR)/bpftool
++		    $(LIBBPF_OUTPUT) | $(OBJ_DIR)/bpftool
+ 	$(Q)$(MAKE) $(submake_extras)  -C $(BPFTOOLDIR)				\
+ 		    ARCH= CROSS_COMPILE= CC=$(HOSTCC) LD=$(HOSTLD)		\
+ 		    EXTRA_CFLAGS='-g -O0'					\
+-		    OUTPUT=$(HOST_BUILD_DIR)/bpftool/				\
+-		    LIBBPF_OUTPUT=$(HOST_BUILD_DIR)/libbpf/			\
+-		    LIBBPF_DESTDIR=$(HOST_OUTPUT_DIR)/				\
+-		    prefix= DESTDIR=$(HOST_OUTPUT_DIR)/ install-bin
++		    OUTPUT=$(OBJ_DIR)/bpftool/					\
++		    LIBBPF_OUTPUT=$(OBJ_DIR)/libbpf/				\
++		    LIBBPF_DESTDIR=$(OUTPUT_DIR)/				\
++		    prefix= DESTDIR=$(OUTPUT_DIR)/ install-bin
+ 
+ $(INCLUDE_DIR)/vmlinux.h: $(VMLINUX_BTF) $(BPFTOOL) | $(INCLUDE_DIR)
+ ifeq ($(VMLINUX_H),)
+@@ -148,16 +148,17 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
+ # C schedulers #
+ ################
+ c-sched-targets :=			\
++	dsp_fallbackdsq_fail		\
++	dsp_localdsq_fail		\
++	enq_last_no_enq_fails		\
++	enqueue_select_cpu_fails	\
++	init_enable_count		\
+ 	minimal				\
+ 	select_cpu_dfl			\
+ 	select_cpu_dfl_nodispatch	\
+ 	select_cpu_dispatch		\
+-	select_cpu_dispatch_dbl_dsp	\
+ 	select_cpu_dispatch_bad_dsq	\
+-	enqueue_select_cpu_fails	\
+-	enq_last_no_enq_fails		\
+-	dsp_localdsq_fail		\
+-	dsp_fallbackdsq_fail
++	select_cpu_dispatch_dbl_dsp
+ 
+ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+ 	$(eval sched=$(notdir $@))
+@@ -167,7 +168,7 @@ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+ TEST_GEN_PROGS := $(c-sched-targets)
+ 
+ override define CLEAN
+-	rm -rf $(OUTPUT_DIR) $(HOST_OUTPUT_DIR)
++	rm -rf $(OUTPUT_DIR)
+ 	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
+ 	rm -f $(TEST_GEN_PROGS)
+ endef
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -158,7 +158,8 @@ c-sched-targets :=			\
+ 	select_cpu_dfl_nodispatch	\
+ 	select_cpu_dispatch		\
+ 	select_cpu_dispatch_bad_dsq	\
+-	select_cpu_dispatch_dbl_dsp
++	select_cpu_dispatch_dbl_dsp	\
++	select_cpu_vtime
+ 
+ $(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+ 	$(eval sched=$(notdir $@))
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -68,7 +68,7 @@ ifneq ($(LLVM),)
+ CFLAGS += -Wno-unused-command-line-argument
+ endif
+ 
+-LDFLAGS = -lelf -lz -lpthread
++LDFLAGS = -lelf -lz -lpthread -lzstd
+ 
+ IS_LITTLE_ENDIAN = $(shell $(CC) -dM -E - </dev/null |				\
+ 			grep 'define __BYTE_ORDER__ __ORDER_LITTLE_ENDIAN__')
+@@ -148,12 +148,6 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
+ # C schedulers #
+ ################
+ c-sched-targets :=			\
+-	dsp_fallbackdsq_fail		\
+-	dsp_localdsq_fail		\
+-	enq_last_no_enq_fails		\
+-	enqueue_select_cpu_fails	\
+-	init_enable_count		\
+-	minimal				\
+ 	select_cpu_dfl			\
+ 	select_cpu_dfl_nodispatch	\
+ 	select_cpu_dispatch		\
+@@ -172,9 +166,39 @@ override define CLEAN
+ 	rm -rf $(OUTPUT_DIR)
+ 	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
+ 	rm -f $(TEST_GEN_PROGS)
++	rm -f runner
+ endef
+ 
+-all: $(TEST_GEN_PROGS)
++auto-test-targets :=			\
++	enq_last_no_enq_fails		\
++	enq_select_cpu_fails		\
++	ddsp_bogus_dsq_fail		\
++	ddsp_vtimelocal_fail		\
++	init_enable_count		\
++	minimal				\
++	test_example
++
++testcase-targets := $(addsuffix .o,$(addprefix $(SCXOBJ_DIR)/,$(auto-test-targets)))
++
++$(SCXOBJ_DIR)/runner.o: runner.c | $(SCXOBJ_DIR)
++	$(CC) $(CFLAGS) -c $< -o $@
++
++# Create all of the test targets object files, whose testcase objects will be
++# registered into the runner in ELF constructors.
++#
++# Note that we must do double expansion here in order to support conditionally
++# compiling BPF object files only if one is present, as the wildcard Make
++# function doesn't support using implicit rules otherwise.
++.SECONDEXPANSION:
++$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o $$(if $$(wildcard $$*.bpf.c), $(INCLUDE_DIR)/%.bpf.skel.h) | $(SCXOBJ_DIR)
++	$(eval test=$(patsubst %.o,%.c,$(notdir $@)))
++	$(CC) $(CFLAGS) -c $< -o $@ $(SCXOBJ_DIR)/runner.o
++
++runner: $(SCXOBJ_DIR)/runner.o $(BPFOBJ) $(testcase-targets)
++	@echo "$(testcase-targets)"
++	$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^
++
++all: runner
+ 
+ .PHONY: all clean help
+ 
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -147,20 +147,6 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
+ ################
+ # C schedulers #
+ ################
+-c-sched-targets :=			\
+-	select_cpu_dfl			\
+-	select_cpu_dfl_nodispatch	\
+-	select_cpu_dispatch		\
+-	select_cpu_dispatch_bad_dsq	\
+-	select_cpu_dispatch_dbl_dsp	\
+-	select_cpu_vtime
+-
+-$(c-sched-targets): %: $(filter-out %.bpf.c,%.c) $(INCLUDE_DIR)/%.bpf.skel.h
+-	$(eval sched=$(notdir $@))
+-	$(CC) $(CFLAGS) -c $(sched).c -o $(SCXOBJ_DIR)/$(sched).o
+-	$(CC) -o $@ $(SCXOBJ_DIR)/$(sched).o $(LIBBPF_OUTPUT) $(LDFLAGS)
+-
+-TEST_GEN_PROGS := $(c-sched-targets)
+ 
+ override define CLEAN
+ 	rm -rf $(OUTPUT_DIR)
+@@ -176,6 +162,12 @@ auto-test-targets :=			\
+ 	ddsp_vtimelocal_fail		\
+ 	init_enable_count		\
+ 	minimal				\
++	select_cpu_dfl			\
++	select_cpu_dfl_nodispatch	\
++	select_cpu_dispatch		\
++	select_cpu_dispatch_bad_dsq	\
++	select_cpu_dispatch_dbl_dsp	\
++	select_cpu_vtime		\
+ 	test_example
+ 
+ testcase-targets := $(addsuffix .o,$(addprefix $(SCXOBJ_DIR)/,$(auto-test-targets)))
+@@ -198,6 +190,8 @@ runner: $(SCXOBJ_DIR)/runner.o $(BPFOBJ) $(testcase-targets)
+ 	@echo "$(testcase-targets)"
+ 	$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^
+ 
++TEST_GEN_PROGS := runner
++
+ all: runner
+ 
+ .PHONY: all clean help
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -161,6 +161,7 @@ auto-test-targets :=			\
+ 	ddsp_bogus_dsq_fail		\
+ 	ddsp_vtimelocal_fail		\
+ 	init_enable_count		\
++	maybe_null			\
+ 	minimal				\
+ 	select_cpu_dfl			\
+ 	select_cpu_dfl_nodispatch	\
+@@ -190,7 +193,7 @@ $(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o $$(if $$(wild
+ 
+ runner: $(SCXOBJ_DIR)/runner.o $(BPFOBJ) $(testcase-targets)
+ 	@echo "$(testcase-targets)"
+-	$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^
++	$(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS)
+ 
+ TEST_GEN_PROGS := runner
+ 
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -183,7 +181,10 @@ $(SCXOBJ_DIR)/maybe_null.o: $(INCLUDE_DIR)/maybe_null_fail.bpf.skel.h
+ # compiling BPF object files only if one is present, as the wildcard Make
+ # function doesn't support using implicit rules otherwise.
+ .SECONDEXPANSION:
+-$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o $$(if $$(wildcard $$*.bpf.c), $(INCLUDE_DIR)/%.bpf.skel.h) | $(SCXOBJ_DIR)
++$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o		\
++	$$(if $$(wildcard $$*.bpf.c), $(INCLUDE_DIR)/%.bpf.skel.h)		\
++	$$(if $$(wildcard $$*_fail.bpf.c), $(INCLUDE_DIR)/%_fail.bpf.skel.h)	\
++	| $(SCXOBJ_DIR)
+ 	$(eval test=$(patsubst %.o,%.c,$(notdir $@)))
+ 	$(CC) $(CFLAGS) -c $< -o $@ $(SCXOBJ_DIR)/runner.o
+ 
+--- tools/testing/selftests/scx/Makefile
++++ tools/testing/selftests/scx/Makefile
+@@ -155,22 +155,28 @@ override define CLEAN
+ 	rm -f runner
+ endef
+ 
++# Every testcase takes all of the BPF progs are dependencies by default. This
++# allows testcases to load any BPF scheduler, which is useful for testcases
++# that don't need their own prog to run their test.
++all_test_bpfprogs := $(foreach prog,$(wildcard *.bpf.c),$(INCLUDE_DIR)/$(patsubst %.c,%.skel.h,$(prog)))
++
+ auto-test-targets :=			\
+ 	enq_last_no_enq_fails		\
+ 	enq_select_cpu_fails		\
+ 	ddsp_bogus_dsq_fail		\
+ 	ddsp_vtimelocal_fail		\
+ 	init_enable_count		\
+-	maybe_null			\
+ 	maximal				\
++	maybe_null			\
+ 	minimal				\
++	reload_loop			\
+ 	select_cpu_dfl			\
+ 	select_cpu_dfl_nodispatch	\
+ 	select_cpu_dispatch		\
+ 	select_cpu_dispatch_bad_dsq	\
+ 	select_cpu_dispatch_dbl_dsp	\
+ 	select_cpu_vtime		\
+-	test_example
++	test_example			\
+ 
+ testcase-targets := $(addsuffix .o,$(addprefix $(SCXOBJ_DIR)/,$(auto-test-targets)))
+ 
+@@ -183,11 +189,7 @@ $(SCXOBJ_DIR)/runner.o: runner.c | $(SCXOBJ_DIR)
+ # Note that we must do double expansion here in order to support conditionally
+ # compiling BPF object files only if one is present, as the wildcard Make
+ # function doesn't support using implicit rules otherwise.
+-.SECONDEXPANSION:
+-$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o		\
+-	$$(if $$(wildcard $$*.bpf.c), $(INCLUDE_DIR)/%.bpf.skel.h)		\
+-	$$(if $$(wildcard $$*_fail.bpf.c), $(INCLUDE_DIR)/%_fail.bpf.skel.h)	\
+-	| $(SCXOBJ_DIR)
++$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o $(all_test_bpfprogs) | $(SCXOBJ_DIR)
+ 	$(eval test=$(patsubst %.o,%.c,$(notdir $@)))
+ 	$(CC) $(CFLAGS) -c $< -o $@ $(SCXOBJ_DIR)/runner.o
+ 
diff --git a/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c.rej b/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c.rej
new file mode 100644
index 000000000..e32be7bcc
--- /dev/null
+++ b/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c.rej
@@ -0,0 +1,65 @@
+--- tools/testing/selftests/scx/dsp_fallbackdsq_fail.bpf.c
++++ tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c
+@@ -8,7 +8,7 @@
+ 
+ char _license[] SEC("license") = "GPL";
+ 
+-s32 BPF_STRUCT_OPS(dsp_fallbackdsq_fail_select_cpu, struct task_struct *p,
++s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_select_cpu, struct task_struct *p,
+ 		   s32 prev_cpu, u64 wake_flags)
+ {
+ 	s32 cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+@@ -26,7 +26,7 @@ s32 BPF_STRUCT_OPS(dsp_fallbackdsq_fail_select_cpu, struct task_struct *p,
+ 	return prev_cpu;
+ }
+ 
+-s32 BPF_STRUCT_OPS(dsp_fallbackdsq_fail_init)
++s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_init)
+ {
+ 	scx_bpf_switch_all();
+ 
+@@ -34,9 +34,9 @@ s32 BPF_STRUCT_OPS(dsp_fallbackdsq_fail_init)
+ }
+ 
+ SEC(".struct_ops.link")
+-struct sched_ext_ops dsp_fallbackdsq_fail_ops = {
+-	.select_cpu		= dsp_fallbackdsq_fail_select_cpu,
+-	.init			= dsp_fallbackdsq_fail_init,
+-	.name			= "dsp_fallbackdsq_fail",
++struct sched_ext_ops ddsp_bogus_dsq_fail_ops = {
++	.select_cpu		= ddsp_bogus_dsq_fail_select_cpu,
++	.init			= ddsp_bogus_dsq_fail_init,
++	.name			= "ddsp_bogus_dsq_fail",
+ 	.timeout_ms		= 1000U,
+ };
+--- tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c
++++ tools/testing/selftests/scx/ddsp_bogus_dsq_fail.bpf.c
+@@ -8,6 +8,8 @@
+ 
+ char _license[] SEC("license") = "GPL";
+ 
++struct user_exit_info uei;
++
+ s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_select_cpu, struct task_struct *p,
+ 		   s32 prev_cpu, u64 wake_flags)
+ {
+@@ -26,6 +28,11 @@ s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_select_cpu, struct task_struct *p,
+ 	return prev_cpu;
+ }
+ 
++void BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_exit, struct scx_exit_info *ei)
++{
++	uei_record(&uei, ei);
++}
++
+ s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_init)
+ {
+ 	scx_bpf_switch_all();
+@@ -36,6 +43,7 @@ s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_init)
+ SEC(".struct_ops.link")
+ struct sched_ext_ops ddsp_bogus_dsq_fail_ops = {
+ 	.select_cpu		= ddsp_bogus_dsq_fail_select_cpu,
++	.exit			= ddsp_bogus_dsq_fail_exit,
+ 	.init			= ddsp_bogus_dsq_fail_init,
+ 	.name			= "ddsp_bogus_dsq_fail",
+ 	.timeout_ms		= 1000U,
diff --git a/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c.rej b/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c.rej
new file mode 100644
index 000000000..33dba56ac
--- /dev/null
+++ b/tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c.rej
@@ -0,0 +1,11 @@
+--- tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c
++++ tools/testing/selftests/scx/ddsp_bogus_dsq_fail.c
+@@ -33,6 +35,8 @@ static enum scx_test_status run(void *ctx)
+ 	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+ 
+ 	sleep(1);
++
++	SCX_EQ(skel->bss->uei.kind, SCX_EXIT_ERROR);
+ 	bpf_link__destroy(link);
+ 
+ 	return SCX_TEST_PASS;
diff --git a/tools/testing/selftests/scx/ddsp_vtimelocal_fail.c.rej b/tools/testing/selftests/scx/ddsp_vtimelocal_fail.c.rej
new file mode 100644
index 000000000..f6fd08c8a
--- /dev/null
+++ b/tools/testing/selftests/scx/ddsp_vtimelocal_fail.c.rej
@@ -0,0 +1,11 @@
+--- tools/testing/selftests/scx/ddsp_vtimelocal_fail.c
++++ tools/testing/selftests/scx/ddsp_vtimelocal_fail.c
+@@ -32,6 +34,8 @@ static enum scx_test_status run(void *ctx)
+ 	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+ 
+ 	sleep(1);
++
++	SCX_EQ(skel->bss->uei.kind, SCX_EXIT_ERROR);
+ 	bpf_link__destroy(link);
+ 
+ 	return SCX_TEST_PASS;
diff --git a/tools/testing/selftests/scx/enq_last_no_enq_fails.c.rej b/tools/testing/selftests/scx/enq_last_no_enq_fails.c.rej
new file mode 100644
index 000000000..22842604a
--- /dev/null
+++ b/tools/testing/selftests/scx/enq_last_no_enq_fails.c.rej
@@ -0,0 +1,72 @@
+--- tools/testing/selftests/scx/enq_last_no_enq_fails.c
++++ tools/testing/selftests/scx/enq_last_no_enq_fails.c
+@@ -4,31 +4,57 @@
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "enq_last_no_enq_fails.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct enq_last_no_enq_fails *skel;
+-	struct bpf_link *link;
+-
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+ 
+ 	skel = enq_last_no_enq_fails__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
++	if (!skel) {
++		SCX_ERR("Failed to open and load skel");
++		return SCX_TEST_FAIL;
++	}
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
++}
++
++static enum scx_test_status run(void *ctx)
++{
++	struct enq_last_no_enq_fails *skel = ctx;
++	struct bpf_link *link;
+ 
+ 	link = bpf_map__attach_struct_ops(skel->maps.enq_last_no_enq_fails_ops);
+-	SCX_BUG_ON(link, "Succeeded in attaching struct_ops");
++	if (link) {
++		SCX_ERR("Incorrectly succeeded in to attaching scheduler");
++		return SCX_TEST_FAIL;
++	}
+ 
+ 	bpf_link__destroy(link);
+-	enq_last_no_enq_fails__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
++}
++
++static void cleanup(void *ctx)
++{
++	struct enq_last_no_enq_fails *skel = ctx;
++
++	enq_last_no_enq_fails__destroy(skel);
+ }
++
++struct scx_test enq_last_no_enq_fails = {
++	.name = "enq_last_no_enq_fails",
++	.description = "Verify we fail to load a scheduler if we specify "
++		       "the SCX_OPS_ENQ_LAST flag without defining "
++		       "ops.enqueue()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&enq_last_no_enq_fails)
diff --git a/tools/testing/selftests/scx/enq_select_cpu_fails.bpf.c.rej b/tools/testing/selftests/scx/enq_select_cpu_fails.bpf.c.rej
new file mode 100644
index 000000000..600cd2f84
--- /dev/null
+++ b/tools/testing/selftests/scx/enq_select_cpu_fails.bpf.c.rej
@@ -0,0 +1,52 @@
+--- tools/testing/selftests/scx/enqueue_select_cpu_fails.bpf.c
++++ tools/testing/selftests/scx/enq_select_cpu_fails.bpf.c
+@@ -1,8 +1,5 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+- * A scheduler that validates the behavior of direct dispatching with a default
+- * select_cpu implementation.
+- *
+  * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+@@ -16,13 +13,13 @@ char _license[] SEC("license") = "GPL";
+ s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags,
+ 			   bool *found) __ksym;
+ 
+-s32 BPF_STRUCT_OPS(enqueue_select_cpu_fails_select_cpu, struct task_struct *p,
++s32 BPF_STRUCT_OPS(enq_select_cpu_fails_select_cpu, struct task_struct *p,
+ 		   s32 prev_cpu, u64 wake_flags)
+ {
+ 	return prev_cpu;
+ }
+ 
+-void BPF_STRUCT_OPS(enqueue_select_cpu_fails_enqueue, struct task_struct *p,
++void BPF_STRUCT_OPS(enq_select_cpu_fails_enqueue, struct task_struct *p,
+ 		    u64 enq_flags)
+ {
+ 	/*
+@@ -37,7 +34,7 @@ void BPF_STRUCT_OPS(enqueue_select_cpu_fails_enqueue, struct task_struct *p,
+ 	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+ }
+ 
+-s32 BPF_STRUCT_OPS(enqueue_select_cpu_fails_init)
++s32 BPF_STRUCT_OPS(enq_select_cpu_fails_init)
+ {
+ 	scx_bpf_switch_all();
+ 
+@@ -45,10 +42,10 @@ s32 BPF_STRUCT_OPS(enqueue_select_cpu_fails_init)
+ }
+ 
+ SEC(".struct_ops.link")
+-struct sched_ext_ops enqueue_select_cpu_fails_ops = {
+-	.select_cpu		= enqueue_select_cpu_fails_select_cpu,
+-	.enqueue		= enqueue_select_cpu_fails_enqueue,
+-	.init			= enqueue_select_cpu_fails_init,
+-	.name			= "enqueue_select_cpu_fails",
++struct sched_ext_ops enq_select_cpu_fails_ops = {
++	.select_cpu		= enq_select_cpu_fails_select_cpu,
++	.enqueue		= enq_select_cpu_fails_enqueue,
++	.init			= enq_select_cpu_fails_init,
++	.name			= "enq_select_cpu_fails",
+ 	.timeout_ms		= 1000U,
+ };
diff --git a/tools/testing/selftests/scx/init_enable_count.bpf.c.rej b/tools/testing/selftests/scx/init_enable_count.bpf.c.rej
new file mode 100644
index 000000000..be15dcbb8
--- /dev/null
+++ b/tools/testing/selftests/scx/init_enable_count.bpf.c.rej
@@ -0,0 +1,22 @@
+--- tools/testing/selftests/scx/init_enable_count.bpf.c
++++ tools/testing/selftests/scx/init_enable_count.bpf.c
+@@ -13,6 +13,7 @@
+ char _license[] SEC("license") = "GPL";
+ 
+ u64 init_task_cnt, exit_task_cnt, enable_cnt, disable_cnt;
++u64 init_fork_cnt, init_transition_cnt;
+ volatile const bool switch_all;
+ 
+ s32 BPF_STRUCT_OPS_SLEEPABLE(cnt_init_task, struct task_struct *p,
+@@ -20,6 +21,11 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(cnt_init_task, struct task_struct *p,
+ {
+ 	__sync_fetch_and_add(&init_task_cnt, 1);
+ 
++	if (args->fork)
++		__sync_fetch_and_add(&init_fork_cnt, 1);
++	else
++		__sync_fetch_and_add(&init_transition_cnt, 1);
++
+ 	return 0;
+ }
+ 
diff --git a/tools/testing/selftests/scx/init_enable_count.c.rej b/tools/testing/selftests/scx/init_enable_count.c.rej
new file mode 100644
index 000000000..387768763
--- /dev/null
+++ b/tools/testing/selftests/scx/init_enable_count.c.rej
@@ -0,0 +1,150 @@
+--- tools/testing/selftests/scx/init_enable_count.c
++++ tools/testing/selftests/scx/init_enable_count.c
+@@ -31,7 +31,7 @@ open_load_prog(bool global)
+ 	return skel;
+ }
+ 
+-static void run_test(bool global)
++static enum scx_test_status run_test(bool global)
+ {
+ 	struct init_enable_count *skel;
+ 	struct bpf_link *link;
+@@ -42,12 +42,12 @@ static void run_test(bool global)
+ 
+ 	skel = open_load_prog(global);
+ 	link = bpf_map__attach_struct_ops(skel->maps.init_enable_count_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+ 
+ 	/* SCHED_EXT children */
+ 	for (i = 0; i < num_children; i++) {
+ 		pids[i] = fork();
+-		SCX_BUG_ON(pids[i] < 0, "Failed to fork child");
++		SCX_FAIL_IF(pids[i] < 0, "Failed to fork child");
+ 
+ 		if (pids[i] == 0) {
+ 			ret = sched_setscheduler(0, SCHED_EXT, &param);
+@@ -67,10 +67,11 @@ static void run_test(bool global)
+ 		}
+ 	}
+ 	for (i = 0; i < num_children; i++) {
+-		SCX_BUG_ON(waitpid(pids[i], &status, 0) != pids[i],
+-			   "Failed to wait for SCX child");
+-		SCX_BUG_ON(status != 0, "SCX child %d exited with status %d",
+-			   i, status);
++		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
++			    "Failed to wait for SCX child\n");
++
++		SCX_FAIL_IF(status != 0, "SCX child %d exited with status %d\n", i,
++			    status);
+ 	}
+ 
+ 	/* SCHED_OTHER children */
+@@ -79,11 +80,13 @@ static void run_test(bool global)
+ 		if (pids[i] == 0)
+ 			exit(0);
+ 	}
++
+ 	for (i = 0; i < num_children; i++) {
+-		SCX_BUG_ON(waitpid(pids[i], &status, 0) != pids[i],
+-			   "Failed to wait for normal child");
+-		SCX_BUG_ON(status != 0,
+-			   "Normal child %d exited with status %d", i, status);
++		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
++			    "Failed to wait for normal child\n");
++
++		SCX_FAIL_IF(status != 0, "Normal child %d exited with status %d\n", i,
++			    status);
+ 	}
+ 
+ 	sleep(1);
+@@ -101,14 +104,25 @@ static void run_test(bool global)
+ 
+ 	bpf_link__destroy(link);
+ 	init_enable_count__destroy(skel);
++
++	return SCX_TEST_PASS;
+ }
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status run(void *ctx)
+ {
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
++	enum scx_test_status status;
+ 
+-	run_test(true);
+-	run_test(false);
++	status = run_test(true);
++	if (status != SCX_TEST_PASS)
++		return status;
+ 
+-	return 0;
++	return run_test(false);
+ }
++
++struct scx_test init_enable_count = {
++	.name = "init_enable_count",
++	.description = "Verify we do the correct amount of counting of init, "
++		       "enable, etc callbacks.",
++	.run = run,
++};
++REGISTER_SCX_TEST(&init_enable_count)
+--- tools/testing/selftests/scx/init_enable_count.c
++++ tools/testing/selftests/scx/init_enable_count.c
+@@ -35,15 +35,39 @@ static enum scx_test_status run_test(bool global)
+ {
+ 	struct init_enable_count *skel;
+ 	struct bpf_link *link;
+-	const u32 num_children = 5;
++	const u32 num_children = 5, num_pre_forks = 1024;
+ 	int ret, i, status;
+ 	struct sched_param param = {};
+-	pid_t pids[num_children];
++	pid_t pids[num_pre_forks];
+ 
+ 	skel = open_load_prog(global);
++
++	/*
++	 * Fork a bunch of children before we attach the scheduler so that we
++	 * ensure (at least in practical terms) that there are more tasks that
++	 * transition from SCHED_OTHER -> SCHED_EXT than there are tasks that
++	 * take the fork() path either below or in other processes.
++	 */
++	for (i = 0; i < num_pre_forks; i++) {
++		pids[i] = fork();
++		SCX_FAIL_IF(pids[i] < 0, "Failed to fork child");
++		if (pids[i] == 0) {
++			sleep(1);
++			exit(0);
++		}
++	}
++
+ 	link = bpf_map__attach_struct_ops(skel->maps.init_enable_count_ops);
+ 	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+ 
++	for (i = 0; i < num_pre_forks; i++) {
++		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
++			    "Failed to wait for pre-forked child\n");
++
++		SCX_FAIL_IF(status != 0, "Pre-forked child %d exited with status %d\n", i,
++			    status);
++	}
++
+ 	/* SCHED_EXT children */
+ 	for (i = 0; i < num_children; i++) {
+ 		pids[i] = fork();
+@@ -101,6 +125,14 @@ static enum scx_test_status run_test(bool global)
+ 		SCX_EQ(skel->bss->enable_cnt, num_children);
+ 		SCX_EQ(skel->bss->disable_cnt, num_children);
+ 	}
++	/*
++	 * We forked a ton of tasks before we attached the scheduler above, so
++	 * this should be fine. Technically it could be flaky if a ton of forks
++	 * are happening at the same time in other processes, but that should
++	 * be exceedingly unlikely.
++	 */
++	SCX_GT(skel->bss->init_transition_cnt, skel->bss->init_fork_cnt);
++	SCX_GE(skel->bss->init_fork_cnt, 2 * num_children);
+ 
+ 	bpf_link__destroy(link);
+ 	init_enable_count__destroy(skel);
diff --git a/tools/testing/selftests/scx/maximal.bpf.c.rej b/tools/testing/selftests/scx/maximal.bpf.c.rej
new file mode 100644
index 000000000..b59abbfd8
--- /dev/null
+++ b/tools/testing/selftests/scx/maximal.bpf.c.rej
@@ -0,0 +1,10 @@
+--- tools/testing/selftests/scx/maximal.bpf.c
++++ tools/testing/selftests/scx/maximal.bpf.c
+@@ -6,7 +6,6 @@
+  *
+  * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+  * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+- * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+  */
+ 
+ #include <scx/common.bpf.h>
diff --git a/tools/testing/selftests/scx/maximal.c.rej b/tools/testing/selftests/scx/maximal.c.rej
new file mode 100644
index 000000000..94cb04340
--- /dev/null
+++ b/tools/testing/selftests/scx/maximal.c.rej
@@ -0,0 +1,10 @@
+--- tools/testing/selftests/scx/maximal.c
++++ tools/testing/selftests/scx/maximal.c
+@@ -2,7 +2,6 @@
+ /*
+  * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+  * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+- * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+  */
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
diff --git a/tools/testing/selftests/scx/maybe_null.bpf.c.rej b/tools/testing/selftests/scx/maybe_null.bpf.c.rej
new file mode 100644
index 000000000..bd046dda5
--- /dev/null
+++ b/tools/testing/selftests/scx/maybe_null.bpf.c.rej
@@ -0,0 +1,29 @@
+--- tools/testing/selftests/scx/maybe_null.bpf.c
++++ tools/testing/selftests/scx/maybe_null.bpf.c
+@@ -1,6 +1,6 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+- * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
++ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+  */
+ 
+ #include <scx/common.bpf.h>
+--- tools/testing/selftests/scx/maybe_null.bpf.c
++++ tools/testing/selftests/scx/maybe_null.bpf.c
+@@ -14,13 +14,13 @@ void BPF_STRUCT_OPS(maybe_null_running, struct task_struct *p)
+ 
+ void BPF_STRUCT_OPS(maybe_null_success_dispatch, s32 cpu, struct task_struct *p)
+ {
+-        if (p != NULL)
+-          vtime_test = p->scx.dsq_vtime;
++	if (p != NULL)
++		vtime_test = p->scx.dsq_vtime;
+ }
+ 
+ SEC(".struct_ops.link")
+ struct sched_ext_ops maybe_null_success = {
+-        .dispatch               = maybe_null_success_dispatch,
++	.dispatch               = maybe_null_success_dispatch,
+ 	.enable			= maybe_null_running,
+ 	.name			= "minimal",
+ };
diff --git a/tools/testing/selftests/scx/maybe_null.c.rej b/tools/testing/selftests/scx/maybe_null.c.rej
new file mode 100644
index 000000000..4a844e856
--- /dev/null
+++ b/tools/testing/selftests/scx/maybe_null.c.rej
@@ -0,0 +1,51 @@
+--- tools/testing/selftests/scx/maybe_null.c
++++ tools/testing/selftests/scx/maybe_null.c
+@@ -1,8 +1,6 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+- * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+- * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+- * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
++ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+  */
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+--- tools/testing/selftests/scx/maybe_null.c
++++ tools/testing/selftests/scx/maybe_null.c
+@@ -12,22 +12,22 @@
+ 
+ static enum scx_test_status run(void *ctx)
+ {
+-        struct maybe_null *skel;
+-        struct maybe_null_fail *fail_skel;
++	struct maybe_null *skel;
++	struct maybe_null_fail *fail_skel;
+ 
+-        skel = maybe_null__open_and_load();
+-        if (!skel) {
+-                SCX_ERR("Failed to open and load maybe_null skel");
+-                return SCX_TEST_FAIL;
+-        }
+-        maybe_null__destroy(skel);
++	skel = maybe_null__open_and_load();
++	if (!skel) {
++		SCX_ERR("Failed to open and load maybe_null skel");
++		return SCX_TEST_FAIL;
++	}
++	maybe_null__destroy(skel);
+ 
+-        fail_skel = maybe_null_fail__open_and_load();
+-        if (fail_skel) {
+-                maybe_null_fail__destroy(fail_skel);
+-                SCX_ERR("Should failed to open and load maybe_null_fail skel");
+-                return SCX_TEST_FAIL;
+-        }
++	fail_skel = maybe_null_fail__open_and_load();
++	if (fail_skel) {
++		maybe_null_fail__destroy(fail_skel);
++		SCX_ERR("Should failed to open and load maybe_null_fail skel");
++		return SCX_TEST_FAIL;
++	}
+ 
+ 	return SCX_TEST_PASS;
+ }
diff --git a/tools/testing/selftests/scx/maybe_null_fail.bpf.c.rej b/tools/testing/selftests/scx/maybe_null_fail.bpf.c.rej
new file mode 100644
index 000000000..ffcfa2a93
--- /dev/null
+++ b/tools/testing/selftests/scx/maybe_null_fail.bpf.c.rej
@@ -0,0 +1,27 @@
+--- tools/testing/selftests/scx/maybe_null_fail.bpf.c
++++ tools/testing/selftests/scx/maybe_null_fail.bpf.c
+@@ -1,6 +1,6 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+- * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
++ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+  */
+ 
+ #include <scx/common.bpf.h>
+--- tools/testing/selftests/scx/maybe_null_fail.bpf.c
++++ tools/testing/selftests/scx/maybe_null_fail.bpf.c
+@@ -14,12 +14,12 @@ void BPF_STRUCT_OPS(maybe_null_running, struct task_struct *p)
+ 
+ void BPF_STRUCT_OPS(maybe_null_fail_dispatch, s32 cpu, struct task_struct *p)
+ {
+-          vtime_test = p->scx.dsq_vtime;
++	vtime_test = p->scx.dsq_vtime;
+ }
+ 
+ SEC(".struct_ops.link")
+ struct sched_ext_ops maybe_null_fail = {
+-        .dispatch               = maybe_null_fail_dispatch,
++	.dispatch               = maybe_null_fail_dispatch,
+ 	.enable			= maybe_null_running,
+ 	.name			= "minimal",
+ };
diff --git a/tools/testing/selftests/scx/minimal.c.rej b/tools/testing/selftests/scx/minimal.c.rej
new file mode 100644
index 000000000..f100a85c2
--- /dev/null
+++ b/tools/testing/selftests/scx/minimal.c.rej
@@ -0,0 +1,85 @@
+--- tools/testing/selftests/scx/minimal.c
++++ tools/testing/selftests/scx/minimal.c
+@@ -1,42 +1,58 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
++ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
++ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
++ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
++#include <sys/wait.h>
++#include <unistd.h>
+ #include "minimal.bpf.skel.h"
++#include "scx_test.h"
+ 
+-static volatile int exit_req;
+-
+-static void sigint_handler(int simple)
++static enum scx_test_status setup(void **ctx)
+ {
+-	exit_req = 1;
++	struct minimal *skel;
++
++	skel = minimal__open_and_load();
++	if (!skel) {
++		SCX_ERR("Failed to open and load skel");
++		return SCX_TEST_FAIL;
++	}
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
+ }
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status run(void *ctx)
+ {
+-	struct minimal *skel;
++	struct minimal *skel = ctx;
+ 	struct bpf_link *link;
+ 
+-	signal(SIGINT, sigint_handler);
+-	signal(SIGTERM, sigint_handler);
++	link = bpf_map__attach_struct_ops(skel->maps.minimal_ops);
++	if (!link) {
++		SCX_ERR("Failed to attach scheduler");
++		return SCX_TEST_FAIL;
++	}
++
++	bpf_link__destroy(link);
+ 
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
++	return SCX_TEST_PASS;
++}
+ 
+-	skel = minimal__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
++static void cleanup(void *ctx)
++{
++	struct minimal *skel = ctx;
+ 
+-	link = bpf_map__attach_struct_ops(skel->maps.minimal_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
+-	sleep(1);
+-	bpf_link__destroy(link);
+ 	minimal__destroy(skel);
+-
+-	return 0;
+ }
++
++struct scx_test minimal = {
++	.name = "minimal",
++	.description = "Verify we can load a fully minimal scheduler",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&minimal)
diff --git a/tools/testing/selftests/scx/runner.c.rej b/tools/testing/selftests/scx/runner.c.rej
new file mode 100644
index 000000000..68603e3b5
--- /dev/null
+++ b/tools/testing/selftests/scx/runner.c.rej
@@ -0,0 +1,68 @@
+--- tools/testing/selftests/scx/runner.c
++++ tools/testing/selftests/scx/runner.c
+@@ -55,6 +55,8 @@ static const char *status_to_result(enum scx_test_status status)
+ 	case SCX_TEST_FAIL:
+ 		return "not ok";
+ 	}
++
++        return NULL;
+ }
+ 
+ static void print_test_result(const struct scx_test *test,
+--- tools/testing/selftests/scx/runner.c
++++ tools/testing/selftests/scx/runner.c
+@@ -21,11 +21,12 @@ const char help_fmt[] =
+ "Usage: %s [-t TEST] [-h]\n"
+ "\n"
+ "  -t TEST       Only run tests whose name includes this string\n"
++"  -s            Include print output for skipped tests\n"
+ "  -q            Don't print the test descriptions during run\n"
+ "  -h            Display this help and exit\n";
+ 
+ static volatile int exit_req;
+-static bool quiet;
++static bool quiet, print_skipped;
+ 
+ #define MAX_SCX_TESTS 2048
+ 
+@@ -132,11 +133,14 @@ int main(int argc, char **argv)
+ 
+ 	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+ 
+-	while ((opt = getopt(argc, argv, "qt:h")) != -1) {
++	while ((opt = getopt(argc, argv, "qst:h")) != -1) {
+ 		switch (opt) {
+ 		case 'q':
+ 			quiet = true;
+ 			break;
++		case 's':
++			print_skipped = true;
++			break;
+ 		case 't':
+ 			filter = optarg;
+ 			break;
+@@ -150,13 +154,21 @@ int main(int argc, char **argv)
+ 		enum scx_test_status status;
+ 		struct scx_test *test = &__scx_tests[i];
+ 
+-		print_test_preamble(test, quiet);
+-
+ 		if (filter && should_skip_test(test, filter)) {
+-			print_test_result(test, SCX_TEST_SKIP, ++testnum);
++			/*
++			 * Printing the skipped tests and their preambles can
++			 * add a lot of noise to the runner output. Printing
++			 * this is only really useful for CI, so let's skip it
++			 * by default.
++			 */
++			if (print_skipped) {
++				print_test_preamble(test, quiet);
++				print_test_result(test, SCX_TEST_SKIP, ++testnum);
++			}
+ 			continue;
+ 		}
+ 
++		print_test_preamble(test, quiet);
+ 		status = run_test(test);
+ 		print_test_result(test, status, ++testnum);
+ 		switch (status) {
diff --git a/tools/testing/selftests/scx/scx_test.h.rej b/tools/testing/selftests/scx/scx_test.h.rej
new file mode 100644
index 000000000..a11b1424d
--- /dev/null
+++ b/tools/testing/selftests/scx/scx_test.h.rej
@@ -0,0 +1,140 @@
+--- tools/testing/selftests/scx/scx_test.h
++++ tools/testing/selftests/scx/scx_test.h
+@@ -8,19 +8,103 @@
+ #ifndef __SCX_TEST_H__
+ #define __SCX_TEST_H__
+ 
++#include <errno.h>
+ #include <scx/common.h>
+ 
+-#define SCX_GT(_x, _y) SCX_BUG_ON((_x) <= (_y), "Expected %s > %s (%lu > %lu)",		\
+-				  #_x, #_y, (u64)(_x), (u64)(_y))
+-#define SCX_GE(_x, _y) SCX_BUG_ON((_x) < (_y), "Expected %s >= %s (%lu >= %lu)",	\
+-				  #_x, #_y, (u64)(_x), (u64)(_y))
+-#define SCX_LT(_x, _y) SCX_BUG_ON((_x) >= (_y), "Expected %s < %s (%lu < %lu)",		\
+-				  #_x, #_y, (u64)(_x), (u64)(_y))
+-#define SCX_LE(_x, _y) SCX_BUG_ON((_x) > (_y), "Expected %s <= %s (%lu <= %lu)",	\
+-				  #_x, #_y, (u64)(_x), (u64)(_y))
+-#define SCX_EQ(_x, _y) SCX_BUG_ON((_x) != (_y), "Expected %s == %s (%lu == %lu)",	\
+-				  #_x, #_y, (u64)(_x), (u64)(_y))
+-#define SCX_ASSERT(_x) SCX_BUG_ON(!(_x), "Expected %s to be true (%lu)",		\
+-				  #_x, (u64)(_x))
++enum scx_test_status {
++	SCX_TEST_PASS = 0,
++	SCX_TEST_SKIP,
++	SCX_TEST_FAIL,
++};
++
++struct scx_test {
++	/**
++	 * name - The name of the testcase.
++	 */
++	const char *name;
++
++	/**
++	 * description - A description of your testcase: what it tests and is
++	 * meant to validate.
++	 */
++	const char *description;
++
++	/*
++	 * setup - Setup the test.
++	 * @ctx: A pointer to a context object that will be passed to run and
++	 *	 cleanup.
++	 *
++	 * An optional callback that allows a testcase to perform setup for its
++	 * run. A test may return SCX_TEST_SKIP to skip the run.
++	 */
++	enum scx_test_status (*setup)(void **ctx);
++
++	/*
++	 * run - Run the test.
++	 * @ctx: Context set in the setup() callback. If @ctx was not set in
++	 *	 setup(), it is NULL.
++	 *
++	 * The main test. Callers should return one of:
++	 *
++	 * - SCX_TEST_PASS: Test passed
++	 * - SCX_TEST_SKIP: Test should be skipped
++	 * - SCX_TEST_FAIL: Test failed
++	 *
++	 * This callback must be defined.
++	 */
++	enum scx_test_status (*run)(void *ctx);
++
++	/*
++	 * cleanup - Perform cleanup following the test
++	 * @ctx: Context set in the setup() callback. If @ctx was not set in
++	 *	 setup(), it is NULL.
++	 *
++	 * An optional callback that allows a test to perform cleanup after
++	 * being run. This callback is run even if the run() callback returns
++	 * SCX_TEST_SKIP or SCX_TEST_FAIL. It is not run if setup() returns
++	 * SCX_TEST_SKIP or SCX_TEST_FAIL.
++	 */
++	void (*cleanup)(void *ctx);
++};
++
++void scx_test_register(struct scx_test *test);
++
++#define REGISTER_SCX_TEST(__test)			\
++	__attribute__((constructor))			\
++	static void ___scxregister##__LINE__(void)	\
++	{						\
++		scx_test_register(__test);		\
++	}
++
++#define SCX_ERR(__fmt, ...)						\
++	do {								\
++		fprintf(stderr, "ERR: %s:%d\n", __FILE__, __LINE__);	\
++		fprintf(stderr, __fmt, ##__VA_ARGS__);			\
++	} while (0)
++
++#define SCX_FAIL(__fmt, ...)						\
++	do {								\
++		SCX_ERR(__fmt, ##__VA_ARGS__);				\
++		return SCX_TEST_FAIL;					\
++	} while (0)
++
++#define SCX_FAIL_IF(__cond, __fmt, ...)					\
++	do {								\
++		if (__cond)						\
++			SCX_FAIL(__fmt, ##__VA_ARGS__);			\
++	} while (0)
++
++#define SCX_GT(_x, _y) SCX_FAIL_IF((_x) <= (_y), "Expected %s > %s (%lu > %lu)",	\
++				   #_x, #_y, (u64)(_x), (u64)(_y))
++#define SCX_GE(_x, _y) SCX_FAIL_IF((_x) < (_y), "Expected %s >= %s (%lu >= %lu)",	\
++				   #_x, #_y, (u64)(_x), (u64)(_y))
++#define SCX_LT(_x, _y) SCX_FAIL_IF((_x) >= (_y), "Expected %s < %s (%lu < %lu)",	\
++				   #_x, #_y, (u64)(_x), (u64)(_y))
++#define SCX_LE(_x, _y) SCX_FAIL_IF((_x) > (_y), "Expected %s <= %s (%lu <= %lu)",	\
++				   #_x, #_y, (u64)(_x), (u64)(_y))
++#define SCX_EQ(_x, _y) SCX_FAIL_IF((_x) != (_y), "Expected %s == %s (%lu == %lu)",	\
++				   #_x, #_y, (u64)(_x), (u64)(_y))
++#define SCX_ASSERT(_x) SCX_FAIL_IF(!(_x), "Expected %s to be true (%lu)",		\
++				   #_x, (u64)(_x))
+ 
+ #endif  // # __SCX_TEST_H__
+--- tools/testing/selftests/scx/scx_test.h
++++ tools/testing/selftests/scx/scx_test.h
+@@ -17,6 +17,19 @@ enum scx_test_status {
+ 	SCX_TEST_FAIL,
+ };
+ 
++/* Copied from include/linux/sched/ext.h */
++enum scx_test_exit_kind {
++        SCX_EXIT_NONE,
++        SCX_EXIT_DONE,
++
++        SCX_EXIT_UNREG = 64,    /* BPF unregistration */
++        SCX_EXIT_SYSRQ,         /* requested by 'S' sysrq */
++
++	SCX_EXIT_ERROR = 1024,  /* runtime error, error msg contains details */
++	SCX_EXIT_ERROR_BPF,     /* ERROR but triggered through scx_bpf_error() */
++	SCX_EXIT_ERROR_STALL,   /* watchdog detected stalled runnable tasks */
++};
++
+ struct scx_test {
+ 	/**
+ 	 * name - The name of the testcase.
diff --git a/tools/testing/selftests/scx/select_cpu_dfl.bpf.c.rej b/tools/testing/selftests/scx/select_cpu_dfl.bpf.c.rej
new file mode 100644
index 000000000..a8d7d6af4
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dfl.bpf.c.rej
@@ -0,0 +1,25 @@
+--- tools/testing/selftests/scx/select_cpu_dfl.bpf.c
++++ tools/testing/selftests/scx/select_cpu_dfl.bpf.c
+@@ -14,14 +14,20 @@ char _license[] SEC("license") = "GPL";
+ 
+ bool saw_local = false;
+ 
++static bool task_is_test(const struct task_struct *p)
++{
++	return !bpf_strncmp(p->comm, 9, "select_cpu");
++}
++
+ void BPF_STRUCT_OPS(select_cpu_dfl_enqueue, struct task_struct *p,
+ 		    u64 enq_flags)
+ {
+ 	const struct cpumask *idle_mask = scx_bpf_get_idle_cpumask();
+ 
+-	if (p->nr_cpus_allowed > 1 &&
+-	    bpf_cpumask_test_cpu(scx_bpf_task_cpu(p), idle_mask))
++	if (task_is_test(p) &&
++	    bpf_cpumask_test_cpu(scx_bpf_task_cpu(p), idle_mask)) {
+ 		saw_local = true;
++	}
+ 	scx_bpf_put_idle_cpumask(idle_mask);
+ 
+ 	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
diff --git a/tools/testing/selftests/scx/select_cpu_dfl.c.rej b/tools/testing/selftests/scx/select_cpu_dfl.c.rej
new file mode 100644
index 000000000..d1bc47d70
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dfl.c.rej
@@ -0,0 +1,77 @@
+--- tools/testing/selftests/scx/select_cpu_dfl.c
++++ tools/testing/selftests/scx/select_cpu_dfl.c
+@@ -4,32 +4,35 @@
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_dfl.bpf.skel.h"
+ #include "scx_test.h"
+ 
+ #define NUM_CHILDREN 1028
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dfl *skel;
++
++	skel = select_cpu_dfl__open_and_load();
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
++}
++
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_dfl *skel = ctx;
+ 	struct bpf_link *link;
+ 	pid_t pids[NUM_CHILDREN];
+ 	int i, status;
+ 
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+-
+-	skel = select_cpu_dfl__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
+-
+ 	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dfl_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+ 
+ 	for (i = 0; i < NUM_CHILDREN; i++) {
+ 		pids[i] = fork();
+@@ -45,8 +48,25 @@ int main(int argc, char **argv)
+ 	}
+ 
+ 	SCX_ASSERT(!skel->bss->saw_local);
++
+ 	bpf_link__destroy(link);
+-	select_cpu_dfl__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
+ }
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_dfl *skel = ctx;
++
++	select_cpu_dfl__destroy(skel);
++}
++
++struct scx_test select_cpu_dfl = {
++	.name = "select_cpu_dfl",
++	.description = "Verify the default ops.select_cpu() dispatches tasks "
++		       "when idles cores are found, and skips ops.enqueue()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_dfl)
diff --git a/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c.rej b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c.rej
new file mode 100644
index 000000000..3c92cb23f
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c.rej
@@ -0,0 +1,22 @@
+--- tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c
++++ tools/testing/selftests/scx/select_cpu_dfl_nodispatch.bpf.c
+@@ -70,8 +70,8 @@ void BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_enqueue, struct task_struct *p,
+ 	scx_bpf_dispatch(p, dsq_id, SCX_SLICE_DFL, enq_flags);
+ }
+ 
+-s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_prep_enable,
+-		   struct task_struct *p, struct scx_enable_args *args)
++s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_init_task,
++		   struct task_struct *p, struct scx_init_task_args *args)
+ {
+ 	if (bpf_task_storage_get(&task_ctx_stor, p, 0,
+ 				 BPF_LOCAL_STORAGE_GET_F_CREATE))
+@@ -91,7 +91,7 @@ SEC(".struct_ops.link")
+ struct sched_ext_ops select_cpu_dfl_nodispatch_ops = {
+ 	.select_cpu		= select_cpu_dfl_nodispatch_select_cpu,
+ 	.enqueue		= select_cpu_dfl_nodispatch_enqueue,
+-	.prep_enable		= select_cpu_dfl_nodispatch_prep_enable,
++	.init_task		= select_cpu_dfl_nodispatch_init_task,
+ 	.init			= select_cpu_dfl_nodispatch_init,
+ 	.name			= "select_cpu_dfl_nodispatch",
+ };
diff --git a/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c.rej b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c.rej
new file mode 100644
index 000000000..038642cdc
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c.rej
@@ -0,0 +1,81 @@
+--- tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c
++++ tools/testing/selftests/scx/select_cpu_dfl_nodispatch.c
+@@ -1,35 +1,38 @@
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /*
+  * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+- * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
++ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_dfl_nodispatch.bpf.skel.h"
+ #include "scx_test.h"
+ 
+ #define NUM_CHILDREN 1028
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dfl_nodispatch *skel;
++
++	skel = select_cpu_dfl_nodispatch__open_and_load();
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
++}
++
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_dfl_nodispatch *skel = ctx;
+ 	struct bpf_link *link;
+ 	pid_t pids[NUM_CHILDREN];
+ 	int i, status;
+ 
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+-
+-	skel = select_cpu_dfl_nodispatch__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
+-
+ 	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dfl_nodispatch_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+ 
+ 	for (i = 0; i < NUM_CHILDREN; i++) {
+ 		pids[i] = fork();
+@@ -45,8 +48,25 @@ int main(int argc, char **argv)
+ 	}
+ 
+ 	SCX_ASSERT(skel->bss->saw_local);
++
+ 	bpf_link__destroy(link);
+-	select_cpu_dfl_nodispatch__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
++}
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_dfl_nodispatch *skel = ctx;
++
++	select_cpu_dfl_nodispatch__destroy(skel);
+ }
++
++struct scx_test select_cpu_dfl_nodispatch = {
++	.name = "select_cpu_dfl_nodispatch",
++	.description = "Verify behavior of scx_bpf_select_cpu_dfl() in "
++		       "ops.select_cpu()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_dfl_nodispatch)
diff --git a/tools/testing/selftests/scx/select_cpu_dispatch.c.rej b/tools/testing/selftests/scx/select_cpu_dispatch.c.rej
new file mode 100644
index 000000000..ca2dc7974
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dispatch.c.rej
@@ -0,0 +1,77 @@
+--- tools/testing/selftests/scx/select_cpu_dispatch.c
++++ tools/testing/selftests/scx/select_cpu_dispatch.c
+@@ -4,32 +4,35 @@
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_dispatch.bpf.skel.h"
+ #include "scx_test.h"
+ 
+ #define NUM_CHILDREN 1028
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dispatch *skel;
++
++	skel = select_cpu_dispatch__open_and_load();
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
++}
++
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_dispatch *skel = ctx;
+ 	struct bpf_link *link;
+ 	pid_t pids[NUM_CHILDREN];
+ 	int i, status;
+ 
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+-
+-	skel = select_cpu_dispatch__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
+-
+ 	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+ 
+ 	for (i = 0; i < NUM_CHILDREN; i++) {
+ 		pids[i] = fork();
+@@ -44,9 +47,24 @@ int main(int argc, char **argv)
+ 		SCX_EQ(status, 0);
+ 	}
+ 
+-
+ 	bpf_link__destroy(link);
+-	select_cpu_dispatch__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
+ }
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_dispatch *skel = ctx;
++
++	select_cpu_dispatch__destroy(skel);
++}
++
++struct scx_test select_cpu_dispatch = {
++	.name = "select_cpu_dispatch",
++	.description = "Test direct dispatching to built-in DSQs from "
++		       "ops.select_cpu()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_dispatch)
diff --git a/tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c.rej b/tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c.rej
new file mode 100644
index 000000000..ce530c28b
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c.rej
@@ -0,0 +1,100 @@
+--- tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c
++++ tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c
+@@ -4,54 +4,55 @@
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_dispatch_bad_dsq.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-#define NUM_CHILDREN 1028
+ #define SCX_EXIT_ERROR 1024
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dispatch_bad_dsq *skel;
+-	struct bpf_link *link;
+-	pid_t pids[NUM_CHILDREN];
+-	int i, status;
+-
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+ 
+ 	skel = select_cpu_dispatch_bad_dsq__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
+ 
+-	/*
+-	 * The scheduler is expected to gracefully exit after bad_dsqoneously
+-	 * double-dispatching from ops.selec_cpu().
+-	 */
+-	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_bad_dsq_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	return SCX_TEST_PASS;
++}
+ 
+-	for (i = 0; i < NUM_CHILDREN; i++) {
+-		pids[i] = fork();
+-		if (pids[i] == 0) {
+-			sleep(1);
+-			exit(0);
+-		}
+-	}
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_dispatch_bad_dsq *skel = ctx;
++	struct bpf_link *link;
+ 
+-	for (i = 0; i < NUM_CHILDREN; i++) {
+-		SCX_EQ(waitpid(pids[i], &status, 0), pids[i]);
+-		SCX_EQ(status, 0);
+-	}
++	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_bad_dsq_ops);
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
++
++	sleep(1);
+ 
+ 	SCX_EQ(skel->bss->uei.kind, SCX_EXIT_ERROR);
+ 	bpf_link__destroy(link);
+-	select_cpu_dispatch_bad_dsq__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
++}
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_dispatch_bad_dsq *skel = ctx;
++
++	select_cpu_dispatch_bad_dsq__destroy(skel);
+ }
++
++struct scx_test select_cpu_dispatch_bad_dsq = {
++	.name = "select_cpu_dispatch_bad_dsq",
++	.description = "Verify graceful failure if we direct-dispatch to a "
++		       "bogus DSQ in ops.select_cpu()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_dispatch_bad_dsq)
+--- tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c
++++ tools/testing/selftests/scx/select_cpu_dispatch_bad_dsq.c
+@@ -11,8 +11,6 @@
+ #include "select_cpu_dispatch_bad_dsq.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-#define SCX_EXIT_ERROR 1024
+-
+ static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dispatch_bad_dsq *skel;
diff --git a/tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c.rej b/tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c.rej
new file mode 100644
index 000000000..5d3723eb8
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c.rej
@@ -0,0 +1,100 @@
+--- tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c
++++ tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c
+@@ -4,54 +4,55 @@
+  * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <signal.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_dispatch_dbl_dsp.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-#define NUM_CHILDREN 1028
+ #define SCX_EXIT_ERROR 1024
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dispatch_dbl_dsp *skel;
+-	struct bpf_link *link;
+-	pid_t pids[NUM_CHILDREN];
+-	int i, status;
+-
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+ 
+ 	skel = select_cpu_dispatch_dbl_dsp__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
+ 
+-	/*
+-	 * The scheduler is expected to gracefully exit after
+-	 * double-dispatching from ops.select_cpu().
+-	 */
+-	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_dbl_dsp_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	return SCX_TEST_PASS;
++}
+ 
+-	for (i = 0; i < NUM_CHILDREN; i++) {
+-		pids[i] = fork();
+-		if (pids[i] == 0) {
+-			sleep(1);
+-			exit(0);
+-		}
+-	}
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_dispatch_dbl_dsp *skel = ctx;
++	struct bpf_link *link;
+ 
+-	for (i = 0; i < NUM_CHILDREN; i++) {
+-		SCX_EQ(waitpid(pids[i], &status, 0), pids[i]);
+-		SCX_EQ(status, 0);
+-	}
++	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_dbl_dsp_ops);
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
++
++	sleep(1);
+ 
+ 	SCX_EQ(skel->bss->uei.kind, SCX_EXIT_ERROR);
+ 	bpf_link__destroy(link);
+-	select_cpu_dispatch_dbl_dsp__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
++}
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_dispatch_dbl_dsp *skel = ctx;
++
++	select_cpu_dispatch_dbl_dsp__destroy(skel);
+ }
++
++struct scx_test select_cpu_dispatch_dbl_dsp = {
++	.name = "select_cpu_dispatch_dbl_dsp",
++	.description = "Verify graceful failure if we dispatch twice to a "
++		       "DSQ in ops.select_cpu()",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_dispatch_dbl_dsp)
+--- tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c
++++ tools/testing/selftests/scx/select_cpu_dispatch_dbl_dsp.c
+@@ -11,8 +11,6 @@
+ #include "select_cpu_dispatch_dbl_dsp.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-#define SCX_EXIT_ERROR 1024
+-
+ static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_dispatch_dbl_dsp *skel;
diff --git a/tools/testing/selftests/scx/select_cpu_vtime.c.rej b/tools/testing/selftests/scx/select_cpu_vtime.c.rej
new file mode 100644
index 000000000..235092bf2
--- /dev/null
+++ b/tools/testing/selftests/scx/select_cpu_vtime.c.rej
@@ -0,0 +1,70 @@
+--- tools/testing/selftests/scx/select_cpu_vtime.c
++++ tools/testing/selftests/scx/select_cpu_vtime.c
+@@ -4,36 +4,56 @@
+  * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+  * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+  */
+-#include <stdio.h>
+-#include <unistd.h>
+-#include <libgen.h>
+ #include <bpf/bpf.h>
+ #include <scx/common.h>
+ #include <sys/wait.h>
++#include <unistd.h>
+ #include "select_cpu_vtime.bpf.skel.h"
+ #include "scx_test.h"
+ 
+-int main(int argc, char **argv)
++static enum scx_test_status setup(void **ctx)
+ {
+ 	struct select_cpu_vtime *skel;
+-	struct bpf_link *link;
+-
+-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+ 
+ 	skel = select_cpu_vtime__open_and_load();
+-	SCX_BUG_ON(!skel, "Failed to open and load skel");
++	SCX_FAIL_IF(!skel, "Failed to open and load skel");
++	*ctx = skel;
++
++	return SCX_TEST_PASS;
++}
++
++static enum scx_test_status run(void *ctx)
++{
++	struct select_cpu_vtime *skel = ctx;
++	struct bpf_link *link;
+ 
+ 	SCX_ASSERT(!skel->bss->consumed);
+ 
+ 	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_vtime_ops);
+-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
++	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+ 
+ 	sleep(1);
+ 
+ 	SCX_ASSERT(skel->bss->consumed);
+ 
+ 	bpf_link__destroy(link);
+-	select_cpu_vtime__destroy(skel);
+ 
+-	return 0;
++	return SCX_TEST_PASS;
++}
++
++static void cleanup(void *ctx)
++{
++	struct select_cpu_vtime *skel = ctx;
++
++	select_cpu_vtime__destroy(skel);
+ }
++
++struct scx_test select_cpu_vtime = {
++	.name = "select_cpu_vtime",
++	.description = "Test doing direct vtime-dispatching from "
++		       "ops.select_cpu(), to a non-built-in DSQ",
++	.setup = setup,
++	.run = run,
++	.cleanup = cleanup,
++};
++REGISTER_SCX_TEST(&select_cpu_vtime)
-- 
2.43.0.232.ge79552d197

