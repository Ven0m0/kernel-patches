From 14e3e5d8b47cbd110b3339909424447110105a10 Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Sat, 3 Feb 2024 09:37:02 -1000
Subject: [PATCH 121/150] scx: Implement scx_bpf_dispatch_cancel()

This can be used to implement interlocking against dequeue. e.g, Let's say
there is one dsq per CPU and the scheduler can stall if a task is dispatched
to a dsq which doesn't intersect with p->cpus_ptr. In the dispatch path, if
you do the following:

	if (bpf_cpumask_test_cpu(target_cpu, p->cpus_ptr))
		scx_bpf_dispatch(p, target_dsq, slice, 0);

The task can still end up in the wrong dsq because the cpumask can change
between bpf_cpumask_test_cpu() and scx_bpf_dispatch(). However,

	scx_bpf_dispatch(p, target_dsq, slice, 0);
	if (!bpf_cpumask_test_cpu(target_cpu, p->cpus_ptr))
		scx_bpf_dispatch_cancel();

eliminates the race window - either the set_cpumask path sees the task
dispatched or the dispatch path sees the updated cpumask. In both cases, the
dispatch gets canceled as it should.

Note that in schedules which don't implement .dequeue(), depending on the
implementation, there can be multiple enqueued instances of the same task.
However, as long as the scheduler guarantees that there is at least one
dispatch attempt on the task since the last enqueueing and that dispatch
attempt interlocks against the dequeue path as above, it's guaranteed to not
act upon stale information or miss dispatching an enqueued task.

Signed-off-by: Tejun Heo <tj@kernel.org>
Cc: Andrea Righi <andrea.righi@canonical.com>
---
 kernel/sched/ext.c | 20 ++++++++++++++++++++
 1 file changed, 20 insertions(+)

diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index 21391ddff..b866a975d 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -4689,6 +4689,25 @@ __bpf_kfunc static u32 scx_bpf_dispatch_nr_slots(void)
 	return scx_dsp_max_batch - __this_cpu_read(scx_dsp_ctx.buf_cursor);
 }
 
+/**
+ * scx_bpf_dispatch_cancel - Cancel the latest dispatch
+ *
+ * Cancel the latest dispatch. Can be called multiple times to cancel further
+ * dispatches. Can only be called from ops.dispatch().
+ */
+void scx_bpf_dispatch_cancel(void)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(&scx_dsp_ctx);
+
+	if (!scx_kf_allowed(SCX_KF_DISPATCH))
+		return;
+
+	if (dspc->buf_cursor > 0)
+		dspc->buf_cursor--;
+	else
+		scx_ops_error("dispatch buffer underflow");
+}
+
 /**
  * scx_bpf_consume - Transfer a task from a DSQ to the current CPU's local DSQ
  * @dsq_id: DSQ to consume
@@ -4736,6 +4755,7 @@ __bpf_kfunc static bool scx_bpf_consume(u64 dsq_id)
 
 BTF_SET8_START(scx_kfunc_ids_dispatch)
 BTF_ID_FLAGS(func, scx_bpf_dispatch_nr_slots)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_cancel)
 BTF_ID_FLAGS(func, scx_bpf_consume)
 BTF_SET8_END(scx_kfunc_ids_dispatch)
 
-- 
2.43.0.232.ge79552d197

