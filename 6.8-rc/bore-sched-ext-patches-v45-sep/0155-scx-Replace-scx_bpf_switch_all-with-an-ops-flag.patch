From e7a6d52b2e0c7ffe831430bb96606ff105bbde2f Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Tue, 5 Mar 2024 14:27:36 -1000
Subject: [PATCH 155/162] scx: Replace scx_bpf_switch_all() with an ops flag

Now that struct_ops can be modified before loading, replace
scx_bpf_switch_all() with new SCX_OPS_SWITCH_PARTIAL flag.
---
 include/linux/sched/ext.h | 27 ++++++++++++++++-----------
 kernel/sched/ext.c        | 38 +++++---------------------------------
 2 files changed, 21 insertions(+), 44 deletions(-)

diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
index 53751824a..6aa8132a8 100644
--- a/include/linux/sched/ext.h
+++ b/include/linux/sched/ext.h
@@ -88,10 +88,16 @@ struct scx_exit_info {
 
 /* sched_ext_ops.flags */
 enum scx_ops_flags {
+	/*
+	 * If set, only tasks with policy set to SCHED_EXT are attached to
+	 * sched_ext. If clear, SCHED_NORMAL tasks are also included.
+	 */
+	SCX_OPS_SWITCH_PARTIAL	= 1LLU << 0,
+
 	/*
 	 * Keep built-in idle tracking even if ops.update_idle() is implemented.
 	 */
-	SCX_OPS_KEEP_BUILTIN_IDLE = 1LLU << 0,
+	SCX_OPS_KEEP_BUILTIN_IDLE = 1LLU << 1,
 
 	/*
 	 * By default, if there are no other task to run on the CPU, ext core
@@ -99,7 +105,7 @@ enum scx_ops_flags {
 	 * flag is specified, such tasks are passed to ops.enqueue() with
 	 * %SCX_ENQ_LAST. See the comment above %SCX_ENQ_LAST for more info.
 	 */
-	SCX_OPS_ENQ_LAST	= 1LLU << 1,
+	SCX_OPS_ENQ_LAST	= 1LLU << 2,
 
 	/*
 	 * An exiting task may schedule after PF_EXITING is set. In such cases,
@@ -112,7 +118,7 @@ enum scx_ops_flags {
 	 * depend on pid lookups and wants to handle these tasks directly, the
 	 * following flag can be used.
 	 */
-	SCX_OPS_ENQ_EXITING	= 1LLU << 2,
+	SCX_OPS_ENQ_EXITING	= 1LLU << 3,
 
 	/*
 	 * CPU cgroup knob enable flags
@@ -655,16 +661,15 @@ enum scx_ent_dsq_flags {
  */
 enum scx_kf_mask {
 	SCX_KF_UNLOCKED		= 0,	  /* not sleepable, not rq locked */
-	/* all non-sleepables may be nested inside INIT and SLEEPABLE */
-	SCX_KF_INIT		= 1 << 0, /* running ops.init() */
-	SCX_KF_SLEEPABLE	= 1 << 1, /* other sleepable init operations */
+	/* all non-sleepables may be nested inside SLEEPABLE */
+	SCX_KF_SLEEPABLE	= 1 << 0, /* sleepable init operations */
 	/* ENQUEUE and DISPATCH may be nested inside CPU_RELEASE */
-	SCX_KF_CPU_RELEASE	= 1 << 2, /* ops.cpu_release() */
+	SCX_KF_CPU_RELEASE	= 1 << 1, /* ops.cpu_release() */
 	/* ops.dequeue (in REST) may be nested inside DISPATCH */
-	SCX_KF_DISPATCH		= 1 << 3, /* ops.dispatch() */
-	SCX_KF_ENQUEUE		= 1 << 4, /* ops.enqueue() and ops.select_cpu() */
-	SCX_KF_SELECT_CPU	= 1 << 5, /* ops.select_cpu() */
-	SCX_KF_REST		= 1 << 6, /* other rq-locked operations */
+	SCX_KF_DISPATCH		= 1 << 2, /* ops.dispatch() */
+	SCX_KF_ENQUEUE		= 1 << 3, /* ops.enqueue() and ops.select_cpu() */
+	SCX_KF_SELECT_CPU	= 1 << 4, /* ops.select_cpu() */
+	SCX_KF_REST		= 1 << 5, /* other rq-locked operations */
 
 	__SCX_KF_RQ_LOCKED	= SCX_KF_CPU_RELEASE | SCX_KF_DISPATCH |
 				  SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index d34271845..07e6d71a3 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -101,7 +101,6 @@ DEFINE_STATIC_KEY_FALSE(__scx_ops_enabled);
 DEFINE_STATIC_PERCPU_RWSEM(scx_fork_rwsem);
 static atomic_t scx_ops_enable_state_var = ATOMIC_INIT(SCX_OPS_DISABLED);
 static atomic_t scx_ops_bypass_depth = ATOMIC_INIT(0);
-static bool scx_switch_all_req;
 static bool scx_switching_all;
 DEFINE_STATIC_KEY_FALSE(__scx_switched_all);
 
@@ -331,8 +330,7 @@ static __always_inline bool scx_kf_allowed(u32 mask)
 		return false;
 	}
 
-	if (unlikely((mask & (SCX_KF_INIT | SCX_KF_SLEEPABLE)) &&
-		     in_interrupt())) {
+	if (unlikely((mask & SCX_KF_SLEEPABLE) && in_interrupt())) {
 		scx_ops_error("sleepable kfunc called from non-sleepable context");
 		return false;
 	}
@@ -3706,9 +3704,8 @@ static int scx_ops_enable(struct sched_ext_ops *ops)
 	 */
 	cpus_read_lock();
 
-	scx_switch_all_req = false;
 	if (scx_ops.init) {
-		ret = SCX_CALL_OP_RET(SCX_KF_INIT, init);
+		ret = SCX_CALL_OP_RET(SCX_KF_SLEEPABLE, init);
 		if (ret) {
 			ret = ops_sanitize_err("init", ret);
 			goto err_disable_unlock_cpus;
@@ -3855,7 +3852,7 @@ static int scx_ops_enable(struct sched_ext_ops *ops)
 	 * transitions here are synchronized against sched_ext_free() through
 	 * scx_tasks_lock.
 	 */
-	WRITE_ONCE(scx_switching_all, scx_switch_all_req);
+	WRITE_ONCE(scx_switching_all, !(ops->flags & SCX_OPS_SWITCH_PARTIAL));
 
 	scx_task_iter_init(&sti);
 	while ((p = scx_task_iter_next_filtered_locked(&sti))) {
@@ -3887,7 +3884,7 @@ static int scx_ops_enable(struct sched_ext_ops *ops)
 		goto err_disable;
 	}
 
-	if (scx_switch_all_req)
+	if (!(ops->flags & SCX_OPS_SWITCH_PARTIAL))
 		static_branch_enable(&__scx_switched_all);
 
 	kobject_uevent(scx_root_kobj, KOBJ_ADD);
@@ -4491,29 +4488,6 @@ void __init init_sched_ext_class(void)
 
 __bpf_kfunc_start_defs();
 
-/**
- * scx_bpf_switch_all - Switch all tasks into SCX
- *
- * Switch all existing and future non-dl/rt tasks to SCX. This can only be
- * called from ops.init(), and actual switching is performed asynchronously.
- */
-__bpf_kfunc void scx_bpf_switch_all(void)
-{
-	if (!scx_kf_allowed(SCX_KF_INIT))
-		return;
-
-	scx_switch_all_req = true;
-}
-
-BTF_KFUNCS_START(scx_kfunc_ids_init)
-BTF_ID_FLAGS(func, scx_bpf_switch_all)
-BTF_KFUNCS_END(scx_kfunc_ids_init)
-
-static const struct btf_kfunc_id_set scx_kfunc_set_init = {
-	.owner			= THIS_MODULE,
-	.set			= &scx_kfunc_ids_init,
-};
-
 /**
  * scx_bpf_create_dsq - Create a custom DSQ
  * @dsq_id: DSQ to create
@@ -4524,7 +4498,7 @@ static const struct btf_kfunc_id_set scx_kfunc_set_init = {
  */
 __bpf_kfunc s32 scx_bpf_create_dsq(u64 dsq_id, s32 node)
 {
-	if (!scx_kf_allowed(SCX_KF_INIT | SCX_KF_SLEEPABLE))
+	if (!scx_kf_allowed(SCX_KF_SLEEPABLE))
 		return -EINVAL;
 
 	if (unlikely(node >= (int)nr_node_ids ||
@@ -5240,8 +5214,6 @@ static int __init scx_init(void)
 	 * check using scx_kf_allowed().
 	 */
 	if ((ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
-					     &scx_kfunc_set_init)) ||
-	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
 					     &scx_kfunc_set_sleepable)) ||
 	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
 					     &scx_kfunc_set_enqueue_dispatch)) ||
-- 
2.43.0.232.ge79552d197

