From c49e3bf5189e1f99f3945bcd67663e2d26e0a902 Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Fri, 2 Feb 2024 08:53:29 -1000
Subject: [PATCH 118/155] scx: Purge most schedulers from tools/sched_ext

The source of truth is https://github.com/sched-ext/scx. Let's just leave
the very simple two - scx_simple and scx_qmap.

Signed-off-by: Tejun Heo <tj@kernel.org>
---
 tools/sched_ext/Makefile                      |   62 +-
 tools/sched_ext/README.md                     |  138 +-
 tools/sched_ext/scx_central.bpf.c             |  367 ----
 tools/sched_ext/scx_central.c                 |  127 --
 tools/sched_ext/scx_flatcg.bpf.c              |  960 ----------
 tools/sched_ext/scx_flatcg.c                  |  225 ---
 tools/sched_ext/scx_flatcg.h                  |   51 -
 tools/sched_ext/scx_layered/.gitignore        |    3 -
 tools/sched_ext/scx_layered/Cargo.toml        |   28 -
 tools/sched_ext/scx_layered/README.md         |   37 -
 tools/sched_ext/scx_layered/build.rs          |   13 -
 tools/sched_ext/scx_layered/rustfmt.toml      |    8 -
 tools/sched_ext/scx_layered/src/bpf/intf.h    |  100 -
 .../sched_ext/scx_layered/src/bpf/main.bpf.c  |  979 ----------
 .../sched_ext/scx_layered/src/bpf/util.bpf.c  |   68 -
 tools/sched_ext/scx_layered/src/bpf_intf.rs   |   10 -
 tools/sched_ext/scx_layered/src/bpf_skel.rs   |   12 -
 tools/sched_ext/scx_layered/src/main.rs       | 1639 -----------------
 tools/sched_ext/scx_pair.bpf.c                |  626 -------
 tools/sched_ext/scx_pair.c                    |  169 --
 tools/sched_ext/scx_pair.h                    |    9 -
 tools/sched_ext/scx_rusty/.gitignore          |    3 -
 tools/sched_ext/scx_rusty/Cargo.toml          |   27 -
 tools/sched_ext/scx_rusty/README.md           |   36 -
 tools/sched_ext/scx_rusty/build.rs            |   13 -
 tools/sched_ext/scx_rusty/rustfmt.toml        |    8 -
 tools/sched_ext/scx_rusty/src/bpf/intf.h      |   97 -
 tools/sched_ext/scx_rusty/src/bpf/main.bpf.c  | 1164 ------------
 tools/sched_ext/scx_rusty/src/bpf_intf.rs     |   10 -
 tools/sched_ext/scx_rusty/src/bpf_skel.rs     |   12 -
 tools/sched_ext/scx_rusty/src/main.rs         | 1271 -------------
 tools/sched_ext/scx_userland.bpf.c            |  349 ----
 tools/sched_ext/scx_userland.c                |  423 -----
 tools/sched_ext/scx_userland.h                |   17 -
 34 files changed, 8 insertions(+), 9053 deletions(-)
 delete mode 100644 tools/sched_ext/scx_central.bpf.c
 delete mode 100644 tools/sched_ext/scx_central.c
 delete mode 100644 tools/sched_ext/scx_flatcg.bpf.c
 delete mode 100644 tools/sched_ext/scx_flatcg.c
 delete mode 100644 tools/sched_ext/scx_flatcg.h
 delete mode 100644 tools/sched_ext/scx_layered/.gitignore
 delete mode 100644 tools/sched_ext/scx_layered/Cargo.toml
 delete mode 100644 tools/sched_ext/scx_layered/README.md
 delete mode 100644 tools/sched_ext/scx_layered/build.rs
 delete mode 100644 tools/sched_ext/scx_layered/rustfmt.toml
 delete mode 100644 tools/sched_ext/scx_layered/src/bpf/intf.h
 delete mode 100644 tools/sched_ext/scx_layered/src/bpf/main.bpf.c
 delete mode 100644 tools/sched_ext/scx_layered/src/bpf/util.bpf.c
 delete mode 100644 tools/sched_ext/scx_layered/src/bpf_intf.rs
 delete mode 100644 tools/sched_ext/scx_layered/src/bpf_skel.rs
 delete mode 100644 tools/sched_ext/scx_layered/src/main.rs
 delete mode 100644 tools/sched_ext/scx_pair.bpf.c
 delete mode 100644 tools/sched_ext/scx_pair.c
 delete mode 100644 tools/sched_ext/scx_pair.h
 delete mode 100644 tools/sched_ext/scx_rusty/.gitignore
 delete mode 100644 tools/sched_ext/scx_rusty/Cargo.toml
 delete mode 100644 tools/sched_ext/scx_rusty/README.md
 delete mode 100644 tools/sched_ext/scx_rusty/build.rs
 delete mode 100644 tools/sched_ext/scx_rusty/rustfmt.toml
 delete mode 100644 tools/sched_ext/scx_rusty/src/bpf/intf.h
 delete mode 100644 tools/sched_ext/scx_rusty/src/bpf/main.bpf.c
 delete mode 100644 tools/sched_ext/scx_rusty/src/bpf_intf.rs
 delete mode 100644 tools/sched_ext/scx_rusty/src/bpf_skel.rs
 delete mode 100644 tools/sched_ext/scx_rusty/src/main.rs
 delete mode 100644 tools/sched_ext/scx_userland.bpf.c
 delete mode 100644 tools/sched_ext/scx_userland.c
 delete mode 100644 tools/sched_ext/scx_userland.h

diff --git a/tools/sched_ext/Makefile b/tools/sched_ext/Makefile
index 7db68d205..731bf6278 100644
--- a/tools/sched_ext/Makefile
+++ b/tools/sched_ext/Makefile
@@ -181,11 +181,7 @@ $(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BP
 
 SCX_COMMON_DEPS := include/scx/common.h include/scx/user_exit_info.h | $(BINDIR)
 
-################
-# C schedulers #
-################
-c-sched-targets = scx_simple scx_qmap scx_central scx_pair scx_flatcg		\
-		  scx_userland
+c-sched-targets = scx_simple scx_qmap
 
 $(addprefix $(BINDIR)/,$(c-sched-targets)): \
 	$(BINDIR)/%: \
@@ -195,39 +191,14 @@ $(addprefix $(BINDIR)/,$(c-sched-targets)): \
 	$(eval sched=$(notdir $@))
 	$(CC) $(CFLAGS) -c $(sched).c -o $(SCXOBJ_DIR)/$(sched).o
 	$(CC) -o $@ $(SCXOBJ_DIR)/$(sched).o $(HOST_BPFOBJ) $(LDFLAGS)
-$(c-sched-targets): %: $(BINDIR)/%
-
 
-###################
-# Rust schedulers #
-###################
-rust-sched-targets := scx_rusty scx_layered
-
-# Separate build target that is available for build systems to use to fetch
-# dependencies in a separate step from building. This allows the scheduler
-# to be compiled without network access.
-#
-# If the regular rust scheduler Make target (e.g. scx_rusty) is invoked without
-# CARGO_OFFLINE=1 (e.g. if building locally), then cargo build will download
-# all of the necessary dependencies, and the deps target can be skipped.
-$(addsuffix _deps,$(rust-sched-targets)):
-	$(eval sched=$(@:_deps=))
-	$(Q)cargo fetch --manifest-path=$(sched)/Cargo.toml
-
-$(rust-sched-targets): %: $(INCLUDE_DIR)/vmlinux.h $(SCX_COMMON_DEPS)
-	$(eval export RUSTFLAGS = -C link-args=-lzstd -C link-args=-lz -C link-args=-lelf -L $(BPFOBJ_DIR))
-	$(eval export BPF_CLANG = $(CLANG))
-	$(eval export BPF_CFLAGS = $(BPF_CFLAGS))
-	$(eval sched=$(notdir $@))
-	$(Q)cargo build --manifest-path=$(sched)/Cargo.toml $(CARGOFLAGS)
-	$(Q)cp $(OUTPUT_DIR)/release/$(sched) $(BINDIR)/$@
+$(c-sched-targets): %: $(BINDIR)/%
 
 install: all
 	$(Q)mkdir -p $(DESTDIR)/usr/local/bin/
 	$(Q)cp $(BINDIR)/* $(DESTDIR)/usr/local/bin/
 
 clean:
-	$(foreach sched,$(rust-sched-targets),cargo clean --manifest-path=$(sched)/Cargo.toml;)
 	rm -rf $(OUTPUT_DIR) $(HOST_OUTPUT_DIR)
 	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
 	rm -f $(c-sched-targets)
@@ -240,7 +211,7 @@ help:
 	@echo   ''
 	@echo   'Alternatively, you may compile individual schedulers:'
 	@echo   ''
-	@printf '  %s\n' $(c-sched-targets) $(rust-sched-targets)
+	@printf '  %s\n' $(c-sched-targets)
 	@echo   ''
 	@echo   'For any scheduler build target, you may specify an alternative'
 	@echo   'build output path with the O= environment variable. For example:'
@@ -251,26 +222,6 @@ help:
 	@echo   '/tmp/sched_ext/build.'
 	@echo   ''
 	@echo   ''
-	@echo   'Rust scheduler targets'
-	@echo   '======================'
-	@echo   ''
-	@printf '  %s\n' $(rust-sched-targets)
-	@printf '  %s_deps\n' $(rust-sched-targets)
-	@echo   ''
-	@echo   'For any rust schedulers built with cargo, you can specify'
-	@echo   'CARGO_OFFLINE=1 to ensure the build portion does not access the'
-	@echo   'network (e.g. if the scheduler is being packaged).'
-	@echo   ''
-	@echo   'For such use cases, the build workflow will look something like this:'
-	@echo   ''
-	@echo   '   make scx_rusty_deps'
-	@echo   '   CARGO_OFFLINE=1 make scx_rusty'
-	@echo   ''
-	@echo   'If network access during build is allowed, you can just make scx_rusty'
-	@echo   'directly without CARGO_OFFLINE, and dependencies will be downloaded'
-	@echo   'during the build step.'
-	@echo   ''
-	@echo   ''
 	@echo   'Installing targets'
 	@echo   '=================='
 	@echo   ''
@@ -287,12 +238,11 @@ help:
 	@echo   'Cleaning targets'
 	@echo   '================'
 	@echo   ''
-	@echo   '  clean		  - Remove all generated files, including intermediate'
-	@echo   '                    rust files for rust schedulers.'
+	@echo   '  clean		  - Remove all generated files'
 
-all_targets: $(c-sched-targets) $(rust-sched-targets)
+all_targets: $(c-sched-targets)
 
-.PHONY: all all_targets $(c-sched-targets) $(rust-sched-targets) clean help
+.PHONY: all all_targets $(c-sched-targets) clean help
 
 # delete failed targets
 .DELETE_ON_ERROR:
diff --git a/tools/sched_ext/README.md b/tools/sched_ext/README.md
index 8e7194ada..407327494 100644
--- a/tools/sched_ext/README.md
+++ b/tools/sched_ext/README.md
@@ -156,8 +156,8 @@ $ make -j($nproc)
 
 # Schedulers
 
-This section lists, in alphabetical order, all of the current example
-schedulers.
+This directory contains the following simple schedulers as examples. For
+more, visit https://github.com/sched-ext/scx.
 
 --------------------------------------------------------------------------------
 
@@ -204,140 +204,6 @@ No
 
 --------------------------------------------------------------------------------
 
-## scx_central
-
-### Overview
-
-A "central" scheduler where scheduling decisions are made from a single CPU.
-This scheduler illustrates how scheduling decisions can be dispatched from a
-single CPU, allowing other cores to run with infinite slices, without timer
-ticks, and without having to incur the overhead of making scheduling decisions.
-
-### Typical Use Case
-
-This scheduler could theoretically be useful for any workload that benefits
-from minimizing scheduling overhead and timer ticks. An example of where this
-could be particularly useful is running VMs, where running with infinite slices
-and no timer ticks allows the VM to avoid unnecessary expensive vmexits.
-
-### Production Ready?
-
-Not yet. While tasks are run with an infinite slice (SCX_SLICE_INF), they're
-preempted every 20ms in a timer callback. The scheduler also puts the core
-schedling logic inside of the central / scheduling CPU's ops.dispatch() path,
-and does not yet have any kind of priority mechanism.
-
---------------------------------------------------------------------------------
-
-## scx_pair
-
-### Overview
-
-A sibling scheduler which ensures that tasks will only ever be co-located on a
-physical core if they're in the same cgroup. It illustrates how a scheduling
-policy could be implemented to mitigate CPU bugs, such as L1TF, and also shows
-how some useful kfuncs such as `scx_bpf_kick_cpu()` can be utilized.
-
-### Typical Use Case
-
-While this scheduler is only meant to be used to illustrate certain sched_ext
-features, with a bit more work (e.g. by adding some form of priority handling
-inside and across cgroups), it could have been used as a way to quickly
-mitigate L1TF before core scheduling was implemented and rolled out.
-
-### Production Ready?
-
-No
-
---------------------------------------------------------------------------------
-
-## scx_flatcg
-
-### Overview
-
-A flattened cgroup hierarchy scheduler. This scheduler implements hierarchical
-weight-based cgroup CPU control by flattening the cgroup hierarchy into a
-single layer, by compounding the active weight share at each level. The effect
-of this is a much more performant CPU controller, which does not need to
-descend down cgroup trees in order to properly compute a cgroup's share.
-
-### Typical Use Case
-
-This scheduler could be useful for any typical workload requiring a CPU
-controller, but which cannot tolerate the higher overheads of the fair CPU
-controller.
-
-### Production Ready?
-
-Yes, though the scheduler (currently) does not adequately accommodate
-thundering herds of cgroups. If, for example, many cgroups which are nested
-behind a low-priority cgroup were to wake up around the same time, they may be
-able to consume more CPU cycles than they are entitled to.
-
---------------------------------------------------------------------------------
-
-## scx_userland
-
-### Overview
-
-A simple weighted vtime scheduler where all scheduling decisions take place in
-user space. This is in contrast to Rusty, where load balancing lives in user
-space, but scheduling decisions are still made in the kernel.
-
-### Typical Use Case
-
-There are many advantages to writing schedulers in user space. For example, you
-can use a debugger, you can write the scheduler in Rust, and you can use data
-structures bundled with your favorite library.
-
-On the other hand, user space scheduling can be hard to get right. You can
-potentially deadlock due to not scheduling a task that's required for the
-scheduler itself to make forward progress (though the sched_ext watchdog will
-protect the system by unloading your scheduler after a timeout if that
-happens). You also have to bootstrap some communication protocol between the
-kernel and user space.
-
-A more robust solution to this would be building a user space scheduling
-framework that abstracts much of this complexity away from you.
-
-### Production Ready?
-
-No. This scheduler uses an ordered list for vtime scheduling, and is stricly
-less performant than just using something like `scx_simple`. It is purely
-meant to illustrate that it's possible to build a user space scheduler on
-top of sched_ext.
-
---------------------------------------------------------------------------------
-
-## scx_rusty
-
-### Overview
-
-A multi-domain, BPF / user space hybrid scheduler. The BPF portion of the
-scheduler does a simple round robin in each domain, and the user space portion
-(written in Rust) calculates the load factor of each domain, and informs BPF of
-how tasks should be load balanced accordingly.
-
-### Typical Use Case
-
-Rusty is designed to be flexible, and accommodate different architectures and
-workloads. Various load balancing thresholds (e.g. greediness, frequenty, etc),
-as well as how Rusty should partition the system into scheduling domains, can
-be tuned to achieve the optimal configuration for any given system or workload.
-
-### Production Ready?
-
-Yes. If tuned correctly, rusty should be performant across various CPU
-architectures and workloads. Rusty by default creates a separate scheduling
-domain per-LLC, so its default configuration may be performant as well.
-
-That said, you may run into an issue with infeasible weights, where a task with
-a very high weight may cause the scheduler to incorrectly leave cores idle
-because it thinks they're necessary to accommodate the compute for a single
-task. This can also happen in CFS, and should soon be addressed for rusty.
-
---------------------------------------------------------------------------------
-
 # Troubleshooting
 
 There are a number of common issues that you may run into when building the
diff --git a/tools/sched_ext/scx_central.bpf.c b/tools/sched_ext/scx_central.bpf.c
deleted file mode 100644
index 51ddb0a14..000000000
--- a/tools/sched_ext/scx_central.bpf.c
+++ /dev/null
@@ -1,367 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * A central FIFO sched_ext scheduler which demonstrates the followings:
- *
- * a. Making all scheduling decisions from one CPU:
- *
- *    The central CPU is the only one making scheduling decisions. All other
- *    CPUs kick the central CPU when they run out of tasks to run.
- *
- *    There is one global BPF queue and the central CPU schedules all CPUs by
- *    dispatching from the global queue to each CPU's local dsq from dispatch().
- *    This isn't the most straightforward. e.g. It'd be easier to bounce
- *    through per-CPU BPF queues. The current design is chosen to maximally
- *    utilize and verify various SCX mechanisms such as LOCAL_ON dispatching.
- *
- * b. Tickless operation
- *
- *    All tasks are dispatched with the infinite slice which allows stopping the
- *    ticks on CONFIG_NO_HZ_FULL kernels running with the proper nohz_full
- *    parameter. The tickless operation can be observed through
- *    /proc/interrupts.
- *
- *    Periodic switching is enforced by a periodic timer checking all CPUs and
- *    preempting them as necessary. Unfortunately, BPF timer currently doesn't
- *    have a way to pin to a specific CPU, so the periodic timer isn't pinned to
- *    the central CPU.
- *
- * c. Preemption
- *
- *    Kthreads are unconditionally queued to the head of a matching local dsq
- *    and dispatched with SCX_DSQ_PREEMPT. This ensures that a kthread is always
- *    prioritized over user threads, which is required for ensuring forward
- *    progress as e.g. the periodic timer may run on a ksoftirqd and if the
- *    ksoftirqd gets starved by a user thread, there may not be anything else to
- *    vacate that user thread.
- *
- *    SCX_KICK_PREEMPT is used to trigger scheduling and CPUs to move to the
- *    next tasks.
- *
- * This scheduler is designed to maximize usage of various SCX mechanisms. A
- * more practical implementation would likely put the scheduling loop outside
- * the central CPU's dispatch() path and add some form of priority mechanism.
- *
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#include <scx/common.bpf.h>
-
-char _license[] SEC("license") = "GPL";
-
-enum {
-	FALLBACK_DSQ_ID		= 0,
-	MS_TO_NS		= 1000LLU * 1000,
-	TIMER_INTERVAL_NS	= 1 * MS_TO_NS,
-};
-
-const volatile bool switch_partial;
-const volatile s32 central_cpu;
-const volatile u32 nr_cpu_ids = 1;	/* !0 for veristat, set during init */
-const volatile u64 slice_ns = SCX_SLICE_DFL;
-
-bool timer_pinned = true;
-u64 nr_total, nr_locals, nr_queued, nr_lost_pids;
-u64 nr_timers, nr_dispatches, nr_mismatches, nr_retries;
-u64 nr_overflows;
-
-struct user_exit_info uei;
-
-struct {
-	__uint(type, BPF_MAP_TYPE_QUEUE);
-	__uint(max_entries, 4096);
-	__type(value, s32);
-} central_q SEC(".maps");
-
-/* can't use percpu map due to bad lookups */
-bool RESIZABLE_ARRAY(data, cpu_gimme_task);
-u64 RESIZABLE_ARRAY(data, cpu_started_at);
-
-struct central_timer {
-	struct bpf_timer timer;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__uint(max_entries, 1);
-	__type(key, u32);
-	__type(value, struct central_timer);
-} central_timer SEC(".maps");
-
-static bool vtime_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
-s32 BPF_STRUCT_OPS(central_select_cpu, struct task_struct *p,
-		   s32 prev_cpu, u64 wake_flags)
-{
-	/*
-	 * Steer wakeups to the central CPU as much as possible to avoid
-	 * disturbing other CPUs. It's safe to blindly return the central cpu as
-	 * select_cpu() is a hint and if @p can't be on it, the kernel will
-	 * automatically pick a fallback CPU.
-	 */
-	return central_cpu;
-}
-
-void BPF_STRUCT_OPS(central_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	s32 pid = p->pid;
-
-	__sync_fetch_and_add(&nr_total, 1);
-
-	/*
-	 * Push per-cpu kthreads at the head of local dsq's and preempt the
-	 * corresponding CPU. This ensures that e.g. ksoftirqd isn't blocked
-	 * behind other threads which is necessary for forward progress
-	 * guarantee as we depend on the BPF timer which may run from ksoftirqd.
-	 */
-	if ((p->flags & PF_KTHREAD) && p->nr_cpus_allowed == 1) {
-		__sync_fetch_and_add(&nr_locals, 1);
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_INF,
-				 enq_flags | SCX_ENQ_PREEMPT);
-		return;
-	}
-
-	if (bpf_map_push_elem(&central_q, &pid, 0)) {
-		__sync_fetch_and_add(&nr_overflows, 1);
-		scx_bpf_dispatch(p, FALLBACK_DSQ_ID, SCX_SLICE_INF, enq_flags);
-		return;
-	}
-
-	__sync_fetch_and_add(&nr_queued, 1);
-
-	if (!scx_bpf_task_running(p))
-		scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
-}
-
-static bool dispatch_to_cpu(s32 cpu)
-{
-	struct task_struct *p;
-	s32 pid;
-
-	bpf_repeat(BPF_MAX_LOOPS) {
-		if (bpf_map_pop_elem(&central_q, &pid))
-			break;
-
-		__sync_fetch_and_sub(&nr_queued, 1);
-
-		p = bpf_task_from_pid(pid);
-		if (!p) {
-			__sync_fetch_and_add(&nr_lost_pids, 1);
-			continue;
-		}
-
-		/*
-		 * If we can't run the task at the top, do the dumb thing and
-		 * bounce it to the fallback dsq.
-		 */
-		if (!bpf_cpumask_test_cpu(cpu, p->cpus_ptr)) {
-			__sync_fetch_and_add(&nr_mismatches, 1);
-			scx_bpf_dispatch(p, FALLBACK_DSQ_ID, SCX_SLICE_INF, 0);
-			bpf_task_release(p);
-			/*
-			 * We might run out of dispatch buffer slots if we continue dispatching
-			 * to the fallback DSQ, without dispatching to the local DSQ of the
-			 * target CPU. In such a case, break the loop now as will fail the
-			 * next dispatch operation.
-			 */
-			if (!scx_bpf_dispatch_nr_slots())
-				break;
-			continue;
-		}
-
-		/* dispatch to local and mark that @cpu doesn't need more */
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL_ON | cpu, SCX_SLICE_INF, 0);
-
-		if (cpu != central_cpu)
-			scx_bpf_kick_cpu(cpu, 0);
-
-		bpf_task_release(p);
-		return true;
-	}
-
-	return false;
-}
-
-void BPF_STRUCT_OPS(central_dispatch, s32 cpu, struct task_struct *prev)
-{
-	if (cpu == central_cpu) {
-		/* dispatch for all other CPUs first */
-		__sync_fetch_and_add(&nr_dispatches, 1);
-
-		bpf_for(cpu, 0, nr_cpu_ids) {
-			bool *gimme;
-
-			if (!scx_bpf_dispatch_nr_slots())
-				break;
-
-			/* central's gimme is never set */
-			gimme = ARRAY_ELEM_PTR(cpu_gimme_task, cpu, nr_cpu_ids);
-			if (gimme && !*gimme)
-				continue;
-
-			if (dispatch_to_cpu(cpu))
-				*gimme = false;
-		}
-
-		/*
-		 * Retry if we ran out of dispatch buffer slots as we might have
-		 * skipped some CPUs and also need to dispatch for self. The ext
-		 * core automatically retries if the local dsq is empty but we
-		 * can't rely on that as we're dispatching for other CPUs too.
-		 * Kick self explicitly to retry.
-		 */
-		if (!scx_bpf_dispatch_nr_slots()) {
-			__sync_fetch_and_add(&nr_retries, 1);
-			scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
-			return;
-		}
-
-		/* look for a task to run on the central CPU */
-		if (scx_bpf_consume(FALLBACK_DSQ_ID))
-			return;
-		dispatch_to_cpu(central_cpu);
-	} else {
-		bool *gimme;
-
-		if (scx_bpf_consume(FALLBACK_DSQ_ID))
-			return;
-
-		gimme = ARRAY_ELEM_PTR(cpu_gimme_task, cpu, nr_cpu_ids);
-		if (gimme)
-			*gimme = true;
-
-		/*
-		 * Force dispatch on the scheduling CPU so that it finds a task
-		 * to run for us.
-		 */
-		scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
-	}
-}
-
-void BPF_STRUCT_OPS(central_running, struct task_struct *p)
-{
-	s32 cpu = scx_bpf_task_cpu(p);
-	u64 *started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
-	if (started_at)
-		*started_at = bpf_ktime_get_ns() ?: 1;	/* 0 indicates idle */
-}
-
-void BPF_STRUCT_OPS(central_stopping, struct task_struct *p, bool runnable)
-{
-	s32 cpu = scx_bpf_task_cpu(p);
-	u64 *started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
-	if (started_at)
-		*started_at = 0;
-}
-
-static int central_timerfn(void *map, int *key, struct bpf_timer *timer)
-{
-	u64 now = bpf_ktime_get_ns();
-	u64 nr_to_kick = nr_queued;
-	s32 i, curr_cpu;
-
-	curr_cpu = bpf_get_smp_processor_id();
-	if (timer_pinned && (curr_cpu != central_cpu)) {
-		scx_bpf_error("Central timer ran on CPU %d, not central CPU %d",
-			      curr_cpu, central_cpu);
-		return 0;
-	}
-
-	bpf_for(i, 0, nr_cpu_ids) {
-		s32 cpu = (nr_timers + i) % nr_cpu_ids;
-		u64 *started_at;
-
-		if (cpu == central_cpu)
-			continue;
-
-		/* kick iff the current one exhausted its slice */
-		started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
-		if (started_at && *started_at &&
-		    vtime_before(now, *started_at + slice_ns))
-			continue;
-
-		/* and there's something pending */
-		if (scx_bpf_dsq_nr_queued(FALLBACK_DSQ_ID) ||
-		    scx_bpf_dsq_nr_queued(SCX_DSQ_LOCAL_ON | cpu))
-			;
-		else if (nr_to_kick)
-			nr_to_kick--;
-		else
-			continue;
-
-		scx_bpf_kick_cpu(cpu, SCX_KICK_PREEMPT);
-	}
-
-	bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
-	__sync_fetch_and_add(&nr_timers, 1);
-	return 0;
-}
-
-int BPF_STRUCT_OPS_SLEEPABLE(central_init)
-{
-	u32 key = 0;
-	struct bpf_timer *timer;
-	int ret;
-
-	if (!switch_partial)
-		scx_bpf_switch_all();
-
-	ret = scx_bpf_create_dsq(FALLBACK_DSQ_ID, -1);
-	if (ret)
-		return ret;
-
-	timer = bpf_map_lookup_elem(&central_timer, &key);
-	if (!timer)
-		return -ESRCH;
-
-	if (bpf_get_smp_processor_id() != central_cpu) {
-		scx_bpf_error("init from non-central CPU");
-		return -EINVAL;
-	}
-
-	bpf_timer_init(timer, &central_timer, CLOCK_MONOTONIC);
-	bpf_timer_set_callback(timer, central_timerfn);
-
-	ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
-	/*
-	 * BPF_F_TIMER_CPU_PIN is pretty new (>=6.7). If we're running in a
-	 * kernel which doesn't have it, bpf_timer_start() will return -EINVAL.
-	 * Retry without the PIN. This would be the perfect use case for
-	 * bpf_core_enum_value_exists() but the enum type doesn't have a name
-	 * and can't be used with bpf_core_enum_value_exists(). Oh well...
-	 */
-	if (ret == -EINVAL) {
-		timer_pinned = false;
-		ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, 0);
-	}
-	if (ret)
-		scx_bpf_error("bpf_timer_start failed (%d)", ret);
-	return ret;
-}
-
-void BPF_STRUCT_OPS(central_exit, struct scx_exit_info *ei)
-{
-	uei_record(&uei, ei);
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops central_ops = {
-	/*
-	 * We are offloading all scheduling decisions to the central CPU and
-	 * thus being the last task on a given CPU doesn't mean anything
-	 * special. Enqueue the last tasks like any other tasks.
-	 */
-	.flags			= SCX_OPS_ENQ_LAST,
-
-	.select_cpu		= (void *)central_select_cpu,
-	.enqueue		= (void *)central_enqueue,
-	.dispatch		= (void *)central_dispatch,
-	.running		= (void *)central_running,
-	.stopping		= (void *)central_stopping,
-	.init			= (void *)central_init,
-	.exit			= (void *)central_exit,
-	.name			= "central",
-};
diff --git a/tools/sched_ext/scx_central.c b/tools/sched_ext/scx_central.c
deleted file mode 100644
index 501505001..000000000
--- a/tools/sched_ext/scx_central.c
+++ /dev/null
@@ -1,127 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#define _GNU_SOURCE
-#include <sched.h>
-#include <stdio.h>
-#include <unistd.h>
-#include <inttypes.h>
-#include <signal.h>
-#include <libgen.h>
-#include <bpf/bpf.h>
-#include <scx/common.h>
-#include "scx_central.bpf.skel.h"
-
-const char help_fmt[] =
-"A central FIFO sched_ext scheduler.\n"
-"\n"
-"See the top-level comment in .bpf.c for more details.\n"
-"\n"
-"Usage: %s [-s SLICE_US] [-c CPU] [-p]\n"
-"\n"
-"  -s SLICE_US   Override slice duration\n"
-"  -c CPU        Override the central CPU (default: 0)\n"
-"  -p            Switch only tasks on SCHED_EXT policy intead of all\n"
-"  -h            Display this help and exit\n";
-
-static volatile int exit_req;
-
-static void sigint_handler(int dummy)
-{
-	exit_req = 1;
-}
-
-int main(int argc, char **argv)
-{
-	struct scx_central *skel;
-	struct bpf_link *link;
-	__u64 seq = 0;
-	__s32 opt;
-	cpu_set_t *cpuset;
-
-	signal(SIGINT, sigint_handler);
-	signal(SIGTERM, sigint_handler);
-
-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
-
-	skel = scx_central__open();
-	SCX_BUG_ON(!skel, "Failed to open skel");
-
-	skel->rodata->central_cpu = 0;
-	skel->rodata->nr_cpu_ids = libbpf_num_possible_cpus();
-
-	while ((opt = getopt(argc, argv, "s:c:ph")) != -1) {
-		switch (opt) {
-		case 's':
-			skel->rodata->slice_ns = strtoull(optarg, NULL, 0) * 1000;
-			break;
-		case 'c':
-			skel->rodata->central_cpu = strtoul(optarg, NULL, 0);
-			break;
-		case 'p':
-			skel->rodata->switch_partial = true;
-			break;
-		default:
-			fprintf(stderr, help_fmt, basename(argv[0]));
-			return opt != 'h';
-		}
-	}
-
-	/* Resize arrays so their element count is equal to cpu count. */
-	RESIZE_ARRAY(data, cpu_gimme_task, skel->rodata->nr_cpu_ids);
-	RESIZE_ARRAY(data, cpu_started_at, skel->rodata->nr_cpu_ids);
-
-	SCX_BUG_ON(scx_central__load(skel), "Failed to load skel");
-
-	/*
-	 * Affinitize the loading thread to the central CPU, as:
-	 * - That's where the BPF timer is first invoked in the BPF program.
-	 * - We probably don't want this user space component to take up a core
-	 *   from a task that would benefit from avoiding preemption on one of
-	 *   the tickless cores.
-	 *
-	 * Until BPF supports pinning the timer, it's not guaranteed that it
-	 * will always be invoked on the central CPU. In practice, this
-	 * suffices the majority of the time.
-	 */
-	cpuset = CPU_ALLOC(skel->rodata->nr_cpu_ids);
-	SCX_BUG_ON(!cpuset, "Failed to allocate cpuset");
-	CPU_ZERO(cpuset);
-	CPU_SET(skel->rodata->central_cpu, cpuset);
-	SCX_BUG_ON(sched_setaffinity(0, sizeof(cpuset), cpuset),
-		   "Failed to affinitize to central CPU %d (max %d)",
-		   skel->rodata->central_cpu, skel->rodata->nr_cpu_ids - 1);
-	CPU_FREE(cpuset);
-
-	link = bpf_map__attach_struct_ops(skel->maps.central_ops);
-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
-
-	if (!skel->data->timer_pinned)
-		printf("WARNING : BPF_F_TIMER_CPU_PIN not available, timer not pinned to central\n");
-
-	while (!exit_req && !uei_exited(&skel->bss->uei)) {
-		printf("[SEQ %llu]\n", seq++);
-		printf("total   :%10" PRIu64 "    local:%10" PRIu64 "   queued:%10" PRIu64 "  lost:%10" PRIu64 "\n",
-		       skel->bss->nr_total,
-		       skel->bss->nr_locals,
-		       skel->bss->nr_queued,
-		       skel->bss->nr_lost_pids);
-		printf("timer   :%10" PRIu64 " dispatch:%10" PRIu64 " mismatch:%10" PRIu64 " retry:%10" PRIu64 "\n",
-		       skel->bss->nr_timers,
-		       skel->bss->nr_dispatches,
-		       skel->bss->nr_mismatches,
-		       skel->bss->nr_retries);
-		printf("overflow:%10" PRIu64 "\n",
-		       skel->bss->nr_overflows);
-		fflush(stdout);
-		sleep(1);
-	}
-
-	bpf_link__destroy(link);
-	uei_print(&skel->bss->uei);
-	scx_central__destroy(skel);
-	return 0;
-}
diff --git a/tools/sched_ext/scx_flatcg.bpf.c b/tools/sched_ext/scx_flatcg.bpf.c
deleted file mode 100644
index d6a947bc9..000000000
--- a/tools/sched_ext/scx_flatcg.bpf.c
+++ /dev/null
@@ -1,960 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * A demo sched_ext flattened cgroup hierarchy scheduler. It implements
- * hierarchical weight-based cgroup CPU control by flattening the cgroup
- * hierarchy into a single layer by compounding the active weight share at each
- * level. Consider the following hierarchy with weights in parentheses:
- *
- * R + A (100) + B (100)
- *   |         \ C (100)
- *   \ D (200)
- *
- * Ignoring the root and threaded cgroups, only B, C and D can contain tasks.
- * Let's say all three have runnable tasks. The total share that each of these
- * three cgroups is entitled to can be calculated by compounding its share at
- * each level.
- *
- * For example, B is competing against C and in that competition its share is
- * 100/(100+100) == 1/2. At its parent level, A is competing against D and A's
- * share in that competition is 200/(200+100) == 1/3. B's eventual share in the
- * system can be calculated by multiplying the two shares, 1/2 * 1/3 == 1/6. C's
- * eventual shaer is the same at 1/6. D is only competing at the top level and
- * its share is 200/(100+200) == 2/3.
- *
- * So, instead of hierarchically scheduling level-by-level, we can consider it
- * as B, C and D competing each other with respective share of 1/6, 1/6 and 2/3
- * and keep updating the eventual shares as the cgroups' runnable states change.
- *
- * This flattening of hierarchy can bring a substantial performance gain when
- * the cgroup hierarchy is nested multiple levels. in a simple benchmark using
- * wrk[8] on apache serving a CGI script calculating sha1sum of a small file, it
- * outperforms CFS by ~3% with CPU controller disabled and by ~10% with two
- * apache instances competing with 2:1 weight ratio nested four level deep.
- *
- * However, the gain comes at the cost of not being able to properly handle
- * thundering herd of cgroups. For example, if many cgroups which are nested
- * behind a low priority parent cgroup wake up around the same time, they may be
- * able to consume more CPU cycles than they are entitled to. In many use cases,
- * this isn't a real concern especially given the performance gain. Also, there
- * are ways to mitigate the problem further by e.g. introducing an extra
- * scheduling layer on cgroup delegation boundaries.
- *
- * The scheduler first picks the cgroup to run and then schedule the tasks
- * within by using nested weighted vtime scheduling by default. The
- * cgroup-internal scheduling can be switched to FIFO with the -f option.
- */
-#include <scx/common.bpf.h>
-#include "scx_flatcg.h"
-
-/*
- * Maximum amount of retries to find a valid cgroup.
- */
-#define CGROUP_MAX_RETRIES 1024
-
-char _license[] SEC("license") = "GPL";
-
-const volatile u32 nr_cpus = 32;	/* !0 for veristat, set during init */
-const volatile u64 cgrp_slice_ns = SCX_SLICE_DFL;
-const volatile bool fifo_sched;
-const volatile bool switch_partial;
-
-u64 cvtime_now;
-struct user_exit_info uei;
-
-struct {
-	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
-	__type(key, u32);
-	__type(value, u64);
-	__uint(max_entries, FCG_NR_STATS);
-} stats SEC(".maps");
-
-static void stat_inc(enum fcg_stat_idx idx)
-{
-	u32 idx_v = idx;
-
-	u64 *cnt_p = bpf_map_lookup_elem(&stats, &idx_v);
-	if (cnt_p)
-		(*cnt_p)++;
-}
-
-struct fcg_cpu_ctx {
-	u64			cur_cgid;
-	u64			cur_at;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
-	__type(key, u32);
-	__type(value, struct fcg_cpu_ctx);
-	__uint(max_entries, 1);
-} cpu_ctx SEC(".maps");
-
-struct {
-	__uint(type, BPF_MAP_TYPE_CGRP_STORAGE);
-	__uint(map_flags, BPF_F_NO_PREALLOC);
-	__type(key, int);
-	__type(value, struct fcg_cgrp_ctx);
-} cgrp_ctx SEC(".maps");
-
-struct cgv_node {
-	struct bpf_rb_node	rb_node;
-	__u64			cvtime;
-	__u64			cgid;
-};
-
-private(CGV_TREE) struct bpf_spin_lock cgv_tree_lock;
-private(CGV_TREE) struct bpf_rb_root cgv_tree __contains(cgv_node, rb_node);
-
-struct cgv_node_stash {
-	struct cgv_node __kptr *node;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_HASH);
-	__uint(max_entries, 16384);
-	__type(key, __u64);
-	__type(value, struct cgv_node_stash);
-} cgv_node_stash SEC(".maps");
-
-struct fcg_task_ctx {
-	u64		bypassed_at;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
-	__uint(map_flags, BPF_F_NO_PREALLOC);
-	__type(key, int);
-	__type(value, struct fcg_task_ctx);
-} task_ctx SEC(".maps");
-
-/* gets inc'd on weight tree changes to expire the cached hweights */
-u64 hweight_gen = 1;
-
-static u64 div_round_up(u64 dividend, u64 divisor)
-{
-	return (dividend + divisor - 1) / divisor;
-}
-
-static bool vtime_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
-static bool cgv_node_less(struct bpf_rb_node *a, const struct bpf_rb_node *b)
-{
-	struct cgv_node *cgc_a, *cgc_b;
-
-	cgc_a = container_of(a, struct cgv_node, rb_node);
-	cgc_b = container_of(b, struct cgv_node, rb_node);
-
-	return cgc_a->cvtime < cgc_b->cvtime;
-}
-
-static struct fcg_cpu_ctx *find_cpu_ctx(void)
-{
-	struct fcg_cpu_ctx *cpuc;
-	u32 idx = 0;
-
-	cpuc = bpf_map_lookup_elem(&cpu_ctx, &idx);
-	if (!cpuc) {
-		scx_bpf_error("cpu_ctx lookup failed");
-		return NULL;
-	}
-	return cpuc;
-}
-
-static struct fcg_cgrp_ctx *find_cgrp_ctx(struct cgroup *cgrp)
-{
-	struct fcg_cgrp_ctx *cgc;
-
-	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
-	if (!cgc) {
-		scx_bpf_error("cgrp_ctx lookup failed for cgid %llu", cgrp->kn->id);
-		return NULL;
-	}
-	return cgc;
-}
-
-static struct fcg_cgrp_ctx *find_ancestor_cgrp_ctx(struct cgroup *cgrp, int level)
-{
-	struct fcg_cgrp_ctx *cgc;
-
-	cgrp = bpf_cgroup_ancestor(cgrp, level);
-	if (!cgrp) {
-		scx_bpf_error("ancestor cgroup lookup failed");
-		return NULL;
-	}
-
-	cgc = find_cgrp_ctx(cgrp);
-	if (!cgc)
-		scx_bpf_error("ancestor cgrp_ctx lookup failed");
-	bpf_cgroup_release(cgrp);
-	return cgc;
-}
-
-static void cgrp_refresh_hweight(struct cgroup *cgrp, struct fcg_cgrp_ctx *cgc)
-{
-	int level;
-
-	if (!cgc->nr_active) {
-		stat_inc(FCG_STAT_HWT_SKIP);
-		return;
-	}
-
-	if (cgc->hweight_gen == hweight_gen) {
-		stat_inc(FCG_STAT_HWT_CACHE);
-		return;
-	}
-
-	stat_inc(FCG_STAT_HWT_UPDATES);
-	bpf_for(level, 0, cgrp->level + 1) {
-		struct fcg_cgrp_ctx *cgc;
-		bool is_active;
-
-		cgc = find_ancestor_cgrp_ctx(cgrp, level);
-		if (!cgc)
-			break;
-
-		if (!level) {
-			cgc->hweight = FCG_HWEIGHT_ONE;
-			cgc->hweight_gen = hweight_gen;
-		} else {
-			struct fcg_cgrp_ctx *pcgc;
-
-			pcgc = find_ancestor_cgrp_ctx(cgrp, level - 1);
-			if (!pcgc)
-				break;
-
-			/*
-			 * We can be oppotunistic here and not grab the
-			 * cgv_tree_lock and deal with the occasional races.
-			 * However, hweight updates are already cached and
-			 * relatively low-frequency. Let's just do the
-			 * straightforward thing.
-			 */
-			bpf_spin_lock(&cgv_tree_lock);
-			is_active = cgc->nr_active;
-			if (is_active) {
-				cgc->hweight_gen = pcgc->hweight_gen;
-				cgc->hweight =
-					div_round_up(pcgc->hweight * cgc->weight,
-						     pcgc->child_weight_sum);
-			}
-			bpf_spin_unlock(&cgv_tree_lock);
-
-			if (!is_active) {
-				stat_inc(FCG_STAT_HWT_RACE);
-				break;
-			}
-		}
-	}
-}
-
-static void cgrp_cap_budget(struct cgv_node *cgv_node, struct fcg_cgrp_ctx *cgc)
-{
-	u64 delta, cvtime, max_budget;
-
-	/*
-	 * A node which is on the rbtree can't be pointed to from elsewhere yet
-	 * and thus can't be updated and repositioned. Instead, we collect the
-	 * vtime deltas separately and apply it asynchronously here.
-	 */
-	delta = cgc->cvtime_delta;
-	__sync_fetch_and_sub(&cgc->cvtime_delta, delta);
-	cvtime = cgv_node->cvtime + delta;
-
-	/*
-	 * Allow a cgroup to carry the maximum budget proportional to its
-	 * hweight such that a full-hweight cgroup can immediately take up half
-	 * of the CPUs at the most while staying at the front of the rbtree.
-	 */
-	max_budget = (cgrp_slice_ns * nr_cpus * cgc->hweight) /
-		(2 * FCG_HWEIGHT_ONE);
-	if (vtime_before(cvtime, cvtime_now - max_budget))
-		cvtime = cvtime_now - max_budget;
-
-	cgv_node->cvtime = cvtime;
-}
-
-static void cgrp_enqueued(struct cgroup *cgrp, struct fcg_cgrp_ctx *cgc)
-{
-	struct cgv_node_stash *stash;
-	struct cgv_node *cgv_node;
-	u64 cgid = cgrp->kn->id;
-
-	/* paired with cmpxchg in try_pick_next_cgroup() */
-	if (__sync_val_compare_and_swap(&cgc->queued, 0, 1)) {
-		stat_inc(FCG_STAT_ENQ_SKIP);
-		return;
-	}
-
-	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
-	if (!stash) {
-		scx_bpf_error("cgv_node lookup failed for cgid %llu", cgid);
-		return;
-	}
-
-	/* NULL if the node is already on the rbtree */
-	cgv_node = bpf_kptr_xchg(&stash->node, NULL);
-	if (!cgv_node) {
-		stat_inc(FCG_STAT_ENQ_RACE);
-		return;
-	}
-
-	bpf_spin_lock(&cgv_tree_lock);
-	cgrp_cap_budget(cgv_node, cgc);
-	bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
-	bpf_spin_unlock(&cgv_tree_lock);
-}
-
-static void set_bypassed_at(struct task_struct *p, struct fcg_task_ctx *taskc)
-{
-	/*
-	 * Tell fcg_stopping() that this bypassed the regular scheduling path
-	 * and should be force charged to the cgroup. 0 is used to indicate that
-	 * the task isn't bypassing, so if the current runtime is 0, go back by
-	 * one nanosecond.
-	 */
-	taskc->bypassed_at = p->se.sum_exec_runtime ?: (u64)-1;
-}
-
-s32 BPF_STRUCT_OPS(fcg_select_cpu, struct task_struct *p, s32 prev_cpu, u64 wake_flags)
-{
-	struct fcg_task_ctx *taskc;
-	bool is_idle = false;
-	s32 cpu;
-
-	cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &is_idle);
-
-	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
-	if (!taskc) {
-		scx_bpf_error("task_ctx lookup failed");
-		return cpu;
-	}
-
-	/*
-	 * If select_cpu_dfl() is recommending local enqueue, the target CPU is
-	 * idle. Follow it and charge the cgroup later in fcg_stopping() after
-	 * the fact.
-	 */
-	if (is_idle) {
-		set_bypassed_at(p, taskc);
-		stat_inc(FCG_STAT_LOCAL);
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, 0);
-	}
-
-	return cpu;
-}
-
-void BPF_STRUCT_OPS(fcg_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	struct fcg_task_ctx *taskc;
-	struct cgroup *cgrp;
-	struct fcg_cgrp_ctx *cgc;
-
-	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
-	if (!taskc) {
-		scx_bpf_error("task_ctx lookup failed");
-		return;
-	}
-
-	/*
-	 * Use the direct dispatching and force charging to deal with tasks with
-	 * custom affinities so that we don't have to worry about per-cgroup
-	 * dq's containing tasks that can't be executed from some CPUs.
-	 */
-	if (p->nr_cpus_allowed != nr_cpus) {
-		set_bypassed_at(p, taskc);
-
-		/*
-		 * The global dq is deprioritized as we don't want to let tasks
-		 * to boost themselves by constraining its cpumask. The
-		 * deprioritization is rather severe, so let's not apply that to
-		 * per-cpu kernel threads. This is ham-fisted. We probably wanna
-		 * implement per-cgroup fallback dq's instead so that we have
-		 * more control over when tasks with custom cpumask get issued.
-		 */
-		if (p->nr_cpus_allowed == 1 && (p->flags & PF_KTHREAD)) {
-			stat_inc(FCG_STAT_LOCAL);
-			scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, enq_flags);
-		} else {
-			stat_inc(FCG_STAT_GLOBAL);
-			scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
-		}
-		return;
-	}
-
-	cgrp = scx_bpf_task_cgroup(p);
-	cgc = find_cgrp_ctx(cgrp);
-	if (!cgc)
-		goto out_release;
-
-	if (fifo_sched) {
-		scx_bpf_dispatch(p, cgrp->kn->id, SCX_SLICE_DFL, enq_flags);
-	} else {
-		u64 tvtime = p->scx.dsq_vtime;
-
-		/*
-		 * Limit the amount of budget that an idling task can accumulate
-		 * to one slice.
-		 */
-		if (vtime_before(tvtime, cgc->tvtime_now - SCX_SLICE_DFL))
-			tvtime = cgc->tvtime_now - SCX_SLICE_DFL;
-
-		scx_bpf_dispatch_vtime(p, cgrp->kn->id, SCX_SLICE_DFL,
-				       tvtime, enq_flags);
-	}
-
-	cgrp_enqueued(cgrp, cgc);
-out_release:
-	bpf_cgroup_release(cgrp);
-}
-
-/*
- * Walk the cgroup tree to update the active weight sums as tasks wake up and
- * sleep. The weight sums are used as the base when calculating the proportion a
- * given cgroup or task is entitled to at each level.
- */
-static void update_active_weight_sums(struct cgroup *cgrp, bool runnable)
-{
-	struct fcg_cgrp_ctx *cgc;
-	bool updated = false;
-	int idx;
-
-	cgc = find_cgrp_ctx(cgrp);
-	if (!cgc)
-		return;
-
-	/*
-	 * In most cases, a hot cgroup would have multiple threads going to
-	 * sleep and waking up while the whole cgroup stays active. In leaf
-	 * cgroups, ->nr_runnable which is updated with __sync operations gates
-	 * ->nr_active updates, so that we don't have to grab the cgv_tree_lock
-	 * repeatedly for a busy cgroup which is staying active.
-	 */
-	if (runnable) {
-		if (__sync_fetch_and_add(&cgc->nr_runnable, 1))
-			return;
-		stat_inc(FCG_STAT_ACT);
-	} else {
-		if (__sync_sub_and_fetch(&cgc->nr_runnable, 1))
-			return;
-		stat_inc(FCG_STAT_DEACT);
-	}
-
-	/*
-	 * If @cgrp is becoming runnable, its hweight should be refreshed after
-	 * it's added to the weight tree so that enqueue has the up-to-date
-	 * value. If @cgrp is becoming quiescent, the hweight should be
-	 * refreshed before it's removed from the weight tree so that the usage
-	 * charging which happens afterwards has access to the latest value.
-	 */
-	if (!runnable)
-		cgrp_refresh_hweight(cgrp, cgc);
-
-	/* propagate upwards */
-	bpf_for(idx, 0, cgrp->level) {
-		int level = cgrp->level - idx;
-		struct fcg_cgrp_ctx *cgc, *pcgc = NULL;
-		bool propagate = false;
-
-		cgc = find_ancestor_cgrp_ctx(cgrp, level);
-		if (!cgc)
-			break;
-		if (level) {
-			pcgc = find_ancestor_cgrp_ctx(cgrp, level - 1);
-			if (!pcgc)
-				break;
-		}
-
-		/*
-		 * We need the propagation protected by a lock to synchronize
-		 * against weight changes. There's no reason to drop the lock at
-		 * each level but bpf_spin_lock() doesn't want any function
-		 * calls while locked.
-		 */
-		bpf_spin_lock(&cgv_tree_lock);
-
-		if (runnable) {
-			if (!cgc->nr_active++) {
-				updated = true;
-				if (pcgc) {
-					propagate = true;
-					pcgc->child_weight_sum += cgc->weight;
-				}
-			}
-		} else {
-			if (!--cgc->nr_active) {
-				updated = true;
-				if (pcgc) {
-					propagate = true;
-					pcgc->child_weight_sum -= cgc->weight;
-				}
-			}
-		}
-
-		bpf_spin_unlock(&cgv_tree_lock);
-
-		if (!propagate)
-			break;
-	}
-
-	if (updated)
-		__sync_fetch_and_add(&hweight_gen, 1);
-
-	if (runnable)
-		cgrp_refresh_hweight(cgrp, cgc);
-}
-
-void BPF_STRUCT_OPS(fcg_runnable, struct task_struct *p, u64 enq_flags)
-{
-	struct cgroup *cgrp;
-
-	cgrp = scx_bpf_task_cgroup(p);
-	update_active_weight_sums(cgrp, true);
-	bpf_cgroup_release(cgrp);
-}
-
-void BPF_STRUCT_OPS(fcg_running, struct task_struct *p)
-{
-	struct cgroup *cgrp;
-	struct fcg_cgrp_ctx *cgc;
-
-	if (fifo_sched)
-		return;
-
-	cgrp = scx_bpf_task_cgroup(p);
-	cgc = find_cgrp_ctx(cgrp);
-	if (cgc) {
-		/*
-		 * @cgc->tvtime_now always progresses forward as tasks start
-		 * executing. The test and update can be performed concurrently
-		 * from multiple CPUs and thus racy. Any error should be
-		 * contained and temporary. Let's just live with it.
-		 */
-		if (vtime_before(cgc->tvtime_now, p->scx.dsq_vtime))
-			cgc->tvtime_now = p->scx.dsq_vtime;
-	}
-	bpf_cgroup_release(cgrp);
-}
-
-void BPF_STRUCT_OPS(fcg_stopping, struct task_struct *p, bool runnable)
-{
-	struct fcg_task_ctx *taskc;
-	struct cgroup *cgrp;
-	struct fcg_cgrp_ctx *cgc;
-
-	/*
-	 * Scale the execution time by the inverse of the weight and charge.
-	 *
-	 * Note that the default yield implementation yields by setting
-	 * @p->scx.slice to zero and the following would treat the yielding task
-	 * as if it has consumed all its slice. If this penalizes yielding tasks
-	 * too much, determine the execution time by taking explicit timestamps
-	 * instead of depending on @p->scx.slice.
-	 */
-	if (!fifo_sched)
-		p->scx.dsq_vtime +=
-			(SCX_SLICE_DFL - p->scx.slice) * 100 / p->scx.weight;
-
-	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
-	if (!taskc) {
-		scx_bpf_error("task_ctx lookup failed");
-		return;
-	}
-
-	if (!taskc->bypassed_at)
-		return;
-
-	cgrp = scx_bpf_task_cgroup(p);
-	cgc = find_cgrp_ctx(cgrp);
-	if (cgc) {
-		__sync_fetch_and_add(&cgc->cvtime_delta,
-				     p->se.sum_exec_runtime - taskc->bypassed_at);
-		taskc->bypassed_at = 0;
-	}
-	bpf_cgroup_release(cgrp);
-}
-
-void BPF_STRUCT_OPS(fcg_quiescent, struct task_struct *p, u64 deq_flags)
-{
-	struct cgroup *cgrp;
-
-	cgrp = scx_bpf_task_cgroup(p);
-	update_active_weight_sums(cgrp, false);
-	bpf_cgroup_release(cgrp);
-}
-
-void BPF_STRUCT_OPS(fcg_cgroup_set_weight, struct cgroup *cgrp, u32 weight)
-{
-	struct fcg_cgrp_ctx *cgc, *pcgc = NULL;
-
-	cgc = find_cgrp_ctx(cgrp);
-	if (!cgc)
-		return;
-
-	if (cgrp->level) {
-		pcgc = find_ancestor_cgrp_ctx(cgrp, cgrp->level - 1);
-		if (!pcgc)
-			return;
-	}
-
-	bpf_spin_lock(&cgv_tree_lock);
-	if (pcgc && cgc->nr_active)
-		pcgc->child_weight_sum += (s64)weight - cgc->weight;
-	cgc->weight = weight;
-	bpf_spin_unlock(&cgv_tree_lock);
-}
-
-static bool try_pick_next_cgroup(u64 *cgidp)
-{
-	struct bpf_rb_node *rb_node;
-	struct cgv_node_stash *stash;
-	struct cgv_node *cgv_node;
-	struct fcg_cgrp_ctx *cgc;
-	struct cgroup *cgrp;
-	u64 cgid;
-
-	/* pop the front cgroup and wind cvtime_now accordingly */
-	bpf_spin_lock(&cgv_tree_lock);
-
-	rb_node = bpf_rbtree_first(&cgv_tree);
-	if (!rb_node) {
-		bpf_spin_unlock(&cgv_tree_lock);
-		stat_inc(FCG_STAT_PNC_NO_CGRP);
-		*cgidp = 0;
-		return true;
-	}
-
-	rb_node = bpf_rbtree_remove(&cgv_tree, rb_node);
-	bpf_spin_unlock(&cgv_tree_lock);
-
-	if (!rb_node) {
-		/*
-		 * This should never happen. bpf_rbtree_first() was called
-		 * above while the tree lock was held, so the node should
-		 * always be present.
-		 */
-		scx_bpf_error("node could not be removed");
-		return true;
-	}
-
-	cgv_node = container_of(rb_node, struct cgv_node, rb_node);
-	cgid = cgv_node->cgid;
-
-	if (vtime_before(cvtime_now, cgv_node->cvtime))
-		cvtime_now = cgv_node->cvtime;
-
-	/*
-	 * If lookup fails, the cgroup's gone. Free and move on. See
-	 * fcg_cgroup_exit().
-	 */
-	cgrp = bpf_cgroup_from_id(cgid);
-	if (!cgrp) {
-		stat_inc(FCG_STAT_PNC_GONE);
-		goto out_free;
-	}
-
-	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
-	if (!cgc) {
-		bpf_cgroup_release(cgrp);
-		stat_inc(FCG_STAT_PNC_GONE);
-		goto out_free;
-	}
-
-	if (!scx_bpf_consume(cgid)) {
-		bpf_cgroup_release(cgrp);
-		stat_inc(FCG_STAT_PNC_EMPTY);
-		goto out_stash;
-	}
-
-	/*
-	 * Successfully consumed from the cgroup. This will be our current
-	 * cgroup for the new slice. Refresh its hweight.
-	 */
-	cgrp_refresh_hweight(cgrp, cgc);
-
-	bpf_cgroup_release(cgrp);
-
-	/*
-	 * As the cgroup may have more tasks, add it back to the rbtree. Note
-	 * that here we charge the full slice upfront and then exact later
-	 * according to the actual consumption. This prevents lowpri thundering
-	 * herd from saturating the machine.
-	 */
-	bpf_spin_lock(&cgv_tree_lock);
-	cgv_node->cvtime += cgrp_slice_ns * FCG_HWEIGHT_ONE / (cgc->hweight ?: 1);
-	cgrp_cap_budget(cgv_node, cgc);
-	bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
-	bpf_spin_unlock(&cgv_tree_lock);
-
-	*cgidp = cgid;
-	stat_inc(FCG_STAT_PNC_NEXT);
-	return true;
-
-out_stash:
-	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
-	if (!stash) {
-		stat_inc(FCG_STAT_PNC_GONE);
-		goto out_free;
-	}
-
-	/*
-	 * Paired with cmpxchg in cgrp_enqueued(). If they see the following
-	 * transition, they'll enqueue the cgroup. If they are earlier, we'll
-	 * see their task in the dq below and requeue the cgroup.
-	 */
-	__sync_val_compare_and_swap(&cgc->queued, 1, 0);
-
-	if (scx_bpf_dsq_nr_queued(cgid)) {
-		bpf_spin_lock(&cgv_tree_lock);
-		bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
-		bpf_spin_unlock(&cgv_tree_lock);
-		stat_inc(FCG_STAT_PNC_RACE);
-	} else {
-		cgv_node = bpf_kptr_xchg(&stash->node, cgv_node);
-		if (cgv_node) {
-			scx_bpf_error("unexpected !NULL cgv_node stash");
-			goto out_free;
-		}
-	}
-
-	return false;
-
-out_free:
-	bpf_obj_drop(cgv_node);
-	return false;
-}
-
-void BPF_STRUCT_OPS(fcg_dispatch, s32 cpu, struct task_struct *prev)
-{
-	struct fcg_cpu_ctx *cpuc;
-	struct fcg_cgrp_ctx *cgc;
-	struct cgroup *cgrp;
-	u64 now = bpf_ktime_get_ns();
-	bool picked_next = false;
-
-	cpuc = find_cpu_ctx();
-	if (!cpuc)
-		return;
-
-	if (!cpuc->cur_cgid)
-		goto pick_next_cgroup;
-
-	if (vtime_before(now, cpuc->cur_at + cgrp_slice_ns)) {
-		if (scx_bpf_consume(cpuc->cur_cgid)) {
-			stat_inc(FCG_STAT_CNS_KEEP);
-			return;
-		}
-		stat_inc(FCG_STAT_CNS_EMPTY);
-	} else {
-		stat_inc(FCG_STAT_CNS_EXPIRE);
-	}
-
-	/*
-	 * The current cgroup is expiring. It was already charged a full slice.
-	 * Calculate the actual usage and accumulate the delta.
-	 */
-	cgrp = bpf_cgroup_from_id(cpuc->cur_cgid);
-	if (!cgrp) {
-		stat_inc(FCG_STAT_CNS_GONE);
-		goto pick_next_cgroup;
-	}
-
-	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
-	if (cgc) {
-		/*
-		 * We want to update the vtime delta and then look for the next
-		 * cgroup to execute but the latter needs to be done in a loop
-		 * and we can't keep the lock held. Oh well...
-		 */
-		bpf_spin_lock(&cgv_tree_lock);
-		__sync_fetch_and_add(&cgc->cvtime_delta,
-				     (cpuc->cur_at + cgrp_slice_ns - now) *
-				     FCG_HWEIGHT_ONE / (cgc->hweight ?: 1));
-		bpf_spin_unlock(&cgv_tree_lock);
-	} else {
-		stat_inc(FCG_STAT_CNS_GONE);
-	}
-
-	bpf_cgroup_release(cgrp);
-
-pick_next_cgroup:
-	cpuc->cur_at = now;
-
-	if (scx_bpf_consume(SCX_DSQ_GLOBAL)) {
-		cpuc->cur_cgid = 0;
-		return;
-	}
-
-	bpf_repeat(CGROUP_MAX_RETRIES) {
-		if (try_pick_next_cgroup(&cpuc->cur_cgid)) {
-			picked_next = true;
-			break;
-		}
-	}
-
-	/*
-	 * This only happens if try_pick_next_cgroup() races against enqueue
-	 * path for more than CGROUP_MAX_RETRIES times, which is extremely
-	 * unlikely and likely indicates an underlying bug. There shouldn't be
-	 * any stall risk as the race is against enqueue.
-	 */
-	if (!picked_next)
-		stat_inc(FCG_STAT_PNC_FAIL);
-}
-
-s32 BPF_STRUCT_OPS(fcg_init_task, struct task_struct *p,
-		   struct scx_init_task_args *args)
-{
-	struct fcg_task_ctx *taskc;
-	struct fcg_cgrp_ctx *cgc;
-
-	/*
-	 * @p is new. Let's ensure that its task_ctx is available. We can sleep
-	 * in this function and the following will automatically use GFP_KERNEL.
-	 */
-	taskc = bpf_task_storage_get(&task_ctx, p, 0,
-				     BPF_LOCAL_STORAGE_GET_F_CREATE);
-	if (!taskc)
-		return -ENOMEM;
-
-	taskc->bypassed_at = 0;
-
-	if (!(cgc = find_cgrp_ctx(args->cgroup)))
-		return -ENOENT;
-
-	p->scx.dsq_vtime = cgc->tvtime_now;
-
-	return 0;
-}
-
-int BPF_STRUCT_OPS_SLEEPABLE(fcg_cgroup_init, struct cgroup *cgrp,
-			     struct scx_cgroup_init_args *args)
-{
-	struct fcg_cgrp_ctx *cgc;
-	struct cgv_node *cgv_node;
-	struct cgv_node_stash empty_stash = {}, *stash;
-	u64 cgid = cgrp->kn->id;
-	int ret;
-
-	/*
-	 * Technically incorrect as cgroup ID is full 64bit while dq ID is
-	 * 63bit. Should not be a problem in practice and easy to spot in the
-	 * unlikely case that it breaks.
-	 */
-	ret = scx_bpf_create_dsq(cgid, -1);
-	if (ret)
-		return ret;
-
-	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0,
-				   BPF_LOCAL_STORAGE_GET_F_CREATE);
-	if (!cgc) {
-		ret = -ENOMEM;
-		goto err_destroy_dsq;
-	}
-
-	cgc->weight = args->weight;
-	cgc->hweight = FCG_HWEIGHT_ONE;
-
-	ret = bpf_map_update_elem(&cgv_node_stash, &cgid, &empty_stash,
-				  BPF_NOEXIST);
-	if (ret) {
-		if (ret != -ENOMEM)
-			scx_bpf_error("unexpected stash creation error (%d)",
-				      ret);
-		goto err_destroy_dsq;
-	}
-
-	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
-	if (!stash) {
-		scx_bpf_error("unexpected cgv_node stash lookup failure");
-		ret = -ENOENT;
-		goto err_destroy_dsq;
-	}
-
-	cgv_node = bpf_obj_new(struct cgv_node);
-	if (!cgv_node) {
-		ret = -ENOMEM;
-		goto err_del_cgv_node;
-	}
-
-	cgv_node->cgid = cgid;
-	cgv_node->cvtime = cvtime_now;
-
-	cgv_node = bpf_kptr_xchg(&stash->node, cgv_node);
-	if (cgv_node) {
-		scx_bpf_error("unexpected !NULL cgv_node stash");
-		ret = -EBUSY;
-		goto err_drop;
-	}
-
-	return 0;
-
-err_drop:
-	bpf_obj_drop(cgv_node);
-err_del_cgv_node:
-	bpf_map_delete_elem(&cgv_node_stash, &cgid);
-err_destroy_dsq:
-	scx_bpf_destroy_dsq(cgid);
-	return ret;
-}
-
-void BPF_STRUCT_OPS(fcg_cgroup_exit, struct cgroup *cgrp)
-{
-	u64 cgid = cgrp->kn->id;
-
-	/*
-	 * For now, there's no way find and remove the cgv_node if it's on the
-	 * cgv_tree. Let's drain them in the dispatch path as they get popped
-	 * off the front of the tree.
-	 */
-	bpf_map_delete_elem(&cgv_node_stash, &cgid);
-	scx_bpf_destroy_dsq(cgid);
-}
-
-void BPF_STRUCT_OPS(fcg_cgroup_move, struct task_struct *p,
-		    struct cgroup *from, struct cgroup *to)
-{
-	struct fcg_cgrp_ctx *from_cgc, *to_cgc;
-	s64 vtime_delta;
-
-	/* find_cgrp_ctx() triggers scx_ops_error() on lookup failures */
-	if (!(from_cgc = find_cgrp_ctx(from)) || !(to_cgc = find_cgrp_ctx(to)))
-		return;
-
-	vtime_delta = p->scx.dsq_vtime - from_cgc->tvtime_now;
-	p->scx.dsq_vtime = to_cgc->tvtime_now + vtime_delta;
-}
-
-s32 BPF_STRUCT_OPS(fcg_init)
-{
-	if (!switch_partial)
-		scx_bpf_switch_all();
-	return 0;
-}
-
-void BPF_STRUCT_OPS(fcg_exit, struct scx_exit_info *ei)
-{
-	uei_record(&uei, ei);
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops flatcg_ops = {
-	.select_cpu		= (void *)fcg_select_cpu,
-	.enqueue		= (void *)fcg_enqueue,
-	.dispatch		= (void *)fcg_dispatch,
-	.runnable		= (void *)fcg_runnable,
-	.running		= (void *)fcg_running,
-	.stopping		= (void *)fcg_stopping,
-	.quiescent		= (void *)fcg_quiescent,
-	.init_task		= (void *)fcg_init_task,
-	.cgroup_set_weight	= (void *)fcg_cgroup_set_weight,
-	.cgroup_init		= (void *)fcg_cgroup_init,
-	.cgroup_exit		= (void *)fcg_cgroup_exit,
-	.cgroup_move		= (void *)fcg_cgroup_move,
-	.init			= (void *)fcg_init,
-	.exit			= (void *)fcg_exit,
-	.flags			= SCX_OPS_CGROUP_KNOB_WEIGHT | SCX_OPS_ENQ_EXITING,
-	.name			= "flatcg",
-};
diff --git a/tools/sched_ext/scx_flatcg.c b/tools/sched_ext/scx_flatcg.c
deleted file mode 100644
index 6c2f9715f..000000000
--- a/tools/sched_ext/scx_flatcg.c
+++ /dev/null
@@ -1,225 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2023 David Vernet <dvernet@meta.com>
- */
-#include <stdio.h>
-#include <signal.h>
-#include <unistd.h>
-#include <libgen.h>
-#include <limits.h>
-#include <inttypes.h>
-#include <fcntl.h>
-#include <time.h>
-#include <bpf/bpf.h>
-#include <scx/common.h>
-#include "scx_flatcg.h"
-#include "scx_flatcg.bpf.skel.h"
-
-#ifndef FILEID_KERNFS
-#define FILEID_KERNFS		0xfe
-#endif
-
-const char help_fmt[] =
-"A flattened cgroup hierarchy sched_ext scheduler.\n"
-"\n"
-"See the top-level comment in .bpf.c for more details.\n"
-"\n"
-"Usage: %s [-s SLICE_US] [-i INTERVAL] [-f] [-p]\n"
-"\n"
-"  -s SLICE_US   Override slice duration\n"
-"  -i INTERVAL   Report interval\n"
-"  -f            Use FIFO scheduling instead of weighted vtime scheduling\n"
-"  -p            Switch only tasks on SCHED_EXT policy intead of all\n"
-"  -h            Display this help and exit\n";
-
-static volatile int exit_req;
-
-static void sigint_handler(int dummy)
-{
-	exit_req = 1;
-}
-
-static float read_cpu_util(__u64 *last_sum, __u64 *last_idle)
-{
-	FILE *fp;
-	char buf[4096];
-	char *line, *cur = NULL, *tok;
-	__u64 sum = 0, idle = 0;
-	__u64 delta_sum, delta_idle;
-	int idx;
-
-	fp = fopen("/proc/stat", "r");
-	if (!fp) {
-		perror("fopen(\"/proc/stat\")");
-		return 0.0;
-	}
-
-	if (!fgets(buf, sizeof(buf), fp)) {
-		perror("fgets(\"/proc/stat\")");
-		fclose(fp);
-		return 0.0;
-	}
-	fclose(fp);
-
-	line = buf;
-	for (idx = 0; (tok = strtok_r(line, " \n", &cur)); idx++) {
-		char *endp = NULL;
-		__u64 v;
-
-		if (idx == 0) {
-			line = NULL;
-			continue;
-		}
-		v = strtoull(tok, &endp, 0);
-		if (!endp || *endp != '\0') {
-			fprintf(stderr, "failed to parse %dth field of /proc/stat (\"%s\")\n",
-				idx, tok);
-			continue;
-		}
-		sum += v;
-		if (idx == 4)
-			idle = v;
-	}
-
-	delta_sum = sum - *last_sum;
-	delta_idle = idle - *last_idle;
-	*last_sum = sum;
-	*last_idle = idle;
-
-	return delta_sum ? (float)(delta_sum - delta_idle) / delta_sum : 0.0;
-}
-
-static void fcg_read_stats(struct scx_flatcg *skel, __u64 *stats)
-{
-	__u64 cnts[FCG_NR_STATS][skel->rodata->nr_cpus];
-	__u32 idx;
-
-	memset(stats, 0, sizeof(stats[0]) * FCG_NR_STATS);
-
-	for (idx = 0; idx < FCG_NR_STATS; idx++) {
-		int ret, cpu;
-
-		ret = bpf_map_lookup_elem(bpf_map__fd(skel->maps.stats),
-					  &idx, cnts[idx]);
-		if (ret < 0)
-			continue;
-		for (cpu = 0; cpu < skel->rodata->nr_cpus; cpu++)
-			stats[idx] += cnts[idx][cpu];
-	}
-}
-
-int main(int argc, char **argv)
-{
-	struct scx_flatcg *skel;
-	struct bpf_link *link;
-	struct timespec intv_ts = { .tv_sec = 2, .tv_nsec = 0 };
-	bool dump_cgrps = false;
-	__u64 last_cpu_sum = 0, last_cpu_idle = 0;
-	__u64 last_stats[FCG_NR_STATS] = {};
-	unsigned long seq = 0;
-	__s32 opt;
-
-	signal(SIGINT, sigint_handler);
-	signal(SIGTERM, sigint_handler);
-
-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
-
-	skel = scx_flatcg__open();
-	SCX_BUG_ON(!skel, "Failed to open skel");
-
-	skel->rodata->nr_cpus = libbpf_num_possible_cpus();
-
-	while ((opt = getopt(argc, argv, "s:i:dfph")) != -1) {
-		double v;
-
-		switch (opt) {
-		case 's':
-			v = strtod(optarg, NULL);
-			skel->rodata->cgrp_slice_ns = v * 1000;
-			break;
-		case 'i':
-			v = strtod(optarg, NULL);
-			intv_ts.tv_sec = v;
-			intv_ts.tv_nsec = (v - (float)intv_ts.tv_sec) * 1000000000;
-			break;
-		case 'd':
-			dump_cgrps = true;
-			break;
-		case 'f':
-			skel->rodata->fifo_sched = true;
-			break;
-		case 'p':
-			skel->rodata->switch_partial = true;
-			break;
-		case 'h':
-		default:
-			fprintf(stderr, help_fmt, basename(argv[0]));
-			return opt != 'h';
-		}
-	}
-
-	printf("slice=%.1lfms intv=%.1lfs dump_cgrps=%d",
-	       (double)skel->rodata->cgrp_slice_ns / 1000000.0,
-	       (double)intv_ts.tv_sec + (double)intv_ts.tv_nsec / 1000000000.0,
-	       dump_cgrps);
-
-	SCX_BUG_ON(scx_flatcg__load(skel), "Failed to load skel");
-
-	link = bpf_map__attach_struct_ops(skel->maps.flatcg_ops);
-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
-
-	while (!exit_req && !uei_exited(&skel->bss->uei)) {
-		__u64 acc_stats[FCG_NR_STATS];
-		__u64 stats[FCG_NR_STATS];
-		float cpu_util;
-		int i;
-
-		cpu_util = read_cpu_util(&last_cpu_sum, &last_cpu_idle);
-
-		fcg_read_stats(skel, acc_stats);
-		for (i = 0; i < FCG_NR_STATS; i++)
-			stats[i] = acc_stats[i] - last_stats[i];
-
-		memcpy(last_stats, acc_stats, sizeof(acc_stats));
-
-		printf("\n[SEQ %6lu cpu=%5.1lf hweight_gen=%" PRIu64 "]\n",
-		       seq++, cpu_util * 100.0, skel->data->hweight_gen);
-		printf("       act:%6llu  deact:%6llu global:%6llu local:%6llu\n",
-		       stats[FCG_STAT_ACT],
-		       stats[FCG_STAT_DEACT],
-		       stats[FCG_STAT_GLOBAL],
-		       stats[FCG_STAT_LOCAL]);
-		printf("HWT  cache:%6llu update:%6llu   skip:%6llu  race:%6llu\n",
-		       stats[FCG_STAT_HWT_CACHE],
-		       stats[FCG_STAT_HWT_UPDATES],
-		       stats[FCG_STAT_HWT_SKIP],
-		       stats[FCG_STAT_HWT_RACE]);
-		printf("ENQ   skip:%6llu   race:%6llu\n",
-		       stats[FCG_STAT_ENQ_SKIP],
-		       stats[FCG_STAT_ENQ_RACE]);
-		printf("CNS   keep:%6llu expire:%6llu  empty:%6llu  gone:%6llu\n",
-		       stats[FCG_STAT_CNS_KEEP],
-		       stats[FCG_STAT_CNS_EXPIRE],
-		       stats[FCG_STAT_CNS_EMPTY],
-		       stats[FCG_STAT_CNS_GONE]);
-		printf("PNC   next:%6llu  empty:%6llu nocgrp:%6llu  gone:%6llu race:%6llu fail:%6llu\n",
-		       stats[FCG_STAT_PNC_NEXT],
-		       stats[FCG_STAT_PNC_EMPTY],
-		       stats[FCG_STAT_PNC_NO_CGRP],
-		       stats[FCG_STAT_PNC_GONE],
-		       stats[FCG_STAT_PNC_RACE],
-		       stats[FCG_STAT_PNC_FAIL]);
-		printf("BAD remove:%6llu\n",
-		       acc_stats[FCG_STAT_BAD_REMOVAL]);
-		fflush(stdout);
-
-		nanosleep(&intv_ts, NULL);
-	}
-
-	bpf_link__destroy(link);
-	uei_print(&skel->bss->uei);
-	scx_flatcg__destroy(skel);
-	return 0;
-}
diff --git a/tools/sched_ext/scx_flatcg.h b/tools/sched_ext/scx_flatcg.h
deleted file mode 100644
index 6f2ea50ac..000000000
--- a/tools/sched_ext/scx_flatcg.h
+++ /dev/null
@@ -1,51 +0,0 @@
-#ifndef __SCX_EXAMPLE_FLATCG_H
-#define __SCX_EXAMPLE_FLATCG_H
-
-enum {
-	FCG_HWEIGHT_ONE		= 1LLU << 16,
-};
-
-enum fcg_stat_idx {
-	FCG_STAT_ACT,
-	FCG_STAT_DEACT,
-	FCG_STAT_LOCAL,
-	FCG_STAT_GLOBAL,
-
-	FCG_STAT_HWT_UPDATES,
-	FCG_STAT_HWT_CACHE,
-	FCG_STAT_HWT_SKIP,
-	FCG_STAT_HWT_RACE,
-
-	FCG_STAT_ENQ_SKIP,
-	FCG_STAT_ENQ_RACE,
-
-	FCG_STAT_CNS_KEEP,
-	FCG_STAT_CNS_EXPIRE,
-	FCG_STAT_CNS_EMPTY,
-	FCG_STAT_CNS_GONE,
-
-	FCG_STAT_PNC_NO_CGRP,
-	FCG_STAT_PNC_NEXT,
-	FCG_STAT_PNC_EMPTY,
-	FCG_STAT_PNC_GONE,
-	FCG_STAT_PNC_RACE,
-	FCG_STAT_PNC_FAIL,
-
-	FCG_STAT_BAD_REMOVAL,
-
-	FCG_NR_STATS,
-};
-
-struct fcg_cgrp_ctx {
-	u32			nr_active;
-	u32			nr_runnable;
-	u32			queued;
-	u32			weight;
-	u32			hweight;
-	u64			child_weight_sum;
-	u64			hweight_gen;
-	s64			cvtime_delta;
-	u64			tvtime_now;
-};
-
-#endif /* __SCX_EXAMPLE_FLATCG_H */
diff --git a/tools/sched_ext/scx_layered/.gitignore b/tools/sched_ext/scx_layered/.gitignore
deleted file mode 100644
index 186dba259..000000000
--- a/tools/sched_ext/scx_layered/.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-src/bpf/.output
-Cargo.lock
-target
diff --git a/tools/sched_ext/scx_layered/Cargo.toml b/tools/sched_ext/scx_layered/Cargo.toml
deleted file mode 100644
index 37a811e38..000000000
--- a/tools/sched_ext/scx_layered/Cargo.toml
+++ /dev/null
@@ -1,28 +0,0 @@
-[package]
-name = "scx_layered"
-version = "0.0.4"
-authors = ["Tejun Heo <htejun@meta.com>", "Meta"]
-edition = "2021"
-description = "Userspace scheduling with BPF for Ads"
-license = "GPL-2.0-only"
-
-[dependencies]
-anyhow = "1.0"
-bitvec = "1.0"
-clap = { version = "4.1", features = ["derive", "env", "unicode", "wrap_help"] }
-ctrlc = { version = "3.1", features = ["termination"] }
-fb_procfs = "0.7"
-lazy_static = "1.4"
-libbpf-rs = "0.22"
-libc = "0.2"
-log = "0.4"
-scx_utils = "0.5"
-serde = { version = "1.0", features = ["derive"] }
-serde_json = "1.0"
-simplelog = "0.12"
-
-[build-dependencies]
-scx_utils = "0.5"
-
-[features]
-enable_backtrace = []
diff --git a/tools/sched_ext/scx_layered/README.md b/tools/sched_ext/scx_layered/README.md
deleted file mode 100644
index 37c554b23..000000000
--- a/tools/sched_ext/scx_layered/README.md
+++ /dev/null
@@ -1,37 +0,0 @@
-# scx_layered
-
-This is a single user-defined scheduler used within [sched_ext](https://github.com/sched-ext/scx/tree/main), which is a Linux kernel feature which enables implementing kernel thread schedulers in BPF and dynamically loading them. [Read more about sched_ext](https://github.com/sched-ext/scx/tree/main).
-
-## Overview
-
-A highly configurable multi-layer BPF / user space hybrid scheduler.
-
-scx_layered allows the user to classify tasks into multiple layers, and apply
-different scheduling policies to those layers. For example, a layer could be
-created of all tasks that are part of the `user.slice` cgroup slice, and a
-policy could be specified that ensures that the layer is given at least 80% CPU
-utilization for some subset of CPUs on the system.
-
-## How To Install
-
-Available as a [Rust crate](https://crates.io/crates/scx_layered): `cargo add scx_layered`
-
-## Typical Use Case
-
-scx_layered is designed to be highly customizable, and can be targeted for
-specific applications. For example, if you had a high-priority service that
-required priority access to all but 1 physical core to ensure acceptable p99
-latencies, you could specify that the service would get priority access to all
-but 1 core on the system. If that service ends up not utilizing all of those
-cores, they could be used by other layers until they're needed.
-
-## Production Ready?
-
-Yes. If tuned correctly, scx_layered should be performant across various CPU
-architectures and workloads.
-
-That said, you may run into an issue with infeasible weights, where a task with
-a very high weight may cause the scheduler to incorrectly leave cores idle
-because it thinks they're necessary to accommodate the compute for a single
-task. This can also happen in CFS, and should soon be addressed for
-scx_layered.
diff --git a/tools/sched_ext/scx_layered/build.rs b/tools/sched_ext/scx_layered/build.rs
deleted file mode 100644
index d26db839c..000000000
--- a/tools/sched_ext/scx_layered/build.rs
+++ /dev/null
@@ -1,13 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-//
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-
-fn main() {
-    scx_utils::BpfBuilder::new()
-        .unwrap()
-        .enable_intf("src/bpf/intf.h", "bpf_intf.rs")
-        .enable_skel("src/bpf/main.bpf.c", "bpf")
-        .build()
-        .unwrap();
-}
diff --git a/tools/sched_ext/scx_layered/rustfmt.toml b/tools/sched_ext/scx_layered/rustfmt.toml
deleted file mode 100644
index b7258ed0a..000000000
--- a/tools/sched_ext/scx_layered/rustfmt.toml
+++ /dev/null
@@ -1,8 +0,0 @@
-# Get help on options with `rustfmt --help=config`
-# Please keep these in alphabetical order.
-edition = "2021"
-group_imports = "StdExternalCrate"
-imports_granularity = "Item"
-merge_derives = false
-use_field_init_shorthand = true
-version = "Two"
diff --git a/tools/sched_ext/scx_layered/src/bpf/intf.h b/tools/sched_ext/scx_layered/src/bpf/intf.h
deleted file mode 100644
index 000f48b4d..000000000
--- a/tools/sched_ext/scx_layered/src/bpf/intf.h
+++ /dev/null
@@ -1,100 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-#ifndef __INTF_H
-#define __INTF_H
-
-#include <stdbool.h>
-#ifndef __kptr
-#ifdef __KERNEL__
-#error "__kptr_ref not defined in the kernel"
-#endif
-#define __kptr
-#endif
-
-#ifndef __KERNEL__
-typedef unsigned long long u64;
-typedef long long s64;
-#endif
-
-#include <scx/ravg.bpf.h>
-
-enum consts {
-	MAX_CPUS_SHIFT		= 9,
-	MAX_CPUS		= 1 << MAX_CPUS_SHIFT,
-	MAX_CPUS_U8		= MAX_CPUS / 8,
-	MAX_TASKS		= 131072,
-	MAX_PATH		= 4096,
-	MAX_COMM		= 16,
-	MAX_LAYER_MATCH_ORS	= 32,
-	MAX_LAYERS		= 16,
-	USAGE_HALF_LIFE		= 100000000,	/* 100ms */
-
-	/* XXX remove */
-	MAX_CGRP_PREFIXES = 32
-};
-
-/* Statistics */
-enum global_stat_idx {
-	GSTAT_TASK_CTX_FREE_FAILED,
-	NR_GSTATS,
-};
-
-enum layer_stat_idx {
-	LSTAT_LOCAL,
-	LSTAT_GLOBAL,
-	LSTAT_OPEN_IDLE,
-	LSTAT_AFFN_VIOL,
-	LSTAT_PREEMPT,
-	NR_LSTATS,
-};
-
-struct cpu_ctx {
-	bool			current_preempt;
-	u64			layer_cycles[MAX_LAYERS];
-	u64			gstats[NR_GSTATS];
-	u64			lstats[MAX_LAYERS][NR_LSTATS];
-};
-
-enum layer_match_kind {
-	MATCH_CGROUP_PREFIX,
-	MATCH_COMM_PREFIX,
-	MATCH_NICE_ABOVE,
-	MATCH_NICE_BELOW,
-
-	NR_LAYER_MATCH_KINDS,
-};
-
-struct layer_match {
-	int		kind;
-	char		cgroup_prefix[MAX_PATH];
-	char		comm_prefix[MAX_COMM];
-	int		nice_above_or_below;
-};
-
-struct layer_match_ands {
-	struct layer_match	matches[NR_LAYER_MATCH_KINDS];
-	int			nr_match_ands;
-};
-
-struct layer {
-	struct layer_match_ands	matches[MAX_LAYER_MATCH_ORS];
-	unsigned int		nr_match_ors;
-	unsigned int		idx;
-	bool			open;
-	bool			preempt;
-
-	u64			vtime_now;
-	u64			nr_tasks;
-
-	u64			load;
-	struct ravg_data	load_rd;
-
-	u64			cpus_seq;
-	unsigned int		refresh_cpus;
-	unsigned char		cpus[MAX_CPUS_U8];
-	unsigned int		nr_cpus;	// managed from BPF side
-};
-
-#endif /* __INTF_H */
diff --git a/tools/sched_ext/scx_layered/src/bpf/main.bpf.c b/tools/sched_ext/scx_layered/src/bpf/main.bpf.c
deleted file mode 100644
index 21dd0e4cd..000000000
--- a/tools/sched_ext/scx_layered/src/bpf/main.bpf.c
+++ /dev/null
@@ -1,979 +0,0 @@
-/* Copyright (c) Meta Platforms, Inc. and affiliates. */
-#include <scx/common.bpf.h>
-#include <scx/ravg_impl.bpf.h>
-#include "intf.h"
-
-#include <errno.h>
-#include <stdbool.h>
-#include <string.h>
-#include <bpf/bpf_core_read.h>
-#include <bpf/bpf_helpers.h>
-#include <bpf/bpf_tracing.h>
-
-char _license[] SEC("license") = "GPL";
-
-const volatile u32 debug = 0;
-const volatile u64 slice_ns = SCX_SLICE_DFL;
-const volatile u32 nr_possible_cpus = 1;
-const volatile u32 nr_layers = 1;
-const volatile bool smt_enabled = true;
-const volatile unsigned char all_cpus[MAX_CPUS_U8];
-
-private(all_cpumask) struct bpf_cpumask __kptr *all_cpumask;
-struct layer layers[MAX_LAYERS];
-u32 fallback_cpu;
-static u32 preempt_cursor;
-
-#define dbg(fmt, args...)	do { if (debug) bpf_printk(fmt, ##args); } while (0)
-#define trace(fmt, args...)	do { if (debug > 1) bpf_printk(fmt, ##args); } while (0)
-
-#include "util.bpf.c"
-
-struct user_exit_info uei;
-
-static inline bool vtime_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
-struct {
-	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
-	__type(key, u32);
-	__type(value, struct cpu_ctx);
-	__uint(max_entries, 1);
-} cpu_ctxs SEC(".maps");
-
-static struct cpu_ctx *lookup_cpu_ctx(int cpu)
-{
-	struct cpu_ctx *cctx;
-	u32 zero = 0;
-
-	if (cpu < 0)
-		cctx = bpf_map_lookup_elem(&cpu_ctxs, &zero);
-	else
-		cctx = bpf_map_lookup_percpu_elem(&cpu_ctxs, &zero, cpu);
-
-	if (!cctx) {
-		scx_bpf_error("no cpu_ctx for cpu %d", cpu);
-		return NULL;
-	}
-
-	return cctx;
-}
-
-static void gstat_inc(enum global_stat_idx idx, struct cpu_ctx *cctx)
-{
-	if (idx < 0 || idx >= NR_GSTATS) {
-		scx_bpf_error("invalid global stat idx %d", idx);
-		return;
-	}
-
-	cctx->gstats[idx]++;
-}
-
-static void lstat_inc(enum layer_stat_idx idx, struct layer *layer, struct cpu_ctx *cctx)
-{
-	u64 *vptr;
-
-	if ((vptr = MEMBER_VPTR(*cctx, .lstats[layer->idx][idx])))
-		(*vptr)++;
-	else
-		scx_bpf_error("invalid layer or stat idxs: %d, %d", idx, layer->idx);
-}
-
-struct lock_wrapper {
-	struct bpf_spin_lock	lock;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__type(key, u32);
-	__type(value, struct lock_wrapper);
-	__uint(max_entries, MAX_LAYERS);
-	__uint(map_flags, 0);
-} layer_load_locks SEC(".maps");
-
-static void adj_load(u32 layer_idx, s64 adj, u64 now)
-{
-	struct layer *layer;
-	struct lock_wrapper *lockw;
-
-	layer = MEMBER_VPTR(layers, [layer_idx]);
-	lockw = bpf_map_lookup_elem(&layer_load_locks, &layer_idx);
-
-	if (!layer || !lockw) {
-		scx_bpf_error("Can't access layer%d or its load_lock", layer_idx);
-		return;
-	}
-
-	bpf_spin_lock(&lockw->lock);
-	layer->load += adj;
-	ravg_accumulate(&layer->load_rd, layer->load, now, USAGE_HALF_LIFE);
-	bpf_spin_unlock(&lockw->lock);
-
-	if (debug && adj < 0 && (s64)layer->load < 0)
-		scx_bpf_error("cpu%d layer%d load underflow (load=%lld adj=%lld)",
-			      bpf_get_smp_processor_id(), layer_idx, layer->load, adj);
-}
-
-struct layer_cpumask_wrapper {
-	struct bpf_cpumask __kptr *cpumask;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__type(key, u32);
-	__type(value, struct layer_cpumask_wrapper);
-	__uint(max_entries, MAX_LAYERS);
-	__uint(map_flags, 0);
-} layer_cpumasks SEC(".maps");
-
-static struct cpumask *lookup_layer_cpumask(int idx)
-{
-	struct layer_cpumask_wrapper *cpumaskw;
-
-	if ((cpumaskw = bpf_map_lookup_elem(&layer_cpumasks, &idx))) {
-		return (struct cpumask *)cpumaskw->cpumask;
-	} else {
-		scx_bpf_error("no layer_cpumask");
-		return NULL;
-	}
-}
-
-static void refresh_cpumasks(int idx)
-{
-	struct layer_cpumask_wrapper *cpumaskw;
-	struct layer *layer;
-	int cpu, total = 0;
-
-	if (!__sync_val_compare_and_swap(&layers[idx].refresh_cpus, 1, 0))
-		return;
-
-	cpumaskw = bpf_map_lookup_elem(&layer_cpumasks, &idx);
-
-	bpf_for(cpu, 0, nr_possible_cpus) {
-		u8 *u8_ptr;
-
-		if ((u8_ptr = MEMBER_VPTR(layers, [idx].cpus[cpu / 8]))) {
-			/*
-			 * XXX - The following test should be outside the loop
-			 * but that makes the verifier think that
-			 * cpumaskw->cpumask might be NULL in the loop.
-			 */
-			barrier_var(cpumaskw);
-			if (!cpumaskw || !cpumaskw->cpumask) {
-				scx_bpf_error("can't happen");
-				return;
-			}
-
-			if (*u8_ptr & (1 << (cpu % 8))) {
-				bpf_cpumask_set_cpu(cpu, cpumaskw->cpumask);
-				total++;
-			} else {
-				bpf_cpumask_clear_cpu(cpu, cpumaskw->cpumask);
-			}
-		} else {
-			scx_bpf_error("can't happen");
-		}
-	}
-
-	// XXX - shouldn't be necessary
-	layer = MEMBER_VPTR(layers, [idx]);
-	if (!layer) {
-		scx_bpf_error("can't happen");
-		return;
-	}
-
-	layer->nr_cpus = total;
-	__sync_fetch_and_add(&layer->cpus_seq, 1);
-	trace("LAYER[%d] now has %d cpus, seq=%llu", idx, layer->nr_cpus, layer->cpus_seq);
-}
-
-SEC("fentry/scheduler_tick")
-int scheduler_tick_fentry(const void *ctx)
-{
-	int idx;
-
-	if (bpf_get_smp_processor_id() == 0)
-		bpf_for(idx, 0, nr_layers)
-			refresh_cpumasks(idx);
-	return 0;
-}
-
-struct task_ctx {
-	int			pid;
-
-	int			layer;
-	bool			refresh_layer;
-	u64			layer_cpus_seq;
-	struct bpf_cpumask __kptr *layered_cpumask;
-
-	bool			all_cpus_allowed;
-	bool			dispatch_local;
-	u64			started_running_at;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_HASH);
-	__type(key, pid_t);
-	__type(value, struct task_ctx);
-	__uint(max_entries, MAX_TASKS);
-	__uint(map_flags, 0);
-} task_ctxs SEC(".maps");
-
-struct task_ctx *lookup_task_ctx_may_fail(struct task_struct *p)
-{
-	s32 pid = p->pid;
-
-	return bpf_map_lookup_elem(&task_ctxs, &pid);
-}
-
-struct task_ctx *lookup_task_ctx(struct task_struct *p)
-{
-	struct task_ctx *tctx;
-	s32 pid = p->pid;
-
-	if ((tctx = bpf_map_lookup_elem(&task_ctxs, &pid))) {
-		return tctx;
-	} else {
-		scx_bpf_error("task_ctx lookup failed");
-		return NULL;
-	}
-}
-
-struct layer *lookup_layer(int idx)
-{
-	if (idx < 0 || idx >= nr_layers) {
-		scx_bpf_error("invalid layer %d", idx);
-		return NULL;
-	}
-	return &layers[idx];
-}
-
-/*
- * Because the layer membership is by the default hierarchy cgroups rather than
- * the CPU controller membership, we can't use ops.cgroup_move(). Let's iterate
- * the tasks manually and set refresh_layer.
- *
- * The iteration isn't synchronized and may fail spuriously. It's not a big
- * practical problem as process migrations are very rare in most modern systems.
- * That said, we eventually want this to be based on CPU controller membership.
- */
-SEC("tp_btf/cgroup_attach_task")
-int BPF_PROG(tp_cgroup_attach_task, struct cgroup *cgrp, const char *cgrp_path,
-	     struct task_struct *leader, bool threadgroup)
-{
-	struct list_head *thread_head;
-	struct task_struct *next;
-	struct task_ctx *tctx;
-	int leader_pid = leader->pid;
-
-	if (!(tctx = lookup_task_ctx_may_fail(leader)))
-		return 0;
-	tctx->refresh_layer = true;
-
-	if (!threadgroup)
-		return 0;
-
-	thread_head = &leader->signal->thread_head;
-
-	if (!(next = bpf_task_acquire(leader))) {
-		scx_bpf_error("failed to acquire leader");
-		return 0;
-	}
-
-	bpf_repeat(MAX_TASKS) {
-		struct task_struct *p;
-		int pid;
-
-		p = container_of(next->thread_node.next, struct task_struct, thread_node);
-		bpf_task_release(next);
-
-		if (&p->thread_node == thread_head) {
-			next = NULL;
-			break;
-		}
-
-		pid = BPF_CORE_READ(p, pid);
-		next = bpf_task_from_pid(pid);
-		if (!next) {
-			bpf_printk("scx_layered: tp_cgroup_attach_task: thread iteration failed");
-			break;
-		}
-
-		if ((tctx = lookup_task_ctx(next)))
-			tctx->refresh_layer = true;
-	}
-
-	if (next)
-		bpf_task_release(next);
-	return 0;
-}
-
-SEC("tp_btf/task_rename")
-int BPF_PROG(tp_task_rename, struct task_struct *p, const char *buf)
-{
-	struct task_ctx *tctx;
-
-	if ((tctx = lookup_task_ctx_may_fail(p)))
-		tctx->refresh_layer = true;
-	return 0;
-}
-
-static void maybe_refresh_layered_cpumask(struct cpumask *layered_cpumask,
-					  struct task_struct *p, struct task_ctx *tctx,
-					  const struct cpumask *layer_cpumask)
-{
-	u64 layer_seq = layers->cpus_seq;
-
-	if (tctx->layer_cpus_seq == layer_seq)
-		return;
-
-	/*
-	 * XXX - We're assuming that the updated @layer_cpumask matching the new
-	 * @layer_seq is visible which may not be true. For now, leave it as-is.
-	 * Let's update once BPF grows enough memory ordering constructs.
-	 */
-	bpf_cpumask_and((struct bpf_cpumask *)layered_cpumask, layer_cpumask, p->cpus_ptr);
-	tctx->layer_cpus_seq = layer_seq;
-	trace("%s[%d] cpumask refreshed to seq %llu", p->comm, p->pid, layer_seq);
-}
-
-static s32 pick_idle_cpu_from(const struct cpumask *cand_cpumask, s32 prev_cpu,
-			      const struct cpumask *idle_cpumask,
-			      const struct cpumask *idle_smtmask)
-{
-	bool prev_in_cand = bpf_cpumask_test_cpu(prev_cpu, cand_cpumask);
-	s32 cpu;
-
-	/*
-	 * If CPU has SMT, any wholly idle CPU is likely a better pick than
-	 * partially idle @prev_cpu.
-	 */
-	if (smt_enabled) {
-		if (prev_in_cand &&
-		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
-		    scx_bpf_test_and_clear_cpu_idle(prev_cpu))
-			return prev_cpu;
-
-		cpu = scx_bpf_pick_idle_cpu(cand_cpumask, SCX_PICK_IDLE_CORE);
-		if (cpu >= 0)
-			return cpu;
-	}
-
-	if (prev_in_cand && scx_bpf_test_and_clear_cpu_idle(prev_cpu))
-		return prev_cpu;
-
-	return scx_bpf_pick_idle_cpu(cand_cpumask, 0);
-}
-
-s32 BPF_STRUCT_OPS(layered_select_cpu, struct task_struct *p, s32 prev_cpu, u64 wake_flags)
-{
-	const struct cpumask *idle_cpumask, *idle_smtmask;
-	struct cpumask *layer_cpumask, *layered_cpumask;
-	struct cpu_ctx *cctx;
-	struct task_ctx *tctx;
-	struct layer *layer;
-	s32 cpu;
-
-	/* look up everything we need */
-	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)) ||
-	    !(layered_cpumask = (struct cpumask *)tctx->layered_cpumask))
-		return prev_cpu;
-
-	/*
-	 * We usually update the layer in layered_runnable() to avoid confusing.
-	 * As layered_select_cpu() takes place before runnable, new tasks would
-	 * still have -1 layer. Just return @prev_cpu.
-	 */
-	if (tctx->layer < 0)
-		return prev_cpu;
-
-	if (!(layer = lookup_layer(tctx->layer)) ||
-	    !(layer_cpumask = lookup_layer_cpumask(tctx->layer)))
-		return prev_cpu;
-
-	if (!(idle_cpumask = scx_bpf_get_idle_cpumask()))
-		return prev_cpu;
-
-	if (!(idle_smtmask = scx_bpf_get_idle_smtmask())) {
-		cpu = prev_cpu;
-		goto out_put_idle_cpumask;
-	}
-
-	/* not much to do if bound to a single CPU */
-	if (p->nr_cpus_allowed == 1) {
-		cpu = prev_cpu;
-		if (scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
-			if (!bpf_cpumask_test_cpu(cpu, layer_cpumask))
-				lstat_inc(LSTAT_AFFN_VIOL, layer, cctx);
-			goto dispatch_local;
-		} else {
-			goto out_put_cpumasks;
-		}
-	}
-
-	maybe_refresh_layered_cpumask(layered_cpumask, p, tctx, layer_cpumask);
-
-	/*
-	 * If CPU has SMT, any wholly idle CPU is likely a better pick than
-	 * partially idle @prev_cpu.
-	 */
-	if ((cpu = pick_idle_cpu_from(layered_cpumask, prev_cpu,
-				      idle_cpumask, idle_smtmask)) >= 0)
-		goto dispatch_local;
-
-	/*
-	 * If the layer is an open one, we can try the whole machine.
-	 */
-	if (layer->open &&
-	    ((cpu = pick_idle_cpu_from(p->cpus_ptr, prev_cpu,
-				       idle_cpumask, idle_smtmask)) >= 0)) {
-		lstat_inc(LSTAT_OPEN_IDLE, layer, cctx);
-		goto dispatch_local;
-	}
-
-	cpu = prev_cpu;
-	goto out_put_cpumasks;
-
-dispatch_local:
-	tctx->dispatch_local = true;
-out_put_cpumasks:
-	scx_bpf_put_idle_cpumask(idle_smtmask);
-out_put_idle_cpumask:
-	scx_bpf_put_idle_cpumask(idle_cpumask);
-	return cpu;
-}
-
-void BPF_STRUCT_OPS(layered_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	struct cpu_ctx *cctx;
-	struct task_ctx *tctx;
-	struct layer *layer;
-	u64 vtime = p->scx.dsq_vtime;
-	u32 idx;
-
-	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)) ||
-	    !(layer = lookup_layer(tctx->layer)))
-		return;
-
-	if (tctx->dispatch_local) {
-		tctx->dispatch_local = false;
-		lstat_inc(LSTAT_LOCAL, layer, cctx);
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, enq_flags);
-		return;
-	}
-
-	lstat_inc(LSTAT_GLOBAL, layer, cctx);
-
-	/*
-	 * Limit the amount of budget that an idling task can accumulate
-	 * to one slice.
-	 */
-	if (vtime_before(vtime, layer->vtime_now - slice_ns))
-		vtime = layer->vtime_now - slice_ns;
-
-	if (!tctx->all_cpus_allowed) {
-		lstat_inc(LSTAT_AFFN_VIOL, layer, cctx);
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, slice_ns, enq_flags);
-		return;
-	}
-
-	scx_bpf_dispatch_vtime(p, tctx->layer, slice_ns, vtime, enq_flags);
-
-	if (!layer->preempt)
-		return;
-
-	bpf_for(idx, 0, nr_possible_cpus) {
-		struct cpu_ctx *cand_cctx;
-		u32 cpu = (preempt_cursor + idx) % nr_possible_cpus;
-
-		if (!all_cpumask ||
-		    !bpf_cpumask_test_cpu(cpu, (const struct cpumask *)all_cpumask))
-			continue;
-		if (!(cand_cctx = lookup_cpu_ctx(cpu)) || cand_cctx->current_preempt)
-			continue;
-
-		scx_bpf_kick_cpu(cpu, SCX_KICK_PREEMPT);
-
-		/*
-		 * Round-robining doesn't have to be strict. Let's not bother
-		 * with atomic ops on $preempt_cursor.
-		 */
-		preempt_cursor = (cpu + 1) % nr_possible_cpus;
-
-		lstat_inc(LSTAT_PREEMPT, layer, cctx);
-		break;
-	}
-}
-
-void BPF_STRUCT_OPS(layered_dispatch, s32 cpu, struct task_struct *prev)
-{
-	int idx;
-
-	/* consume preempting layers first */
-	bpf_for(idx, 0, nr_layers)
-		if (layers[idx].preempt && scx_bpf_consume(idx))
-			return;
-
-	/* consume !open layers second */
-	bpf_for(idx, 0, nr_layers) {
-		struct layer *layer = &layers[idx];
-		struct cpumask *layer_cpumask;
-
-		if (layer->open)
-			continue;
-
-		/* consume matching layers */
-		if (!(layer_cpumask = lookup_layer_cpumask(idx)))
-			return;
-
-		if (bpf_cpumask_test_cpu(cpu, layer_cpumask) ||
-		    (cpu == fallback_cpu && layer->nr_cpus == 0)) {
-			if (scx_bpf_consume(idx))
-				return;
-		}
-	}
-
-	/* consume !preempting open layers */
-	bpf_for(idx, 0, nr_layers) {
-		if (!layers[idx].preempt && layers[idx].open &&
-		    scx_bpf_consume(idx))
-			return;
-	}
-}
-
-static bool match_one(struct layer_match *match, struct task_struct *p, const char *cgrp_path)
-{
-	switch (match->kind) {
-	case MATCH_CGROUP_PREFIX: {
-		return match_prefix(match->cgroup_prefix, cgrp_path, MAX_PATH);
-	}
-	case MATCH_COMM_PREFIX: {
-		char comm[MAX_COMM];
-		memcpy(comm, p->comm, MAX_COMM);
-		return match_prefix(match->comm_prefix, comm, MAX_COMM);
-	}
-	case MATCH_NICE_ABOVE:
-		return (s32)p->static_prio - 120 > match->nice_above_or_below;
-	case MATCH_NICE_BELOW:
-		return (s32)p->static_prio - 120 < match->nice_above_or_below;
-	default:
-		scx_bpf_error("invalid match kind %d", match->kind);
-		return false;
-	}
-}
-
-static bool match_layer(struct layer *layer, struct task_struct *p, const char *cgrp_path)
-{
-	u32 nr_match_ors = layer->nr_match_ors;
-	u64 or_idx, and_idx;
-
-	if (nr_match_ors > MAX_LAYER_MATCH_ORS) {
-		scx_bpf_error("too many ORs");
-		return false;
-	}
-
-	bpf_for(or_idx, 0, nr_match_ors) {
-		struct layer_match_ands *ands;
-		bool matched = true;
-
-		barrier_var(or_idx);
-		if (or_idx >= MAX_LAYER_MATCH_ORS)
-			return false; /* can't happen */
-		ands = &layer->matches[or_idx];
-
-		if (ands->nr_match_ands > NR_LAYER_MATCH_KINDS) {
-			scx_bpf_error("too many ANDs");
-			return false;
-		}
-
-		bpf_for(and_idx, 0, ands->nr_match_ands) {
-			struct layer_match *match;
-
-			barrier_var(and_idx);
-			if (and_idx >= NR_LAYER_MATCH_KINDS)
-				return false; /* can't happen */
-			match = &ands->matches[and_idx];
-
-			if (!match_one(match, p, cgrp_path)) {
-				matched = false;
-				break;
-			}
-		}
-
-		if (matched)
-			return true;
-	}
-
-	return false;
-}
-
-static void maybe_refresh_layer(struct task_struct *p, struct task_ctx *tctx)
-{
-	const char *cgrp_path;
-	bool matched = false;
-	u64 idx;	// XXX - int makes verifier unhappy
-
-	if (!tctx->refresh_layer)
-		return;
-	tctx->refresh_layer = false;
-
-	if (!(cgrp_path = format_cgrp_path(p->cgroups->dfl_cgrp)))
-		return;
-
-	if (tctx->layer >= 0 && tctx->layer < nr_layers)
-		__sync_fetch_and_add(&layers[tctx->layer].nr_tasks, -1);
-
-	bpf_for(idx, 0, nr_layers) {
-		if (match_layer(&layers[idx], p, cgrp_path)) {
-			matched = true;
-			break;
-		}
-	}
-
-	if (matched) {
-		struct layer *layer = &layers[idx];
-
-		tctx->layer = idx;
-		tctx->layer_cpus_seq = layer->cpus_seq - 1;
-		__sync_fetch_and_add(&layer->nr_tasks, 1);
-		/*
-		 * XXX - To be correct, we'd need to calculate the vtime
-		 * delta in the previous layer, scale it by the load
-		 * fraction difference and then offset from the new
-		 * layer's vtime_now. For now, just do the simple thing
-		 * and assume the offset to be zero.
-		 *
-		 * Revisit if high frequency dynamic layer switching
-		 * needs to be supported.
-		 */
-		p->scx.dsq_vtime = layer->vtime_now;
-	} else {
-		scx_bpf_error("[%s]%d didn't match any layer", p->comm, p->pid);
-	}
-
-	if (tctx->layer < nr_layers - 1)
-		trace("LAYER=%d %s[%d] cgrp=\"%s\"",
-		      tctx->layer, p->comm, p->pid, cgrp_path);
-}
-
-void BPF_STRUCT_OPS(layered_runnable, struct task_struct *p, u64 enq_flags)
-{
-	u64 now = bpf_ktime_get_ns();
-	struct task_ctx *tctx;
-
-	if (!(tctx = lookup_task_ctx(p)))
-		return;
-
-	maybe_refresh_layer(p, tctx);
-
-	adj_load(tctx->layer, p->scx.weight, now);
-}
-
-void BPF_STRUCT_OPS(layered_running, struct task_struct *p)
-{
-	struct cpu_ctx *cctx;
-	struct task_ctx *tctx;
-	struct layer *layer;
-
-	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)) ||
-	    !(layer = lookup_layer(tctx->layer)))
-		return;
-
-	if (vtime_before(layer->vtime_now, p->scx.dsq_vtime))
-		layer->vtime_now = p->scx.dsq_vtime;
-
-	cctx->current_preempt = layer->preempt;
-	tctx->started_running_at = bpf_ktime_get_ns();
-}
-
-void BPF_STRUCT_OPS(layered_stopping, struct task_struct *p, bool runnable)
-{
-	struct cpu_ctx *cctx;
-	struct task_ctx *tctx;
-	u64 used;
-	u32 layer;
-
-	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)))
-		return;
-
-	layer = tctx->layer;
-	if (layer >= nr_layers) {
-		scx_bpf_error("invalid layer %u", layer);
-		return;
-	}
-
-	used = bpf_ktime_get_ns() - tctx->started_running_at;
-	cctx->layer_cycles[layer] += used;
-	cctx->current_preempt = false;
-
-	/* scale the execution time by the inverse of the weight and charge */
-	p->scx.dsq_vtime += used * 100 / p->scx.weight;
-}
-
-void BPF_STRUCT_OPS(layered_quiescent, struct task_struct *p, u64 deq_flags)
-{
-	struct task_ctx *tctx;
-
-	if ((tctx = lookup_task_ctx(p)))
-		adj_load(tctx->layer, -(s64)p->scx.weight, bpf_ktime_get_ns());
-}
-
-void BPF_STRUCT_OPS(layered_set_weight, struct task_struct *p, u32 weight)
-{
-	struct task_ctx *tctx;
-
-	if ((tctx = lookup_task_ctx(p)))
-		tctx->refresh_layer = true;
-}
-
-void BPF_STRUCT_OPS(layered_set_cpumask, struct task_struct *p,
-		    const struct cpumask *cpumask)
-{
-	struct task_ctx *tctx;
-
-	if (!(tctx = lookup_task_ctx(p)))
-		return;
-
-	if (!all_cpumask) {
-		scx_bpf_error("NULL all_cpumask");
-		return;
-	}
-
-	tctx->all_cpus_allowed =
-		bpf_cpumask_subset((const struct cpumask *)all_cpumask, cpumask);
-}
-
-s32 BPF_STRUCT_OPS(layered_init_task, struct task_struct *p,
-		   struct scx_init_task_args *args)
-{
-	struct task_ctx tctx_init = {
-		.pid = p->pid,
-		.layer = -1,
-		.refresh_layer = true,
-	};
-	struct task_ctx *tctx;
-	struct bpf_cpumask *cpumask;
-	s32 pid = p->pid;
-	s32 ret;
-
-	if (all_cpumask)
-		tctx_init.all_cpus_allowed =
-			bpf_cpumask_subset((const struct cpumask *)all_cpumask, p->cpus_ptr);
-	else
-		scx_bpf_error("missing all_cpumask");
-
-	/*
-	 * XXX - We want BPF_NOEXIST but bpf_map_delete_elem() in .disable() may
-	 * fail spuriously due to BPF recursion protection triggering
-	 * unnecessarily.
-	 */
-	if ((ret = bpf_map_update_elem(&task_ctxs, &pid, &tctx_init, 0 /*BPF_NOEXIST*/))) {
-		scx_bpf_error("task_ctx allocation failure, ret=%d", ret);
-		return ret;
-	}
-
-	/*
-	 * Read the entry from the map immediately so we can add the cpumask
-	 * with bpf_kptr_xchg().
-	 */
-	if (!(tctx = lookup_task_ctx(p)))
-		return -ENOENT;
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask) {
-		bpf_map_delete_elem(&task_ctxs, &pid);
-		return -ENOMEM;
-	}
-
-	cpumask = bpf_kptr_xchg(&tctx->layered_cpumask, cpumask);
-	if (cpumask) {
-		/* Should never happen as we just inserted it above. */
-		bpf_cpumask_release(cpumask);
-		bpf_map_delete_elem(&task_ctxs, &pid);
-		return -EINVAL;
-	}
-
-	/*
-	 * We are matching cgroup hierarchy path directly rather than the CPU
-	 * controller path. As the former isn't available during the scheduler
-	 * fork path, let's delay the layer selection until the first
-	 * runnable().
-	 */
-
-	return 0;
-}
-
-void BPF_STRUCT_OPS(layered_exit_task, struct task_struct *p,
-		    struct scx_exit_task_args *args)
-{
-	struct cpu_ctx *cctx;
-	struct task_ctx *tctx;
-	s32 pid = p->pid;
-	int ret;
-
-	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)))
-		return;
-
-	if (tctx->layer >= 0 && tctx->layer < nr_layers)
-		__sync_fetch_and_add(&layers[tctx->layer].nr_tasks, -1);
-
-	/*
-	 * XXX - There's no reason delete should fail here but BPF's recursion
-	 * protection can unnecessarily fail the operation. The fact that
-	 * deletions aren't reliable means that we sometimes leak task_ctx and
-	 * can't use BPF_NOEXIST on allocation in .prep_enable().
-	 */
-	ret = bpf_map_delete_elem(&task_ctxs, &pid);
-	if (ret)
-		gstat_inc(GSTAT_TASK_CTX_FREE_FAILED, cctx);
-}
-
-s32 BPF_STRUCT_OPS_SLEEPABLE(layered_init)
-{
-	struct bpf_cpumask *cpumask;
-	int i, j, k, nr_online_cpus, ret;
-
-	scx_bpf_switch_all();
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask)
-		return -ENOMEM;
-
-	nr_online_cpus = 0;
-	bpf_for(i, 0, nr_possible_cpus) {
-		const volatile u8 *u8_ptr;
-
-		if ((u8_ptr = MEMBER_VPTR(all_cpus, [i / 8]))) {
-			if (*u8_ptr & (1 << (i % 8))) {
-				bpf_cpumask_set_cpu(i, cpumask);
-				nr_online_cpus++;
-			}
-		} else {
-			return -EINVAL;
-		}
-	}
-
-	cpumask = bpf_kptr_xchg(&all_cpumask, cpumask);
-	if (cpumask)
-		bpf_cpumask_release(cpumask);
-
-	dbg("CFG: Dumping configuration, nr_online_cpus=%d smt_enabled=%d",
-	    nr_online_cpus, smt_enabled);
-
-	bpf_for(i, 0, nr_layers) {
-		struct layer *layer = &layers[i];
-
-		dbg("CFG LAYER[%d] open=%d preempt=%d",
-		    i, layer->open, layer->preempt);
-
-		if (layer->nr_match_ors > MAX_LAYER_MATCH_ORS) {
-			scx_bpf_error("too many ORs");
-			return -EINVAL;
-		}
-
-		bpf_for(j, 0, layer->nr_match_ors) {
-			struct layer_match_ands *ands = MEMBER_VPTR(layers, [i].matches[j]);
-			if (!ands) {
-				scx_bpf_error("shouldn't happen");
-				return -EINVAL;
-			}
-
-			if (ands->nr_match_ands > NR_LAYER_MATCH_KINDS) {
-				scx_bpf_error("too many ANDs");
-				return -EINVAL;
-			}
-
-			dbg("CFG   OR[%02d]", j);
-
-			bpf_for(k, 0, ands->nr_match_ands) {
-				char header[32];
-				u64 header_data[1] = { k };
-				struct layer_match *match;
-
-				bpf_snprintf(header, sizeof(header), "CFG     AND[%02d]:",
-					     header_data, sizeof(header_data));
-
-				match = MEMBER_VPTR(layers, [i].matches[j].matches[k]);
-				if (!match) {
-					scx_bpf_error("shouldn't happen");
-					return -EINVAL;
-				}
-
-				switch (match->kind) {
-				case MATCH_CGROUP_PREFIX:
-					dbg("%s CGROUP_PREFIX \"%s\"", header, match->cgroup_prefix);
-					break;
-				case MATCH_COMM_PREFIX:
-					dbg("%s COMM_PREFIX \"%s\"", header, match->comm_prefix);
-					break;
-				case MATCH_NICE_ABOVE:
-					dbg("%s NICE_ABOVE %d", header, match->nice_above_or_below);
-					break;
-				case MATCH_NICE_BELOW:
-					dbg("%s NICE_BELOW %d", header, match->nice_above_or_below);
-					break;
-				default:
-					scx_bpf_error("%s Invalid kind", header);
-					return -EINVAL;
-				}
-			}
-			if (ands->nr_match_ands == 0)
-				dbg("CFG     DEFAULT");
-		}
-	}
-
-	bpf_for(i, 0, nr_layers) {
-		struct layer_cpumask_wrapper *cpumaskw;
-
-		layers[i].idx = i;
-
-		ret = scx_bpf_create_dsq(i, -1);
-		if (ret < 0)
-			return ret;
-
-		if (!(cpumaskw = bpf_map_lookup_elem(&layer_cpumasks, &i)))
-			return -ENONET;
-
-		cpumask = bpf_cpumask_create();
-		if (!cpumask)
-			return -ENOMEM;
-
-		/*
-		 * Start all layers with full cpumask so that everything runs
-		 * everywhere. This will soon be updated by refresh_cpumasks()
-		 * once the scheduler starts running.
-		 */
-		bpf_cpumask_setall(cpumask);
-
-		cpumask = bpf_kptr_xchg(&cpumaskw->cpumask, cpumask);
-		if (cpumask)
-			bpf_cpumask_release(cpumask);
-	}
-
-	return 0;
-}
-
-void BPF_STRUCT_OPS(layered_exit, struct scx_exit_info *ei)
-{
-	uei_record(&uei, ei);
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops layered = {
-	.select_cpu		= (void *)layered_select_cpu,
-	.enqueue		= (void *)layered_enqueue,
-	.dispatch		= (void *)layered_dispatch,
-	.runnable		= (void *)layered_runnable,
-	.running		= (void *)layered_running,
-	.stopping		= (void *)layered_stopping,
-	.quiescent		= (void *)layered_quiescent,
-	.set_weight		= (void *)layered_set_weight,
-	.set_cpumask		= (void *)layered_set_cpumask,
-	.init_task		= (void *)layered_init_task,
-	.exit_task		= (void *)layered_exit_task,
-	.init			= (void *)layered_init,
-	.exit			= (void *)layered_exit,
-	.name			= "layered",
-};
diff --git a/tools/sched_ext/scx_layered/src/bpf/util.bpf.c b/tools/sched_ext/scx_layered/src/bpf/util.bpf.c
deleted file mode 100644
index 703e0eece..000000000
--- a/tools/sched_ext/scx_layered/src/bpf/util.bpf.c
+++ /dev/null
@@ -1,68 +0,0 @@
-/* to be included in the main bpf.c file */
-
-struct {
-	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
-	__uint(key_size, sizeof(u32));
-	/* double size because verifier can't follow length calculation */
-	__uint(value_size, 2 * MAX_PATH);
-	__uint(max_entries, 1);
-} cgrp_path_bufs SEC(".maps");
-
-static char *format_cgrp_path(struct cgroup *cgrp)
-{
-	u32 zero = 0;
-	char *path = bpf_map_lookup_elem(&cgrp_path_bufs, &zero);
-	u32 len = 0, level, max_level;
-
-	if (!path) {
-		scx_bpf_error("cgrp_path_buf lookup failed");
-		return NULL;
-	}
-
-	max_level = cgrp->level;
-	if (max_level > 127)
-		max_level = 127;
-
-	bpf_for(level, 1, max_level + 1) {
-		int ret;
-
-		if (level > 1 && len < MAX_PATH - 1)
-			path[len++] = '/';
-
-		if (len >= MAX_PATH - 1) {
-			scx_bpf_error("cgrp_path_buf overflow");
-			return NULL;
-		}
-
-		ret = bpf_probe_read_kernel_str(path + len, MAX_PATH - len - 1,
-						BPF_CORE_READ(cgrp, ancestors[level], kn, name));
-		if (ret < 0) {
-			scx_bpf_error("bpf_probe_read_kernel_str failed");
-			return NULL;
-		}
-
-		len += ret - 1;
-	}
-
-	if (len >= MAX_PATH - 2) {
-		scx_bpf_error("cgrp_path_buf overflow");
-		return NULL;
-	}
-	path[len] = '/';
-	path[len + 1] = '\0';
-
-	return path;
-}
-
-static inline bool match_prefix(const char *prefix, const char *str, u32 max_len)
-{
-	int c;
-
-	bpf_for(c, 0, max_len) {
-		if (prefix[c] == '\0')
-			return true;
-		if (str[c] != prefix[c])
-			return false;
-	}
-	return false;
-}
diff --git a/tools/sched_ext/scx_layered/src/bpf_intf.rs b/tools/sched_ext/scx_layered/src/bpf_intf.rs
deleted file mode 100644
index 0ed31f8e0..000000000
--- a/tools/sched_ext/scx_layered/src/bpf_intf.rs
+++ /dev/null
@@ -1,10 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-#![allow(non_upper_case_globals)]
-#![allow(non_camel_case_types)]
-#![allow(non_snake_case)]
-#![allow(dead_code)]
-
-include!(concat!(env!("OUT_DIR"), "/bpf_intf.rs"));
diff --git a/tools/sched_ext/scx_layered/src/bpf_skel.rs b/tools/sched_ext/scx_layered/src/bpf_skel.rs
deleted file mode 100644
index 063ccf896..000000000
--- a/tools/sched_ext/scx_layered/src/bpf_skel.rs
+++ /dev/null
@@ -1,12 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-
-// We can't directly include the generated skeleton in main.rs as it may
-// contain compiler attributes that can't be `include!()`ed via macro and we
-// can't use the `#[path = "..."]` because `concat!(env!("OUT_DIR"),
-// "/bpf.skel.rs")` does not work inside the path attribute yet (see
-// https://github.com/rust-lang/rust/pull/83366).
-
-include!(concat!(env!("OUT_DIR"), "/bpf_skel.rs"));
diff --git a/tools/sched_ext/scx_layered/src/main.rs b/tools/sched_ext/scx_layered/src/main.rs
deleted file mode 100644
index 5b5374226..000000000
--- a/tools/sched_ext/scx_layered/src/main.rs
+++ /dev/null
@@ -1,1639 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-mod bpf_skel;
-pub use bpf_skel::*;
-pub mod bpf_intf;
-
-use std::collections::BTreeMap;
-use std::collections::BTreeSet;
-use std::ffi::CStr;
-use std::ffi::CString;
-use std::fs;
-use std::io::Read;
-use std::io::Write;
-use std::ops::Sub;
-use std::sync::atomic::AtomicBool;
-use std::sync::atomic::Ordering;
-use std::sync::Arc;
-use std::time::Duration;
-use std::time::Instant;
-
-use ::fb_procfs as procfs;
-use anyhow::anyhow;
-use anyhow::bail;
-use anyhow::Context;
-use anyhow::Result;
-use bitvec::prelude::*;
-use clap::Parser;
-use libbpf_rs::skel::OpenSkel as _;
-use libbpf_rs::skel::Skel as _;
-use libbpf_rs::skel::SkelBuilder as _;
-use log::debug;
-use log::info;
-use log::trace;
-use scx_utils::ravg::ravg_read;
-use serde::Deserialize;
-use serde::Serialize;
-
-const RAVG_FRAC_BITS: u32 = bpf_intf::ravg_consts_RAVG_FRAC_BITS;
-const MAX_CPUS: usize = bpf_intf::consts_MAX_CPUS as usize;
-const MAX_PATH: usize = bpf_intf::consts_MAX_PATH as usize;
-const MAX_COMM: usize = bpf_intf::consts_MAX_COMM as usize;
-const MAX_LAYER_MATCH_ORS: usize = bpf_intf::consts_MAX_LAYER_MATCH_ORS as usize;
-const MAX_LAYERS: usize = bpf_intf::consts_MAX_LAYERS as usize;
-const USAGE_HALF_LIFE: u32 = bpf_intf::consts_USAGE_HALF_LIFE;
-const USAGE_HALF_LIFE_F64: f64 = USAGE_HALF_LIFE as f64 / 1_000_000_000.0;
-const NR_GSTATS: usize = bpf_intf::global_stat_idx_NR_GSTATS as usize;
-const NR_LSTATS: usize = bpf_intf::layer_stat_idx_NR_LSTATS as usize;
-const NR_LAYER_MATCH_KINDS: usize = bpf_intf::layer_match_kind_NR_LAYER_MATCH_KINDS as usize;
-const CORE_CACHE_LEVEL: u32 = 2;
-
-lazy_static::lazy_static! {
-    static ref NR_POSSIBLE_CPUS: usize = libbpf_rs::num_possible_cpus().unwrap();
-    static ref USAGE_DECAY: f64 = 0.5f64.powf(1.0 / USAGE_HALF_LIFE_F64);
-}
-
-/// scx_layered: A highly configurable multi-layer sched_ext scheduler
-///
-/// scx_layered allows classifying tasks into multiple layers and applying
-/// different scheduling policies to them. The configuration is specified in
-/// json and composed of two parts - matches and policies.
-///
-/// Matches
-/// =======
-///
-/// Whenever a task is forked or its attributes are changed, the task goes
-/// through a series of matches to determine the layer it belongs to. A
-/// match set is composed of OR groups of AND blocks. An example:
-///
-///   "matches": [
-///     [
-///       {
-///         "CgroupPrefix": "system.slice/"
-///       }
-///     ],
-///     [
-///       {
-///         "CommPrefix": "fbagent"
-///       },
-///       {
-///         "NiceAbove": 0
-///       }
-///     ]
-///   ],
-///
-/// The outer array contains the OR groups and the inner AND blocks, so the
-/// above matches:
-///
-/// * Tasks which are in the cgroup sub-hierarchy under "system.slice".
-/// * Or tasks whose comm starts with "fbagent" and have a nice value > 0.
-///
-/// Currently, the following matches are supported:
-///
-/// * CgroupPrefix: Matches the prefix of the cgroup that the task belongs
-///   to. As this is a string match, whether the pattern has the trailing
-///   '/' makes a difference. For example, "TOP/CHILD/" only matches tasks
-///   which are under that particular cgroup while "TOP/CHILD" also matches
-///   tasks under "TOP/CHILD0/" or "TOP/CHILD1/".
-///
-/// * CommPrefix: Matches the task's comm prefix.
-///
-/// * NiceAbove: Matches if the task's nice value is greater than the
-///   pattern.
-///
-/// * NiceBelow: Matches if the task's nice value is smaller than the
-///   pattern.
-///
-/// While there are complexity limitations as the matches are performed in
-/// BPF, it is straightforward to add more types of matches.
-///
-/// Policies
-/// ========
-///
-/// The following is an example policy configuration for a layer.
-///
-///   "kind": {
-///     "Confined": {
-///       "cpus_range": [1, 8],
-///       "util_range": [0.8, 0.9]
-///     }
-///   }
-///
-/// It's of "Confined" kind, which tries to concentrate the layer's tasks
-/// into a limited number of CPUs. In the above case, the number of CPUs
-/// assigned to the layer is scaled between 1 and 8 so that the per-cpu
-/// utilization is kept between 80% and 90%. If the CPUs are loaded higher
-/// than 90%, more CPUs are allocated to the layer. If the utilization drops
-/// below 80%, the layer loses CPUs.
-///
-/// Currently, the following policy kinds are supported:
-///
-/// * Confined: Tasks are restricted to the allocated CPUs. The number of
-///   CPUs allocated is modulated to keep the per-CPU utilization in
-///   "util_range". The range can optionally be restricted with the
-///   "cpus_range" property.
-///
-/// * Grouped: Similar to Confined but tasks may spill outside if there are
-///   idle CPUs outside the allocated ones. If "preempt" is true, tasks in
-///   this layer will preempt tasks which belong to other non-preempting
-///   layers when no idle CPUs are available.
-///
-/// * Open: Prefer the CPUs which are not occupied by Confined or Grouped
-///   layers. Tasks in this group will spill into occupied CPUs if there are
-///   no unoccupied idle CPUs. If "preempt" is true, tasks in this layer
-///   will preempt tasks which belong to other non-preempting layers when no
-///   idle CPUs are available.
-///
-/// Similar to matches, adding new policies and extending existing ones
-/// should be relatively straightforward.
-///
-/// Configuration example and running scx_layered
-/// =============================================
-///
-/// A scx_layered config is composed of layer configs and a layer config is
-/// composed of a name, a set of matches and a policy block. Running the
-/// following will write an example configuration into example.json.
-///
-///   $ scx_layered -e example.json
-///
-/// Note that the last layer in the configuration must have an empty match
-/// set as it must match all tasks which haven't been matched into previous
-/// layers.
-///
-/// The configuration can be specified in multiple json files and command
-/// line arguments. Each must contain valid layer configurations and they're
-/// concatenated in the specified order. In most cases, something like the
-/// following should do.
-///
-///   $ scx_layered file:example.json
-///
-/// Statistics
-/// ==========
-///
-/// scx_layered will print out a set of statistics every monitoring
-/// interval.
-///
-///   tot= 117909 local=86.20 open_idle= 0.21 affn_viol= 1.37 tctx_err=9 proc=6ms
-///   busy= 34.2 util= 1733.6 load=  21744.1 fallback_cpu=  1
-///     batch    : util/frac=   11.8/  0.7 load/frac=     29.7:  0.1 tasks=  2597
-///                tot=   3478 local=67.80 open_idle= 0.00 preempt= 0.00 affn_viol= 0.00
-///                cpus=  2 [  2,  2] 04000001 00000000
-///     immediate: util/frac= 1218.8/ 70.3 load/frac=  21399.9: 98.4 tasks=  1107
-///                tot=  68997 local=90.57 open_idle= 0.26 preempt= 9.36 affn_viol= 0.00
-///                cpus= 50 [ 50, 50] fbfffffe 000fffff
-///     normal   : util/frac=  502.9/ 29.0 load/frac=    314.5:  1.4 tasks=  3512
-///                tot=  45434 local=80.97 open_idle= 0.16 preempt= 0.00 affn_viol= 3.56
-///                cpus= 50 [ 50, 50] fbfffffe 000fffff
-///
-/// Global statistics:
-///
-/// - tot: Total scheduling events in the period.
-///
-/// - local: % that got scheduled directly into an idle CPU.
-///
-/// - open_idle: % of open layer tasks scheduled into occupied idle CPUs.
-///
-/// - affn_viol: % which violated configured policies due to CPU affinity
-///   restrictions.
-///
-/// - proc: CPU time this binary consumed during the period.
-///
-/// - busy: CPU busy % (100% means all CPUs were fully occupied)
-///
-/// - util: CPU utilization % (100% means one CPU was fully occupied)
-///
-/// - load: Sum of weight * duty_cycle for all tasks
-///
-/// Per-layer statistics:
-///
-/// - util/frac: CPU utilization and fraction % (sum of fractions across
-///   layers is always 100%).
-///
-/// - load/frac: Load sum and fraction %.
-///
-/// - tasks: Number of tasks.
-///
-/// - tot: Total scheduling events.
-///
-/// - open_idle: % of tasks scheduled into idle CPUs occupied by other layers.
-///
-/// - preempt: % of tasks that preempted other tasks.
-///
-/// - affn_viol: % which violated configured policies due to CPU affinity
-///   restrictions.
-///
-/// - cpus: CUR_NR_CPUS [MIN_NR_CPUS, MAX_NR_CPUS] CUR_CPU_MASK
-///
-#[derive(Debug, Parser)]
-#[command(verbatim_doc_comment)]
-struct Opts {
-    /// Scheduling slice duration in microseconds.
-    #[clap(short = 's', long, default_value = "20000")]
-    slice_us: u64,
-
-    /// Scheduling interval in seconds.
-    #[clap(short = 'i', long, default_value = "0.1")]
-    interval: f64,
-
-    /// Monitoring interval in seconds.
-    #[clap(short = 'm', long, default_value = "2.0")]
-    monitor: f64,
-
-    /// Disable load-fraction based max layer CPU limit. ***NOTE***
-    /// load-fraction calculation is currently broken due to lack of
-    /// infeasible weight adjustments. Setting this option is recommended.
-    #[clap(short = 'n', long)]
-    no_load_frac_limit: bool,
-
-    /// Enable verbose output including libbpf details. Specify multiple
-    /// times to increase verbosity.
-    #[clap(short = 'v', long, action = clap::ArgAction::Count)]
-    verbose: u8,
-
-    /// Write example layer specifications into the file and exit.
-    #[clap(short = 'e', long)]
-    example: Option<String>,
-
-    /// Layer specification. See --help.
-    specs: Vec<String>,
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-enum LayerMatch {
-    CgroupPrefix(String),
-    CommPrefix(String),
-    NiceAbove(i32),
-    NiceBelow(i32),
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-enum LayerKind {
-    Confined {
-        cpus_range: Option<(usize, usize)>,
-        util_range: (f64, f64),
-    },
-    Grouped {
-        cpus_range: Option<(usize, usize)>,
-        util_range: (f64, f64),
-        preempt: bool,
-    },
-    Open {
-        preempt: bool,
-    },
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-struct LayerSpec {
-    name: String,
-    comment: Option<String>,
-    matches: Vec<Vec<LayerMatch>>,
-    kind: LayerKind,
-}
-
-impl LayerSpec {
-    fn parse(input: &str) -> Result<Vec<Self>> {
-        let config: LayerConfig = if input.starts_with("f:") || input.starts_with("file:") {
-            let mut f = fs::OpenOptions::new()
-                .read(true)
-                .open(input.split_once(':').unwrap().1)?;
-            let mut content = String::new();
-            f.read_to_string(&mut content)?;
-            serde_json::from_str(&content)?
-        } else {
-            serde_json::from_str(input)?
-        };
-        Ok(config.specs)
-    }
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-#[serde(transparent)]
-struct LayerConfig {
-    specs: Vec<LayerSpec>,
-}
-
-fn now_monotonic() -> u64 {
-    let mut time = libc::timespec {
-        tv_sec: 0,
-        tv_nsec: 0,
-    };
-    let ret = unsafe { libc::clock_gettime(libc::CLOCK_MONOTONIC, &mut time) };
-    assert!(ret == 0);
-    time.tv_sec as u64 * 1_000_000_000 + time.tv_nsec as u64
-}
-
-fn read_total_cpu(reader: &procfs::ProcReader) -> Result<procfs::CpuStat> {
-    reader
-        .read_stat()
-        .context("Failed to read procfs")?
-        .total_cpu
-        .ok_or_else(|| anyhow!("Could not read total cpu stat in proc"))
-}
-
-fn calc_util(curr: &procfs::CpuStat, prev: &procfs::CpuStat) -> Result<f64> {
-    match (curr, prev) {
-        (
-            procfs::CpuStat {
-                user_usec: Some(curr_user),
-                nice_usec: Some(curr_nice),
-                system_usec: Some(curr_system),
-                idle_usec: Some(curr_idle),
-                iowait_usec: Some(curr_iowait),
-                irq_usec: Some(curr_irq),
-                softirq_usec: Some(curr_softirq),
-                stolen_usec: Some(curr_stolen),
-                ..
-            },
-            procfs::CpuStat {
-                user_usec: Some(prev_user),
-                nice_usec: Some(prev_nice),
-                system_usec: Some(prev_system),
-                idle_usec: Some(prev_idle),
-                iowait_usec: Some(prev_iowait),
-                irq_usec: Some(prev_irq),
-                softirq_usec: Some(prev_softirq),
-                stolen_usec: Some(prev_stolen),
-                ..
-            },
-        ) => {
-            let idle_usec = curr_idle - prev_idle;
-            let iowait_usec = curr_iowait - prev_iowait;
-            let user_usec = curr_user - prev_user;
-            let system_usec = curr_system - prev_system;
-            let nice_usec = curr_nice - prev_nice;
-            let irq_usec = curr_irq - prev_irq;
-            let softirq_usec = curr_softirq - prev_softirq;
-            let stolen_usec = curr_stolen - prev_stolen;
-
-            let busy_usec =
-                user_usec + system_usec + nice_usec + irq_usec + softirq_usec + stolen_usec;
-            let total_usec = idle_usec + busy_usec + iowait_usec;
-            if total_usec > 0 {
-                Ok(((busy_usec as f64) / (total_usec as f64)).clamp(0.0, 1.0))
-            } else {
-                Ok(1.0)
-            }
-        }
-        _ => {
-            bail!("Missing stats in cpustat");
-        }
-    }
-}
-
-fn copy_into_cstr(dst: &mut [i8], src: &str) {
-    let cstr = CString::new(src).unwrap();
-    let bytes = unsafe { std::mem::transmute::<&[u8], &[i8]>(cstr.as_bytes_with_nul()) };
-    dst[0..bytes.len()].copy_from_slice(bytes);
-}
-
-fn format_bitvec(bitvec: &BitVec) -> String {
-    let mut vals = Vec::<u32>::new();
-    let mut val: u32 = 0;
-    for (idx, bit) in bitvec.iter().enumerate() {
-        if idx > 0 && idx % 32 == 0 {
-            vals.push(val);
-            val = 0;
-        }
-        if *bit {
-            val |= 1 << (idx % 32);
-        }
-    }
-    vals.push(val);
-    let mut output = vals
-        .iter()
-        .fold(String::new(), |string, v| format!("{}{:08x} ", string, v));
-    output.pop();
-    output
-}
-
-fn read_cpu_ctxs(skel: &BpfSkel) -> Result<Vec<bpf_intf::cpu_ctx>> {
-    let mut cpu_ctxs = vec![];
-    let cpu_ctxs_vec = skel
-        .maps()
-        .cpu_ctxs()
-        .lookup_percpu(&0u32.to_ne_bytes(), libbpf_rs::MapFlags::ANY)
-        .context("Failed to lookup cpu_ctx")?
-        .unwrap();
-    for cpu in 0..*NR_POSSIBLE_CPUS {
-        cpu_ctxs.push(*unsafe {
-            &*(cpu_ctxs_vec[cpu].as_slice().as_ptr() as *const bpf_intf::cpu_ctx)
-        });
-    }
-    Ok(cpu_ctxs)
-}
-
-#[derive(Clone, Debug)]
-struct BpfStats {
-    gstats: Vec<u64>,
-    lstats: Vec<Vec<u64>>,
-    lstats_sums: Vec<u64>,
-}
-
-impl BpfStats {
-    fn read(cpu_ctxs: &[bpf_intf::cpu_ctx], nr_layers: usize) -> Self {
-        let mut gstats = vec![0u64; NR_GSTATS];
-        let mut lstats = vec![vec![0u64; NR_LSTATS]; nr_layers];
-
-        for cpu in 0..*NR_POSSIBLE_CPUS {
-            for stat in 0..NR_GSTATS {
-                gstats[stat] += cpu_ctxs[cpu].gstats[stat];
-            }
-            for layer in 0..nr_layers {
-                for stat in 0..NR_LSTATS {
-                    lstats[layer][stat] += cpu_ctxs[cpu].lstats[layer][stat];
-                }
-            }
-        }
-
-        let mut lstats_sums = vec![0u64; NR_LSTATS];
-        for layer in 0..nr_layers {
-            for stat in 0..NR_LSTATS {
-                lstats_sums[stat] += lstats[layer][stat];
-            }
-        }
-
-        Self {
-            gstats,
-            lstats,
-            lstats_sums,
-        }
-    }
-}
-
-impl<'a, 'b> Sub<&'b BpfStats> for &'a BpfStats {
-    type Output = BpfStats;
-
-    fn sub(self, rhs: &'b BpfStats) -> BpfStats {
-        let vec_sub = |l: &[u64], r: &[u64]| l.iter().zip(r.iter()).map(|(l, r)| *l - *r).collect();
-        BpfStats {
-            gstats: vec_sub(&self.gstats, &rhs.gstats),
-            lstats: self
-                .lstats
-                .iter()
-                .zip(rhs.lstats.iter())
-                .map(|(l, r)| vec_sub(l, r))
-                .collect(),
-            lstats_sums: vec_sub(&self.lstats_sums, &rhs.lstats_sums),
-        }
-    }
-}
-
-struct Stats {
-    nr_layers: usize,
-    at: Instant,
-
-    nr_layer_tasks: Vec<usize>,
-
-    total_load: f64,
-    layer_loads: Vec<f64>,
-
-    total_util: f64, // Running AVG of sum of layer_utils
-    layer_utils: Vec<f64>,
-    prev_layer_cycles: Vec<u64>,
-
-    cpu_busy: f64, // Read from /proc, maybe higher than total_util
-    prev_total_cpu: procfs::CpuStat,
-
-    bpf_stats: BpfStats,
-    prev_bpf_stats: BpfStats,
-}
-
-impl Stats {
-    fn read_layer_loads(skel: &mut BpfSkel, nr_layers: usize) -> (f64, Vec<f64>) {
-        let now_mono = now_monotonic();
-        let layer_loads: Vec<f64> = skel
-            .bss()
-            .layers
-            .iter()
-            .take(nr_layers)
-            .map(|layer| {
-                let rd = &layer.load_rd;
-                ravg_read(
-                    rd.val,
-                    rd.val_at,
-                    rd.old,
-                    rd.cur,
-                    now_mono,
-                    USAGE_HALF_LIFE,
-                    RAVG_FRAC_BITS,
-                )
-            })
-            .collect();
-        (layer_loads.iter().sum(), layer_loads)
-    }
-
-    fn read_layer_cycles(cpu_ctxs: &[bpf_intf::cpu_ctx], nr_layers: usize) -> Vec<u64> {
-        let mut layer_cycles = vec![0u64; nr_layers];
-
-        for cpu in 0..*NR_POSSIBLE_CPUS {
-            for layer in 0..nr_layers {
-                layer_cycles[layer] += cpu_ctxs[cpu].layer_cycles[layer];
-            }
-        }
-
-        layer_cycles
-    }
-
-    fn new(skel: &mut BpfSkel, proc_reader: &procfs::ProcReader) -> Result<Self> {
-        let nr_layers = skel.rodata().nr_layers as usize;
-        let bpf_stats = BpfStats::read(&read_cpu_ctxs(skel)?, nr_layers);
-
-        Ok(Self {
-            at: Instant::now(),
-            nr_layers,
-
-            nr_layer_tasks: vec![0; nr_layers],
-
-            total_load: 0.0,
-            layer_loads: vec![0.0; nr_layers],
-
-            total_util: 0.0,
-            layer_utils: vec![0.0; nr_layers],
-            prev_layer_cycles: vec![0; nr_layers],
-
-            cpu_busy: 0.0,
-            prev_total_cpu: read_total_cpu(&proc_reader)?,
-
-            bpf_stats: bpf_stats.clone(),
-            prev_bpf_stats: bpf_stats,
-        })
-    }
-
-    fn refresh(
-        &mut self,
-        skel: &mut BpfSkel,
-        proc_reader: &procfs::ProcReader,
-        now: Instant,
-    ) -> Result<()> {
-        let elapsed = now.duration_since(self.at).as_secs_f64() as f64;
-        let cpu_ctxs = read_cpu_ctxs(skel)?;
-
-        let nr_layer_tasks: Vec<usize> = skel
-            .bss()
-            .layers
-            .iter()
-            .take(self.nr_layers)
-            .map(|layer| layer.nr_tasks as usize)
-            .collect();
-
-        let (total_load, layer_loads) = Self::read_layer_loads(skel, self.nr_layers);
-
-        let cur_layer_cycles = Self::read_layer_cycles(&cpu_ctxs, self.nr_layers);
-        let cur_layer_utils: Vec<f64> = cur_layer_cycles
-            .iter()
-            .zip(self.prev_layer_cycles.iter())
-            .map(|(cur, prev)| (cur - prev) as f64 / 1_000_000_000.0 / elapsed)
-            .collect();
-        let layer_utils: Vec<f64> = cur_layer_utils
-            .iter()
-            .zip(self.layer_utils.iter())
-            .map(|(cur, prev)| {
-                let decay = USAGE_DECAY.powf(elapsed);
-                prev * decay + cur * (1.0 - decay)
-            })
-            .collect();
-
-        let cur_total_cpu = read_total_cpu(proc_reader)?;
-        let cpu_busy = calc_util(&cur_total_cpu, &self.prev_total_cpu)?;
-
-        let cur_bpf_stats = BpfStats::read(&cpu_ctxs, self.nr_layers);
-        let bpf_stats = &cur_bpf_stats - &self.prev_bpf_stats;
-
-        *self = Self {
-            at: now,
-            nr_layers: self.nr_layers,
-
-            nr_layer_tasks,
-
-            total_load,
-            layer_loads,
-
-            total_util: layer_utils.iter().sum(),
-            layer_utils: layer_utils.try_into().unwrap(),
-            prev_layer_cycles: cur_layer_cycles,
-
-            cpu_busy,
-            prev_total_cpu: cur_total_cpu,
-
-            bpf_stats,
-            prev_bpf_stats: cur_bpf_stats,
-        };
-        Ok(())
-    }
-}
-
-#[derive(Debug, Default)]
-struct UserExitInfo {
-    kind: i32,
-    reason: Option<String>,
-    msg: Option<String>,
-}
-
-impl UserExitInfo {
-    fn read(bpf_uei: &bpf_bss_types::user_exit_info) -> Result<Self> {
-        let kind = unsafe { std::ptr::read_volatile(&bpf_uei.kind as *const _) };
-
-        let (reason, msg) = if kind != 0 {
-            (
-                Some(
-                    unsafe { CStr::from_ptr(bpf_uei.reason.as_ptr() as *const _) }
-                        .to_str()
-                        .context("Failed to convert reason to string")?
-                        .to_string(),
-                )
-                .filter(|s| !s.is_empty()),
-                Some(
-                    unsafe { CStr::from_ptr(bpf_uei.msg.as_ptr() as *const _) }
-                        .to_str()
-                        .context("Failed to convert msg to string")?
-                        .to_string(),
-                )
-                .filter(|s| !s.is_empty()),
-            )
-        } else {
-            (None, None)
-        };
-
-        Ok(Self { kind, reason, msg })
-    }
-
-    fn exited(bpf_uei: &bpf_bss_types::user_exit_info) -> Result<bool> {
-        Ok(Self::read(bpf_uei)?.kind != 0)
-    }
-
-    fn report(&self) -> Result<()> {
-        let why = match (&self.reason, &self.msg) {
-            (Some(reason), None) => format!("{}", reason),
-            (Some(reason), Some(msg)) => format!("{} ({})", reason, msg),
-            _ => "".into(),
-        };
-
-        match self.kind {
-            0 => Ok(()),
-            etype => {
-                if etype != 64 {
-                    bail!("EXIT: kind={} {}", etype, why);
-                } else {
-                    info!("EXIT: {}", why);
-                    Ok(())
-                }
-            }
-        }
-    }
-}
-
-#[derive(Debug)]
-struct CpuPool {
-    nr_cores: usize,
-    nr_cpus: usize,
-    all_cpus: BitVec,
-    core_cpus: Vec<BitVec>,
-    cpu_core: Vec<usize>,
-    available_cores: BitVec,
-    first_cpu: usize,
-    fallback_cpu: usize, // next free or the first CPU if none is free
-}
-
-impl CpuPool {
-    fn new() -> Result<Self> {
-        if *NR_POSSIBLE_CPUS > MAX_CPUS {
-            bail!(
-                "NR_POSSIBLE_CPUS {} > MAX_CPUS {}",
-                *NR_POSSIBLE_CPUS,
-                MAX_CPUS
-            );
-        }
-
-        let mut cpu_to_cache = vec![]; // (cpu_id, Option<cache_id>)
-        let mut cache_ids = BTreeSet::<usize>::new();
-        let mut nr_offline = 0;
-
-        // Build cpu -> cache ID mapping.
-        for cpu in 0..*NR_POSSIBLE_CPUS {
-            let path = format!(
-                "/sys/devices/system/cpu/cpu{}/cache/index{}/id",
-                cpu, CORE_CACHE_LEVEL
-            );
-            let id = match std::fs::read_to_string(&path) {
-                Ok(val) => Some(val.trim().parse::<usize>().with_context(|| {
-                    format!("Failed to parse {:?}'s content {:?}", &path, &val)
-                })?),
-                Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
-                    nr_offline += 1;
-                    None
-                }
-                Err(e) => return Err(e).with_context(|| format!("Failed to open {:?}", &path)),
-            };
-
-            cpu_to_cache.push(id);
-            if let Some(id) = id {
-                cache_ids.insert(id);
-            }
-        }
-
-        let nr_cpus = *NR_POSSIBLE_CPUS - nr_offline;
-
-        // Cache IDs may have holes. Assign consecutive core IDs to existing
-        // cache IDs.
-        let mut cache_to_core = BTreeMap::<usize, usize>::new();
-        let mut nr_cores = 0;
-        for cache_id in cache_ids.iter() {
-            cache_to_core.insert(*cache_id, nr_cores);
-            nr_cores += 1;
-        }
-
-        // Build core -> cpumask and cpu -> core mappings.
-        let mut all_cpus = bitvec![0; *NR_POSSIBLE_CPUS];
-        let mut core_cpus = vec![bitvec![0; *NR_POSSIBLE_CPUS]; nr_cores];
-        let mut cpu_core = vec![];
-
-        for (cpu, cache) in cpu_to_cache.iter().enumerate().take(*NR_POSSIBLE_CPUS) {
-            if let Some(cache_id) = cache {
-                let core_id = cache_to_core[cache_id];
-                all_cpus.set(cpu, true);
-                core_cpus[core_id].set(cpu, true);
-                cpu_core.push(core_id);
-            }
-        }
-
-        info!(
-            "CPUs: online/possible={}/{} nr_cores={}",
-            nr_cpus, *NR_POSSIBLE_CPUS, nr_cores,
-        );
-
-        let first_cpu = core_cpus[0].first_one().unwrap();
-
-        let mut cpu_pool = Self {
-            nr_cores,
-            nr_cpus,
-            all_cpus,
-            core_cpus,
-            cpu_core,
-            available_cores: bitvec![1; nr_cores],
-            first_cpu,
-            fallback_cpu: first_cpu,
-        };
-        cpu_pool.update_fallback_cpu();
-        Ok(cpu_pool)
-    }
-
-    fn update_fallback_cpu(&mut self) {
-        match self.available_cores.first_one() {
-            Some(next) => self.fallback_cpu = self.core_cpus[next].first_one().unwrap(),
-            None => self.fallback_cpu = self.first_cpu,
-        }
-    }
-
-    fn alloc<'a>(&'a mut self) -> Option<&'a BitVec> {
-        let core = self.available_cores.first_one()?;
-        self.available_cores.set(core, false);
-        self.update_fallback_cpu();
-        Some(&self.core_cpus[core])
-    }
-
-    fn cpus_to_cores(&self, cpus_to_match: &BitVec) -> Result<BitVec> {
-        let mut cpus = cpus_to_match.clone();
-        let mut cores = bitvec![0; self.nr_cores];
-
-        while let Some(cpu) = cpus.first_one() {
-            let core = self.cpu_core[cpu];
-
-            if (self.core_cpus[core].clone() & !cpus.clone()).count_ones() != 0 {
-                bail!(
-                    "CPUs {} partially intersect with core {} ({})",
-                    cpus_to_match,
-                    core,
-                    self.core_cpus[core],
-                );
-            }
-
-            cpus &= !self.core_cpus[core].clone();
-            cores.set(core, true);
-        }
-
-        Ok(cores)
-    }
-
-    fn free<'a>(&'a mut self, cpus_to_free: &BitVec) -> Result<()> {
-        let cores = self.cpus_to_cores(cpus_to_free)?;
-        if (self.available_cores.clone() & &cores).any() {
-            bail!("Some of CPUs {} are already free", cpus_to_free);
-        }
-        self.available_cores |= cores;
-        self.update_fallback_cpu();
-        Ok(())
-    }
-
-    fn next_to_free<'a>(&'a self, cands: &BitVec) -> Result<Option<&'a BitVec>> {
-        let last = match cands.last_one() {
-            Some(ret) => ret,
-            None => return Ok(None),
-        };
-        let core = self.cpu_core[last];
-        if (self.core_cpus[core].clone() & !cands.clone()).count_ones() != 0 {
-            bail!(
-                "CPUs{} partially intersect with core {} ({})",
-                cands,
-                core,
-                self.core_cpus[core]
-            );
-        }
-
-        Ok(Some(&self.core_cpus[core]))
-    }
-
-    fn available_cpus(&self) -> BitVec {
-        let mut cpus = bitvec![0; self.nr_cpus];
-        for core in self.available_cores.iter_ones() {
-            cpus |= &self.core_cpus[core];
-        }
-        cpus
-    }
-}
-
-#[derive(Debug)]
-struct Layer {
-    name: String,
-    kind: LayerKind,
-
-    nr_cpus: usize,
-    cpus: BitVec,
-}
-
-impl Layer {
-    fn new(cpu_pool: &mut CpuPool, name: &str, kind: LayerKind) -> Result<Self> {
-        match &kind {
-            LayerKind::Confined {
-                cpus_range,
-                util_range,
-            } => {
-                let cpus_range = cpus_range.unwrap_or((0, std::usize::MAX));
-                if cpus_range.0 > cpus_range.1 || cpus_range.1 == 0 {
-                    bail!("invalid cpus_range {:?}", cpus_range);
-                }
-                if util_range.0 < 0.0
-                    || util_range.0 > 1.0
-                    || util_range.1 < 0.0
-                    || util_range.1 > 1.0
-                    || util_range.0 >= util_range.1
-                {
-                    bail!("invalid util_range {:?}", util_range);
-                }
-            }
-            _ => {}
-        }
-
-        let nr_cpus = cpu_pool.nr_cpus;
-
-        let mut layer = Self {
-            name: name.into(),
-            kind,
-
-            nr_cpus: 0,
-            cpus: bitvec![0; nr_cpus],
-        };
-
-        match &layer.kind {
-            LayerKind::Confined {
-                cpus_range,
-                util_range,
-            }
-            | LayerKind::Grouped {
-                cpus_range,
-                util_range,
-                ..
-            } => {
-                layer.resize_confined_or_grouped(
-                    cpu_pool,
-                    *cpus_range,
-                    *util_range,
-                    (0.0, 0.0),
-                    (0.0, 0.0),
-                    false,
-                )?;
-            }
-            _ => {}
-        }
-
-        Ok(layer)
-    }
-
-    fn grow_confined_or_grouped(
-        &mut self,
-        cpu_pool: &mut CpuPool,
-        (cpus_min, cpus_max): (usize, usize),
-        (_util_low, util_high): (f64, f64),
-        (layer_load, total_load): (f64, f64),
-        (layer_util, _total_util): (f64, f64),
-        no_load_frac_limit: bool,
-    ) -> Result<bool> {
-        if self.nr_cpus >= cpus_max {
-            return Ok(false);
-        }
-
-        // Do we already have enough?
-        if self.nr_cpus >= cpus_min
-            && (layer_util == 0.0
-                || (self.nr_cpus > 0 && layer_util / self.nr_cpus as f64 <= util_high))
-        {
-            return Ok(false);
-        }
-
-        // Can't have more CPUs than our load fraction.
-        if !no_load_frac_limit
-            && self.nr_cpus >= cpus_min
-            && (total_load >= 0.0
-                && self.nr_cpus as f64 / cpu_pool.nr_cpus as f64 >= layer_load / total_load)
-        {
-            trace!(
-                "layer-{} needs more CPUs (util={:.3}) but is over the load fraction",
-                &self.name,
-                layer_util
-            );
-            return Ok(false);
-        }
-
-        let new_cpus = match cpu_pool.alloc().clone() {
-            Some(ret) => ret.clone(),
-            None => {
-                trace!("layer-{} can't grow, no CPUs", &self.name);
-                return Ok(false);
-            }
-        };
-
-        trace!(
-            "layer-{} adding {} CPUs to {} CPUs",
-            &self.name,
-            new_cpus.count_ones(),
-            self.nr_cpus
-        );
-
-        self.nr_cpus += new_cpus.count_ones();
-        self.cpus |= &new_cpus;
-        Ok(true)
-    }
-
-    fn cpus_to_free(
-        &self,
-        cpu_pool: &mut CpuPool,
-        (cpus_min, _cpus_max): (usize, usize),
-        (util_low, util_high): (f64, f64),
-        (layer_load, total_load): (f64, f64),
-        (layer_util, _total_util): (f64, f64),
-        no_load_frac_limit: bool,
-    ) -> Result<Option<BitVec>> {
-        if self.nr_cpus <= cpus_min {
-            return Ok(None);
-        }
-
-        let cpus_to_free = match cpu_pool.next_to_free(&self.cpus)? {
-            Some(ret) => ret.clone(),
-            None => return Ok(None),
-        };
-
-        let nr_to_free = cpus_to_free.count_ones();
-
-        // If we'd be over the load fraction even after freeing
-        // $cpus_to_free, we have to free.
-        if !no_load_frac_limit
-            && total_load >= 0.0
-            && (self.nr_cpus - nr_to_free) as f64 / cpu_pool.nr_cpus as f64
-                >= layer_load / total_load
-        {
-            return Ok(Some(cpus_to_free));
-        }
-
-        if layer_util / self.nr_cpus as f64 >= util_low {
-            return Ok(None);
-        }
-
-        // Can't shrink if losing the CPUs pushes us over @util_high.
-        match self.nr_cpus - nr_to_free {
-            0 => {
-                if layer_util > 0.0 {
-                    return Ok(None);
-                }
-            }
-            nr_left => {
-                if layer_util / nr_left as f64 >= util_high {
-                    return Ok(None);
-                }
-            }
-        }
-
-        return Ok(Some(cpus_to_free));
-    }
-
-    fn shrink_confined_or_grouped(
-        &mut self,
-        cpu_pool: &mut CpuPool,
-        cpus_range: (usize, usize),
-        util_range: (f64, f64),
-        load: (f64, f64),
-        util: (f64, f64),
-        no_load_frac_limit: bool,
-    ) -> Result<bool> {
-        match self.cpus_to_free(
-            cpu_pool,
-            cpus_range,
-            util_range,
-            load,
-            util,
-            no_load_frac_limit,
-        )? {
-            Some(cpus_to_free) => {
-                trace!("freeing CPUs {}", &cpus_to_free);
-                self.nr_cpus -= cpus_to_free.count_ones();
-                self.cpus &= !cpus_to_free.clone();
-                cpu_pool.free(&cpus_to_free)?;
-                Ok(true)
-            }
-            None => Ok(false),
-        }
-    }
-
-    fn resize_confined_or_grouped(
-        &mut self,
-        cpu_pool: &mut CpuPool,
-        cpus_range: Option<(usize, usize)>,
-        util_range: (f64, f64),
-        load: (f64, f64),
-        util: (f64, f64),
-        no_load_frac_limit: bool,
-    ) -> Result<i64> {
-        let cpus_range = cpus_range.unwrap_or((0, std::usize::MAX));
-        let mut adjusted = 0;
-
-        while self.grow_confined_or_grouped(
-            cpu_pool,
-            cpus_range,
-            util_range,
-            load,
-            util,
-            no_load_frac_limit,
-        )? {
-            adjusted += 1;
-            trace!("{} grew, adjusted={}", &self.name, adjusted);
-        }
-
-        if adjusted == 0 {
-            while self.shrink_confined_or_grouped(
-                cpu_pool,
-                cpus_range,
-                util_range,
-                load,
-                util,
-                no_load_frac_limit,
-            )? {
-                adjusted -= 1;
-                trace!("{} shrunk, adjusted={}", &self.name, adjusted);
-            }
-        }
-
-        if adjusted != 0 {
-            trace!("{} done resizing, adjusted={}", &self.name, adjusted);
-        }
-        Ok(adjusted)
-    }
-}
-
-struct Scheduler<'a> {
-    skel: BpfSkel<'a>,
-    struct_ops: Option<libbpf_rs::Link>,
-    layer_specs: Vec<LayerSpec>,
-
-    sched_intv: Duration,
-    monitor_intv: Duration,
-    no_load_frac_limit: bool,
-
-    cpu_pool: CpuPool,
-    layers: Vec<Layer>,
-
-    proc_reader: procfs::ProcReader,
-    sched_stats: Stats,
-    report_stats: Stats,
-
-    nr_layer_cpus_min_max: Vec<(usize, usize)>,
-    processing_dur: Duration,
-    prev_processing_dur: Duration,
-}
-
-impl<'a> Scheduler<'a> {
-    fn init_layers(skel: &mut OpenBpfSkel, specs: &Vec<LayerSpec>) -> Result<()> {
-        skel.rodata_mut().nr_layers = specs.len() as u32;
-
-        for (spec_i, spec) in specs.iter().enumerate() {
-            let layer = &mut skel.bss_mut().layers[spec_i];
-
-            for (or_i, or) in spec.matches.iter().enumerate() {
-                for (and_i, and) in or.iter().enumerate() {
-                    let mt = &mut layer.matches[or_i].matches[and_i];
-                    match and {
-                        LayerMatch::CgroupPrefix(prefix) => {
-                            mt.kind = bpf_intf::layer_match_kind_MATCH_CGROUP_PREFIX as i32;
-                            copy_into_cstr(&mut mt.cgroup_prefix, prefix.as_str());
-                        }
-                        LayerMatch::CommPrefix(prefix) => {
-                            mt.kind = bpf_intf::layer_match_kind_MATCH_COMM_PREFIX as i32;
-                            copy_into_cstr(&mut mt.comm_prefix, prefix.as_str());
-                        }
-                        LayerMatch::NiceAbove(nice) => {
-                            mt.kind = bpf_intf::layer_match_kind_MATCH_NICE_ABOVE as i32;
-                            mt.nice_above_or_below = *nice;
-                        }
-                        LayerMatch::NiceBelow(nice) => {
-                            mt.kind = bpf_intf::layer_match_kind_MATCH_NICE_BELOW as i32;
-                            mt.nice_above_or_below = *nice;
-                        }
-                    }
-                }
-                layer.matches[or_i].nr_match_ands = or.len() as i32;
-            }
-
-            layer.nr_match_ors = spec.matches.len() as u32;
-
-            match &spec.kind {
-                LayerKind::Open { preempt } | LayerKind::Grouped { preempt, .. } => {
-                    layer.open = true;
-                    layer.preempt = *preempt;
-                }
-                _ => {}
-            }
-        }
-
-        Ok(())
-    }
-
-    fn init(opts: &Opts, layer_specs: Vec<LayerSpec>) -> Result<Self> {
-        let nr_layers = layer_specs.len();
-        let mut cpu_pool = CpuPool::new()?;
-
-        // Open the BPF prog first for verification.
-        let mut skel_builder = BpfSkelBuilder::default();
-        skel_builder.obj_builder.debug(opts.verbose > 1);
-        let mut skel = skel_builder.open().context("Failed to open BPF program")?;
-
-        // Initialize skel according to @opts.
-        skel.rodata_mut().debug = opts.verbose as u32;
-        skel.rodata_mut().slice_ns = opts.slice_us * 1000;
-        skel.rodata_mut().nr_possible_cpus = *NR_POSSIBLE_CPUS as u32;
-        skel.rodata_mut().smt_enabled = cpu_pool.nr_cpus > cpu_pool.nr_cores;
-        for cpu in cpu_pool.all_cpus.iter_ones() {
-            skel.rodata_mut().all_cpus[cpu / 8] |= 1 << (cpu % 8);
-        }
-        Self::init_layers(&mut skel, &layer_specs)?;
-
-        // Attach.
-        let mut skel = skel.load().context("Failed to load BPF program")?;
-        skel.attach().context("Failed to attach BPF program")?;
-        let struct_ops = Some(
-            skel.maps_mut()
-                .layered()
-                .attach_struct_ops()
-                .context("Failed to attach layered struct ops")?,
-        );
-        info!("Layered Scheduler Attached");
-
-        let mut layers = vec![];
-        for spec in layer_specs.iter() {
-            layers.push(Layer::new(&mut cpu_pool, &spec.name, spec.kind.clone())?);
-        }
-
-        // Other stuff.
-        let proc_reader = procfs::ProcReader::new();
-
-        Ok(Self {
-            struct_ops, // should be held to keep it attached
-            layer_specs,
-
-            sched_intv: Duration::from_secs_f64(opts.interval),
-            monitor_intv: Duration::from_secs_f64(opts.monitor),
-            no_load_frac_limit: opts.no_load_frac_limit,
-
-            cpu_pool,
-            layers,
-
-            sched_stats: Stats::new(&mut skel, &proc_reader)?,
-            report_stats: Stats::new(&mut skel, &proc_reader)?,
-
-            nr_layer_cpus_min_max: vec![(0, 0); nr_layers],
-            processing_dur: Duration::from_millis(0),
-            prev_processing_dur: Duration::from_millis(0),
-
-            proc_reader,
-            skel,
-        })
-    }
-
-    fn update_bpf_layer_cpumask(layer: &Layer, bpf_layer: &mut bpf_bss_types::layer) {
-        for bit in 0..layer.cpus.len() {
-            if layer.cpus[bit] {
-                bpf_layer.cpus[bit / 8] |= 1 << (bit % 8);
-            } else {
-                bpf_layer.cpus[bit / 8] &= !(1 << (bit % 8));
-            }
-        }
-        bpf_layer.refresh_cpus = 1;
-    }
-
-    fn step(&mut self) -> Result<()> {
-        let started_at = Instant::now();
-        self.sched_stats
-            .refresh(&mut self.skel, &self.proc_reader, started_at)?;
-        let mut updated = false;
-
-        for idx in 0..self.layers.len() {
-            match self.layers[idx].kind {
-                LayerKind::Confined {
-                    cpus_range,
-                    util_range,
-                }
-                | LayerKind::Grouped {
-                    cpus_range,
-                    util_range,
-                    ..
-                } => {
-                    let load = (
-                        self.sched_stats.layer_loads[idx],
-                        self.sched_stats.total_load,
-                    );
-                    let util = (
-                        self.sched_stats.layer_utils[idx],
-                        self.sched_stats.total_util,
-                    );
-                    if self.layers[idx].resize_confined_or_grouped(
-                        &mut self.cpu_pool,
-                        cpus_range,
-                        util_range,
-                        load,
-                        util,
-                        self.no_load_frac_limit,
-                    )? != 0
-                    {
-                        Self::update_bpf_layer_cpumask(
-                            &self.layers[idx],
-                            &mut self.skel.bss_mut().layers[idx],
-                        );
-                        updated = true;
-                    }
-                }
-                _ => {}
-            }
-        }
-
-        if updated {
-            let available_cpus = self.cpu_pool.available_cpus();
-            let nr_available_cpus = available_cpus.count_ones();
-            for idx in 0..self.layers.len() {
-                let layer = &mut self.layers[idx];
-                let bpf_layer = &mut self.skel.bss_mut().layers[idx];
-                match &layer.kind {
-                    LayerKind::Open { .. } => {
-                        layer.cpus.copy_from_bitslice(&available_cpus);
-                        layer.nr_cpus = nr_available_cpus;
-                        Self::update_bpf_layer_cpumask(layer, bpf_layer);
-                    }
-                    _ => {}
-                }
-            }
-
-            self.skel.bss_mut().fallback_cpu = self.cpu_pool.fallback_cpu as u32;
-
-            for (lidx, layer) in self.layers.iter().enumerate() {
-                self.nr_layer_cpus_min_max[lidx] = (
-                    self.nr_layer_cpus_min_max[lidx].0.min(layer.nr_cpus),
-                    self.nr_layer_cpus_min_max[lidx].1.max(layer.nr_cpus),
-                );
-            }
-        }
-
-        self.processing_dur += Instant::now().duration_since(started_at);
-        Ok(())
-    }
-
-    fn report(&mut self) -> Result<()> {
-        let started_at = Instant::now();
-        self.report_stats
-            .refresh(&mut self.skel, &self.proc_reader, started_at)?;
-        let stats = &self.report_stats;
-
-        let processing_dur = self.processing_dur - self.prev_processing_dur;
-        self.prev_processing_dur = self.processing_dur;
-
-        let lsum = |idx| stats.bpf_stats.lstats_sums[idx as usize];
-        let total = lsum(bpf_intf::layer_stat_idx_LSTAT_LOCAL)
-            + lsum(bpf_intf::layer_stat_idx_LSTAT_GLOBAL);
-        let lsum_pct = |idx| {
-            if total != 0 {
-                lsum(idx) as f64 / total as f64 * 100.0
-            } else {
-                0.0
-            }
-        };
-
-        info!(
-            "tot={:7} local={:5.2} open_idle={:5.2} affn_viol={:5.2} tctx_err={} proc={:?}ms",
-            total,
-            lsum_pct(bpf_intf::layer_stat_idx_LSTAT_LOCAL),
-            lsum_pct(bpf_intf::layer_stat_idx_LSTAT_OPEN_IDLE),
-            lsum_pct(bpf_intf::layer_stat_idx_LSTAT_AFFN_VIOL),
-            stats.prev_bpf_stats.gstats
-                [bpf_intf::global_stat_idx_GSTAT_TASK_CTX_FREE_FAILED as usize],
-            processing_dur.as_millis(),
-        );
-
-        info!(
-            "busy={:5.1} util={:7.1} load={:9.1} fallback_cpu={:3}",
-            stats.cpu_busy * 100.0,
-            stats.total_util * 100.0,
-            stats.total_load,
-            self.cpu_pool.fallback_cpu,
-        );
-
-        let header_width = self
-            .layer_specs
-            .iter()
-            .map(|spec| spec.name.len())
-            .max()
-            .unwrap()
-            .max(4);
-
-        let calc_frac = |a, b| {
-            if b != 0.0 { a / b * 100.0 } else { 0.0 }
-        };
-
-        for (lidx, (spec, layer)) in self.layer_specs.iter().zip(self.layers.iter()).enumerate() {
-            let lstat = |sidx| stats.bpf_stats.lstats[lidx][sidx as usize];
-            let ltotal = lstat(bpf_intf::layer_stat_idx_LSTAT_LOCAL)
-                + lstat(bpf_intf::layer_stat_idx_LSTAT_GLOBAL);
-            let lstat_pct = |sidx| {
-                if ltotal != 0 {
-                    lstat(sidx) as f64 / ltotal as f64 * 100.0
-                } else {
-                    0.0
-                }
-            };
-
-            info!(
-                "  {:<width$}: util/frac={:7.1}/{:5.1} load/frac={:9.1}:{:5.1} tasks={:6}",
-                spec.name,
-                stats.layer_utils[lidx] * 100.0,
-                calc_frac(stats.layer_utils[lidx], stats.total_util),
-                stats.layer_loads[lidx],
-                calc_frac(stats.layer_loads[lidx], stats.total_load),
-                stats.nr_layer_tasks[lidx],
-                width = header_width,
-            );
-            info!(
-                "  {:<width$}  tot={:7} local={:5.2} open_idle={:5.2} preempt={:5.2} affn_viol={:5.2}",
-                "",
-                ltotal,
-                lstat_pct(bpf_intf::layer_stat_idx_LSTAT_LOCAL),
-                lstat_pct(bpf_intf::layer_stat_idx_LSTAT_OPEN_IDLE),
-                lstat_pct(bpf_intf::layer_stat_idx_LSTAT_PREEMPT),
-                lstat_pct(bpf_intf::layer_stat_idx_LSTAT_AFFN_VIOL),
-                width = header_width,
-            );
-            info!(
-                "  {:<width$}  cpus={:3} [{:3},{:3}] {}",
-                "",
-                layer.nr_cpus,
-                self.nr_layer_cpus_min_max[lidx].0,
-                self.nr_layer_cpus_min_max[lidx].1,
-                format_bitvec(&layer.cpus),
-                width = header_width
-            );
-            self.nr_layer_cpus_min_max[lidx] = (layer.nr_cpus, layer.nr_cpus);
-        }
-
-        self.processing_dur += Instant::now().duration_since(started_at);
-        Ok(())
-    }
-
-    fn run(&mut self, shutdown: Arc<AtomicBool>) -> Result<()> {
-        let now = Instant::now();
-        let mut next_sched_at = now + self.sched_intv;
-        let mut next_monitor_at = now + self.monitor_intv;
-
-        while !shutdown.load(Ordering::Relaxed) && !UserExitInfo::exited(&self.skel.bss().uei)? {
-            let now = Instant::now();
-
-            if now >= next_sched_at {
-                self.step()?;
-                while next_sched_at < now {
-                    next_sched_at += self.sched_intv;
-                }
-            }
-
-            if now >= next_monitor_at {
-                self.report()?;
-                while next_monitor_at < now {
-                    next_monitor_at += self.monitor_intv;
-                }
-            }
-
-            std::thread::sleep(
-                next_sched_at
-                    .min(next_monitor_at)
-                    .duration_since(Instant::now()),
-            );
-        }
-
-        self.struct_ops.take();
-        UserExitInfo::read(&self.skel.bss().uei)?.report()
-    }
-}
-
-impl<'a> Drop for Scheduler<'a> {
-    fn drop(&mut self) {
-        if let Some(struct_ops) = self.struct_ops.take() {
-            drop(struct_ops);
-        }
-    }
-}
-
-fn write_example_file(path: &str) -> Result<()> {
-    let example = LayerConfig {
-        specs: vec![
-            LayerSpec {
-                name: "batch".into(),
-                comment: Some("tasks under system.slice or tasks with nice value > 0".into()),
-                matches: vec![
-                    vec![LayerMatch::CgroupPrefix("system.slice/".into())],
-                    vec![LayerMatch::NiceAbove(0)],
-                ],
-                kind: LayerKind::Confined {
-                    cpus_range: Some((0, 16)),
-                    util_range: (0.8, 0.9),
-                },
-            },
-            LayerSpec {
-                name: "immediate".into(),
-                comment: Some("tasks under workload.slice with nice value < 0".into()),
-                matches: vec![vec![
-                    LayerMatch::CgroupPrefix("workload.slice/".into()),
-                    LayerMatch::NiceBelow(0),
-                ]],
-                kind: LayerKind::Open { preempt: true },
-            },
-            LayerSpec {
-                name: "normal".into(),
-                comment: Some("the rest".into()),
-                matches: vec![vec![]],
-                kind: LayerKind::Grouped {
-                    cpus_range: None,
-                    util_range: (0.5, 0.6),
-                    preempt: false,
-                },
-            },
-        ],
-    };
-
-    let mut f = fs::OpenOptions::new()
-        .create_new(true)
-        .write(true)
-        .open(path)?;
-    Ok(f.write_all(serde_json::to_string_pretty(&example)?.as_bytes())?)
-}
-
-fn verify_layer_specs(specs: &[LayerSpec]) -> Result<()> {
-    let nr_specs = specs.len();
-    if nr_specs == 0 {
-        bail!("No layer spec");
-    }
-    if nr_specs > MAX_LAYERS {
-        bail!("Too many layer specs");
-    }
-
-    for (idx, spec) in specs.iter().enumerate() {
-        if idx < nr_specs - 1 {
-            if spec.matches.len() == 0 {
-                bail!("Non-terminal spec {:?} has NULL matches", spec.name);
-            }
-        } else {
-            if spec.matches.len() != 1 || spec.matches[0].len() != 0 {
-                bail!("Terminal spec {:?} must have an empty match", spec.name);
-            }
-        }
-
-        if spec.matches.len() > MAX_LAYER_MATCH_ORS {
-            bail!(
-                "Spec {:?} has too many ({}) OR match blocks",
-                spec.name,
-                spec.matches.len()
-            );
-        }
-
-        for (ands_idx, ands) in spec.matches.iter().enumerate() {
-            if ands.len() > NR_LAYER_MATCH_KINDS {
-                bail!(
-                    "Spec {:?}'s {}th OR block has too many ({}) match conditions",
-                    spec.name,
-                    ands_idx,
-                    ands.len()
-                );
-            }
-            for one in ands.iter() {
-                match one {
-                    LayerMatch::CgroupPrefix(prefix) => {
-                        if prefix.len() > MAX_PATH {
-                            bail!("Spec {:?} has too long a cgroup prefix", spec.name);
-                        }
-                    }
-                    LayerMatch::CommPrefix(prefix) => {
-                        if prefix.len() > MAX_COMM {
-                            bail!("Spec {:?} has too long a comm prefix", spec.name);
-                        }
-                    }
-                    _ => {}
-                }
-            }
-        }
-
-        match spec.kind {
-            LayerKind::Confined {
-                cpus_range,
-                util_range,
-            }
-            | LayerKind::Grouped {
-                cpus_range,
-                util_range,
-                ..
-            } => {
-                if let Some((cpus_min, cpus_max)) = cpus_range {
-                    if cpus_min > cpus_max {
-                        bail!(
-                            "Spec {:?} has invalid cpus_range({}, {})",
-                            spec.name,
-                            cpus_min,
-                            cpus_max
-                        );
-                    }
-                }
-                if util_range.0 >= util_range.1 {
-                    bail!(
-                        "Spec {:?} has invalid util_range ({}, {})",
-                        spec.name,
-                        util_range.0,
-                        util_range.1
-                    );
-                }
-            }
-            _ => {}
-        }
-    }
-
-    Ok(())
-}
-
-fn main() -> Result<()> {
-    let opts = Opts::parse();
-
-    let llv = match opts.verbose {
-        0 => simplelog::LevelFilter::Info,
-        1 => simplelog::LevelFilter::Debug,
-        _ => simplelog::LevelFilter::Trace,
-    };
-    let mut lcfg = simplelog::ConfigBuilder::new();
-    lcfg.set_time_level(simplelog::LevelFilter::Error)
-        .set_location_level(simplelog::LevelFilter::Off)
-        .set_target_level(simplelog::LevelFilter::Off)
-        .set_thread_level(simplelog::LevelFilter::Off);
-    simplelog::TermLogger::init(
-        llv,
-        lcfg.build(),
-        simplelog::TerminalMode::Stderr,
-        simplelog::ColorChoice::Auto,
-    )?;
-
-    debug!("opts={:?}", &opts);
-
-    if let Some(path) = &opts.example {
-        write_example_file(path)?;
-        return Ok(());
-    }
-
-    let mut layer_config = LayerConfig { specs: vec![] };
-    for (idx, input) in opts.specs.iter().enumerate() {
-        layer_config.specs.append(
-            &mut LayerSpec::parse(input)
-                .context(format!("Failed to parse specs[{}] ({:?})", idx, input))?,
-        );
-    }
-
-    debug!("specs={}", serde_json::to_string_pretty(&layer_config)?);
-    verify_layer_specs(&layer_config.specs)?;
-
-    let mut sched = Scheduler::init(&opts, layer_config.specs)?;
-
-    let shutdown = Arc::new(AtomicBool::new(false));
-    let shutdown_clone = shutdown.clone();
-    ctrlc::set_handler(move || {
-        shutdown_clone.store(true, Ordering::Relaxed);
-    })
-    .context("Error setting Ctrl-C handler")?;
-
-    sched.run(shutdown)
-}
diff --git a/tools/sched_ext/scx_pair.bpf.c b/tools/sched_ext/scx_pair.bpf.c
deleted file mode 100644
index 9da53c4b3..000000000
--- a/tools/sched_ext/scx_pair.bpf.c
+++ /dev/null
@@ -1,626 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * A demo sched_ext core-scheduler which always makes every sibling CPU pair
- * execute from the same CPU cgroup.
- *
- * This scheduler is a minimal implementation and would need some form of
- * priority handling both inside each cgroup and across the cgroups to be
- * practically useful.
- *
- * Each CPU in the system is paired with exactly one other CPU, according to a
- * "stride" value that can be specified when the BPF scheduler program is first
- * loaded. Throughout the runtime of the scheduler, these CPU pairs guarantee
- * that they will only ever schedule tasks that belong to the same CPU cgroup.
- *
- * Scheduler Initialization
- * ------------------------
- *
- * The scheduler BPF program is first initialized from user space, before it is
- * enabled. During this initialization process, each CPU on the system is
- * assigned several values that are constant throughout its runtime:
- *
- * 1. *Pair CPU*: The CPU that it synchronizes with when making scheduling
- *		  decisions. Paired CPUs always schedule tasks from the same
- *		  CPU cgroup, and synchronize with each other to guarantee
- *		  that this constraint is not violated.
- * 2. *Pair ID*:  Each CPU pair is assigned a Pair ID, which is used to access
- *		  a struct pair_ctx object that is shared between the pair.
- * 3. *In-pair-index*: An index, 0 or 1, that is assigned to each core in the
- *		       pair. Each struct pair_ctx has an active_mask field,
- *		       which is a bitmap used to indicate whether each core
- *		       in the pair currently has an actively running task.
- *		       This index specifies which entry in the bitmap corresponds
- *		       to each CPU in the pair.
- *
- * During this initialization, the CPUs are paired according to a "stride" that
- * may be specified when invoking the user space program that initializes and
- * loads the scheduler. By default, the stride is 1/2 the total number of CPUs.
- *
- * Tasks and cgroups
- * -----------------
- *
- * Every cgroup in the system is registered with the scheduler using the
- * pair_cgroup_init() callback, and every task in the system is associated with
- * exactly one cgroup. At a high level, the idea with the pair scheduler is to
- * always schedule tasks from the same cgroup within a given CPU pair. When a
- * task is enqueued (i.e. passed to the pair_enqueue() callback function), its
- * cgroup ID is read from its task struct, and then a corresponding queue map
- * is used to FIFO-enqueue the task for that cgroup.
- *
- * If you look through the implementation of the scheduler, you'll notice that
- * there is quite a bit of complexity involved with looking up the per-cgroup
- * FIFO queue that we enqueue tasks in. For example, there is a cgrp_q_idx_hash
- * BPF hash map that is used to map a cgroup ID to a globally unique ID that's
- * allocated in the BPF program. This is done because we use separate maps to
- * store the FIFO queue of tasks, and the length of that map, per cgroup. This
- * complexity is only present because of current deficiencies in BPF that will
- * soon be addressed. The main point to keep in mind is that newly enqueued
- * tasks are added to their cgroup's FIFO queue.
- *
- * Dispatching tasks
- * -----------------
- *
- * This section will describe how enqueued tasks are dispatched and scheduled.
- * Tasks are dispatched in pair_dispatch(), and at a high level the workflow is
- * as follows:
- *
- * 1. Fetch the struct pair_ctx for the current CPU. As mentioned above, this is
- *    the structure that's used to synchronize amongst the two pair CPUs in their
- *    scheduling decisions. After any of the following events have occurred:
- *
- * - The cgroup's slice run has expired, or
- * - The cgroup becomes empty, or
- * - Either CPU in the pair is preempted by a higher priority scheduling class
- *
- * The cgroup transitions to the draining state and stops executing new tasks
- * from the cgroup.
- *
- * 2. If the pair is still executing a task, mark the pair_ctx as draining, and
- *    wait for the pair CPU to be preempted.
- *
- * 3. Otherwise, if the pair CPU is not running a task, we can move onto
- *    scheduling new tasks. Pop the next cgroup id from the top_q queue.
- *
- * 4. Pop a task from that cgroup's FIFO task queue, and begin executing it.
- *
- * Note again that this scheduling behavior is simple, but the implementation
- * is complex mostly because this it hits several BPF shortcomings and has to
- * work around in often awkward ways. Most of the shortcomings are expected to
- * be resolved in the near future which should allow greatly simplifying this
- * scheduler.
- *
- * Dealing with preemption
- * -----------------------
- *
- * SCX is the lowest priority sched_class, and could be preempted by them at
- * any time. To address this, the scheduler implements pair_cpu_release() and
- * pair_cpu_acquire() callbacks which are invoked by the core scheduler when
- * the scheduler loses and gains control of the CPU respectively.
- *
- * In pair_cpu_release(), we mark the pair_ctx as having been preempted, and
- * then invoke:
- *
- * scx_bpf_kick_cpu(pair_cpu, SCX_KICK_PREEMPT | SCX_KICK_WAIT);
- *
- * This preempts the pair CPU, and waits until it has re-entered the scheduler
- * before returning. This is necessary to ensure that the higher priority
- * sched_class that preempted our scheduler does not schedule a task
- * concurrently with our pair CPU.
- *
- * When the CPU is re-acquired in pair_cpu_acquire(), we unmark the preemption
- * in the pair_ctx, and send another resched IPI to the pair CPU to re-enable
- * pair scheduling.
- *
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#include <scx/common.bpf.h>
-#include "scx_pair.h"
-
-char _license[] SEC("license") = "GPL";
-
-const volatile bool switch_partial;
-
-/* !0 for veristat, set during init */
-const volatile u32 nr_cpu_ids = 1;
-
-/* a pair of CPUs stay on a cgroup for this duration */
-const volatile u32 pair_batch_dur_ns = SCX_SLICE_DFL;
-
-/* cpu ID -> pair cpu ID */
-const volatile s32 RESIZABLE_ARRAY(rodata, pair_cpu);
-
-/* cpu ID -> pair_id */
-const volatile u32 RESIZABLE_ARRAY(rodata, pair_id);
-
-/* CPU ID -> CPU # in the pair (0 or 1) */
-const volatile u32 RESIZABLE_ARRAY(rodata, in_pair_idx);
-
-struct pair_ctx {
-	struct bpf_spin_lock	lock;
-
-	/* the cgroup the pair is currently executing */
-	u64			cgid;
-
-	/* the pair started executing the current cgroup at */
-	u64			started_at;
-
-	/* whether the current cgroup is draining */
-	bool			draining;
-
-	/* the CPUs that are currently active on the cgroup */
-	u32			active_mask;
-
-	/*
-	 * the CPUs that are currently preempted and running tasks in a
-	 * different scheduler.
-	 */
-	u32			preempted_mask;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__type(key, u32);
-	__type(value, struct pair_ctx);
-} pair_ctx SEC(".maps");
-
-/* queue of cgrp_q's possibly with tasks on them */
-struct {
-	__uint(type, BPF_MAP_TYPE_QUEUE);
-	/*
-	 * Because it's difficult to build strong synchronization encompassing
-	 * multiple non-trivial operations in BPF, this queue is managed in an
-	 * opportunistic way so that we guarantee that a cgroup w/ active tasks
-	 * is always on it but possibly multiple times. Once we have more robust
-	 * synchronization constructs and e.g. linked list, we should be able to
-	 * do this in a prettier way but for now just size it big enough.
-	 */
-	__uint(max_entries, 4 * MAX_CGRPS);
-	__type(value, u64);
-} top_q SEC(".maps");
-
-/* per-cgroup q which FIFOs the tasks from the cgroup */
-struct cgrp_q {
-	__uint(type, BPF_MAP_TYPE_QUEUE);
-	__uint(max_entries, MAX_QUEUED);
-	__type(value, u32);
-};
-
-/*
- * Ideally, we want to allocate cgrp_q and cgrq_q_len in the cgroup local
- * storage; however, a cgroup local storage can only be accessed from the BPF
- * progs attached to the cgroup. For now, work around by allocating array of
- * cgrp_q's and then allocating per-cgroup indices.
- *
- * Another caveat: It's difficult to populate a large array of maps statically
- * or from BPF. Initialize it from userland.
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY_OF_MAPS);
-	__uint(max_entries, MAX_CGRPS);
-	__type(key, s32);
-	__array(values, struct cgrp_q);
-} cgrp_q_arr SEC(".maps");
-
-static u64 cgrp_q_len[MAX_CGRPS];
-
-/*
- * This and cgrp_q_idx_hash combine into a poor man's IDR. This likely would be
- * useful to have as a map type.
- */
-static u32 cgrp_q_idx_cursor;
-static u64 cgrp_q_idx_busy[MAX_CGRPS];
-
-/*
- * All added up, the following is what we do:
- *
- * 1. When a cgroup is enabled, RR cgroup_q_idx_busy array doing cmpxchg looking
- *    for a free ID. If not found, fail cgroup creation with -EBUSY.
- *
- * 2. Hash the cgroup ID to the allocated cgrp_q_idx in the following
- *    cgrp_q_idx_hash.
- *
- * 3. Whenever a cgrp_q needs to be accessed, first look up the cgrp_q_idx from
- *    cgrp_q_idx_hash and then access the corresponding entry in cgrp_q_arr.
- *
- * This is sadly complicated for something pretty simple. Hopefully, we should
- * be able to simplify in the future.
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_HASH);
-	__uint(max_entries, MAX_CGRPS);
-	__uint(key_size, sizeof(u64));		/* cgrp ID */
-	__uint(value_size, sizeof(s32));	/* cgrp_q idx */
-} cgrp_q_idx_hash SEC(".maps");
-
-/* statistics */
-u64 nr_total, nr_dispatched, nr_missing, nr_kicks, nr_preemptions;
-u64 nr_exps, nr_exp_waits, nr_exp_empty;
-u64 nr_cgrp_next, nr_cgrp_coll, nr_cgrp_empty;
-
-struct user_exit_info uei;
-
-static bool time_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
-void BPF_STRUCT_OPS(pair_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	struct cgroup *cgrp;
-	struct cgrp_q *cgq;
-	s32 pid = p->pid;
-	u64 cgid;
-	u32 *q_idx;
-	u64 *cgq_len;
-
-	__sync_fetch_and_add(&nr_total, 1);
-
-	cgrp = scx_bpf_task_cgroup(p);
-	cgid = cgrp->kn->id;
-	bpf_cgroup_release(cgrp);
-
-	/* find the cgroup's q and push @p into it */
-	q_idx = bpf_map_lookup_elem(&cgrp_q_idx_hash, &cgid);
-	if (!q_idx) {
-		scx_bpf_error("failed to lookup q_idx for cgroup[%llu]", cgid);
-		return;
-	}
-
-	cgq = bpf_map_lookup_elem(&cgrp_q_arr, q_idx);
-	if (!cgq) {
-		scx_bpf_error("failed to lookup q_arr for cgroup[%llu] q_idx[%u]",
-			      cgid, *q_idx);
-		return;
-	}
-
-	if (bpf_map_push_elem(cgq, &pid, 0)) {
-		scx_bpf_error("cgroup[%llu] queue overflow", cgid);
-		return;
-	}
-
-	/* bump q len, if going 0 -> 1, queue cgroup into the top_q */
-	cgq_len = MEMBER_VPTR(cgrp_q_len, [*q_idx]);
-	if (!cgq_len) {
-		scx_bpf_error("MEMBER_VTPR malfunction");
-		return;
-	}
-
-	if (!__sync_fetch_and_add(cgq_len, 1) &&
-	    bpf_map_push_elem(&top_q, &cgid, 0)) {
-		scx_bpf_error("top_q overflow");
-		return;
-	}
-}
-
-static int lookup_pairc_and_mask(s32 cpu, struct pair_ctx **pairc, u32 *mask)
-{
-	u32 *vptr;
-
-	vptr = (u32 *)ARRAY_ELEM_PTR(pair_id, cpu, nr_cpu_ids);
-	if (!vptr)
-		return -EINVAL;
-
-	*pairc = bpf_map_lookup_elem(&pair_ctx, vptr);
-	if (!(*pairc))
-		return -EINVAL;
-
-	vptr = (u32 *)ARRAY_ELEM_PTR(in_pair_idx, cpu, nr_cpu_ids);
-	if (!vptr)
-		return -EINVAL;
-
-	*mask = 1U << *vptr;
-
-	return 0;
-}
-
-static int try_dispatch(s32 cpu)
-{
-	struct pair_ctx *pairc;
-	struct bpf_map *cgq_map;
-	struct task_struct *p;
-	u64 now = bpf_ktime_get_ns();
-	bool kick_pair = false;
-	bool expired, pair_preempted;
-	u32 *vptr, in_pair_mask;
-	s32 pid, q_idx;
-	u64 cgid;
-	int ret;
-
-	ret = lookup_pairc_and_mask(cpu, &pairc, &in_pair_mask);
-	if (ret) {
-		scx_bpf_error("failed to lookup pairc and in_pair_mask for cpu[%d]",
-			      cpu);
-		return -ENOENT;
-	}
-
-	bpf_spin_lock(&pairc->lock);
-	pairc->active_mask &= ~in_pair_mask;
-
-	expired = time_before(pairc->started_at + pair_batch_dur_ns, now);
-	if (expired || pairc->draining) {
-		u64 new_cgid = 0;
-
-		__sync_fetch_and_add(&nr_exps, 1);
-
-		/*
-		 * We're done with the current cgid. An obvious optimization
-		 * would be not draining if the next cgroup is the current one.
-		 * For now, be dumb and always expire.
-		 */
-		pairc->draining = true;
-
-		pair_preempted = pairc->preempted_mask;
-		if (pairc->active_mask || pair_preempted) {
-			/*
-			 * The other CPU is still active, or is no longer under
-			 * our control due to e.g. being preempted by a higher
-			 * priority sched_class. We want to wait until this
-			 * cgroup expires, or until control of our pair CPU has
-			 * been returned to us.
-			 *
-			 * If the pair controls its CPU, and the time already
-			 * expired, kick.  When the other CPU arrives at
-			 * dispatch and clears its active mask, it'll push the
-			 * pair to the next cgroup and kick this CPU.
-			 */
-			__sync_fetch_and_add(&nr_exp_waits, 1);
-			bpf_spin_unlock(&pairc->lock);
-			if (expired && !pair_preempted)
-				kick_pair = true;
-			goto out_maybe_kick;
-		}
-
-		bpf_spin_unlock(&pairc->lock);
-
-		/*
-		 * Pick the next cgroup. It'd be easier / cleaner to not drop
-		 * pairc->lock and use stronger synchronization here especially
-		 * given that we'll be switching cgroups significantly less
-		 * frequently than tasks. Unfortunately, bpf_spin_lock can't
-		 * really protect anything non-trivial. Let's do opportunistic
-		 * operations instead.
-		 */
-		bpf_repeat(BPF_MAX_LOOPS) {
-			u32 *q_idx;
-			u64 *cgq_len;
-
-			if (bpf_map_pop_elem(&top_q, &new_cgid)) {
-				/* no active cgroup, go idle */
-				__sync_fetch_and_add(&nr_exp_empty, 1);
-				return 0;
-			}
-
-			q_idx = bpf_map_lookup_elem(&cgrp_q_idx_hash, &new_cgid);
-			if (!q_idx)
-				continue;
-
-			/*
-			 * This is the only place where empty cgroups are taken
-			 * off the top_q.
-			 */
-			cgq_len = MEMBER_VPTR(cgrp_q_len, [*q_idx]);
-			if (!cgq_len || !*cgq_len)
-				continue;
-
-			/*
-			 * If it has any tasks, requeue as we may race and not
-			 * execute it.
-			 */
-			bpf_map_push_elem(&top_q, &new_cgid, 0);
-			break;
-		}
-
-		bpf_spin_lock(&pairc->lock);
-
-		/*
-		 * The other CPU may already have started on a new cgroup while
-		 * we dropped the lock. Make sure that we're still draining and
-		 * start on the new cgroup.
-		 */
-		if (pairc->draining && !pairc->active_mask) {
-			__sync_fetch_and_add(&nr_cgrp_next, 1);
-			pairc->cgid = new_cgid;
-			pairc->started_at = now;
-			pairc->draining = false;
-			kick_pair = true;
-		} else {
-			__sync_fetch_and_add(&nr_cgrp_coll, 1);
-		}
-	}
-
-	cgid = pairc->cgid;
-	pairc->active_mask |= in_pair_mask;
-	bpf_spin_unlock(&pairc->lock);
-
-	/* again, it'd be better to do all these with the lock held, oh well */
-	vptr = bpf_map_lookup_elem(&cgrp_q_idx_hash, &cgid);
-	if (!vptr) {
-		scx_bpf_error("failed to lookup q_idx for cgroup[%llu]", cgid);
-		return -ENOENT;
-	}
-	q_idx = *vptr;
-
-	/* claim one task from cgrp_q w/ q_idx */
-	bpf_repeat(BPF_MAX_LOOPS) {
-		u64 *cgq_len, len;
-
-		cgq_len = MEMBER_VPTR(cgrp_q_len, [q_idx]);
-		if (!cgq_len || !(len = *(volatile u64 *)cgq_len)) {
-			/* the cgroup must be empty, expire and repeat */
-			__sync_fetch_and_add(&nr_cgrp_empty, 1);
-			bpf_spin_lock(&pairc->lock);
-			pairc->draining = true;
-			pairc->active_mask &= ~in_pair_mask;
-			bpf_spin_unlock(&pairc->lock);
-			return -EAGAIN;
-		}
-
-		if (__sync_val_compare_and_swap(cgq_len, len, len - 1) != len)
-			continue;
-
-		break;
-	}
-
-	cgq_map = bpf_map_lookup_elem(&cgrp_q_arr, &q_idx);
-	if (!cgq_map) {
-		scx_bpf_error("failed to lookup cgq_map for cgroup[%llu] q_idx[%d]",
-			      cgid, q_idx);
-		return -ENOENT;
-	}
-
-	if (bpf_map_pop_elem(cgq_map, &pid)) {
-		scx_bpf_error("cgq_map is empty for cgroup[%llu] q_idx[%d]",
-			      cgid, q_idx);
-		return -ENOENT;
-	}
-
-	p = bpf_task_from_pid(pid);
-	if (p) {
-		__sync_fetch_and_add(&nr_dispatched, 1);
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, 0);
-		bpf_task_release(p);
-	} else {
-		/* we don't handle dequeues, retry on lost tasks */
-		__sync_fetch_and_add(&nr_missing, 1);
-		return -EAGAIN;
-	}
-
-out_maybe_kick:
-	if (kick_pair) {
-		s32 *pair = (s32 *)ARRAY_ELEM_PTR(pair_cpu, cpu, nr_cpu_ids);
-		if (pair) {
-			__sync_fetch_and_add(&nr_kicks, 1);
-			scx_bpf_kick_cpu(*pair, SCX_KICK_PREEMPT);
-		}
-	}
-	return 0;
-}
-
-void BPF_STRUCT_OPS(pair_dispatch, s32 cpu, struct task_struct *prev)
-{
-	bpf_repeat(BPF_MAX_LOOPS) {
-		if (try_dispatch(cpu) != -EAGAIN)
-			break;
-	}
-}
-
-void BPF_STRUCT_OPS(pair_cpu_acquire, s32 cpu, struct scx_cpu_acquire_args *args)
-{
-	int ret;
-	u32 in_pair_mask;
-	struct pair_ctx *pairc;
-	bool kick_pair;
-
-	ret = lookup_pairc_and_mask(cpu, &pairc, &in_pair_mask);
-	if (ret)
-		return;
-
-	bpf_spin_lock(&pairc->lock);
-	pairc->preempted_mask &= ~in_pair_mask;
-	/* Kick the pair CPU, unless it was also preempted. */
-	kick_pair = !pairc->preempted_mask;
-	bpf_spin_unlock(&pairc->lock);
-
-	if (kick_pair) {
-		s32 *pair = (s32 *)ARRAY_ELEM_PTR(pair_cpu, cpu, nr_cpu_ids);
-
-		if (pair) {
-			__sync_fetch_and_add(&nr_kicks, 1);
-			scx_bpf_kick_cpu(*pair, SCX_KICK_PREEMPT);
-		}
-	}
-}
-
-void BPF_STRUCT_OPS(pair_cpu_release, s32 cpu, struct scx_cpu_release_args *args)
-{
-	int ret;
-	u32 in_pair_mask;
-	struct pair_ctx *pairc;
-	bool kick_pair;
-
-	ret = lookup_pairc_and_mask(cpu, &pairc, &in_pair_mask);
-	if (ret)
-		return;
-
-	bpf_spin_lock(&pairc->lock);
-	pairc->preempted_mask |= in_pair_mask;
-	pairc->active_mask &= ~in_pair_mask;
-	/* Kick the pair CPU if it's still running. */
-	kick_pair = pairc->active_mask;
-	pairc->draining = true;
-	bpf_spin_unlock(&pairc->lock);
-
-	if (kick_pair) {
-		s32 *pair = (s32 *)ARRAY_ELEM_PTR(pair_cpu, cpu, nr_cpu_ids);
-
-		if (pair) {
-			__sync_fetch_and_add(&nr_kicks, 1);
-			scx_bpf_kick_cpu(*pair, SCX_KICK_PREEMPT | SCX_KICK_WAIT);
-		}
-	}
-	__sync_fetch_and_add(&nr_preemptions, 1);
-}
-
-s32 BPF_STRUCT_OPS(pair_cgroup_init, struct cgroup *cgrp)
-{
-	u64 cgid = cgrp->kn->id;
-	s32 i, q_idx;
-
-	bpf_for(i, 0, MAX_CGRPS) {
-		q_idx = __sync_fetch_and_add(&cgrp_q_idx_cursor, 1) % MAX_CGRPS;
-		if (!__sync_val_compare_and_swap(&cgrp_q_idx_busy[q_idx], 0, 1))
-			break;
-	}
-	if (i == MAX_CGRPS)
-		return -EBUSY;
-
-	if (bpf_map_update_elem(&cgrp_q_idx_hash, &cgid, &q_idx, BPF_ANY)) {
-		u64 *busy = MEMBER_VPTR(cgrp_q_idx_busy, [q_idx]);
-		if (busy)
-			*busy = 0;
-		return -EBUSY;
-	}
-
-	return 0;
-}
-
-void BPF_STRUCT_OPS(pair_cgroup_exit, struct cgroup *cgrp)
-{
-	u64 cgid = cgrp->kn->id;
-	s32 *q_idx;
-
-	q_idx = bpf_map_lookup_elem(&cgrp_q_idx_hash, &cgid);
-	if (q_idx) {
-		u64 *busy = MEMBER_VPTR(cgrp_q_idx_busy, [*q_idx]);
-		if (busy)
-			*busy = 0;
-		bpf_map_delete_elem(&cgrp_q_idx_hash, &cgid);
-	}
-}
-
-s32 BPF_STRUCT_OPS(pair_init)
-{
-	if (!switch_partial)
-		scx_bpf_switch_all();
-	return 0;
-}
-
-void BPF_STRUCT_OPS(pair_exit, struct scx_exit_info *ei)
-{
-	uei_record(&uei, ei);
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops pair_ops = {
-	.enqueue		= (void *)pair_enqueue,
-	.dispatch		= (void *)pair_dispatch,
-	.cpu_acquire		= (void *)pair_cpu_acquire,
-	.cpu_release		= (void *)pair_cpu_release,
-	.cgroup_init		= (void *)pair_cgroup_init,
-	.cgroup_exit		= (void *)pair_cgroup_exit,
-	.init			= (void *)pair_init,
-	.exit			= (void *)pair_exit,
-	.name			= "pair",
-};
diff --git a/tools/sched_ext/scx_pair.c b/tools/sched_ext/scx_pair.c
deleted file mode 100644
index 1eb30efeb..000000000
--- a/tools/sched_ext/scx_pair.c
+++ /dev/null
@@ -1,169 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#include <stdio.h>
-#include <unistd.h>
-#include <inttypes.h>
-#include <signal.h>
-#include <libgen.h>
-#include <bpf/bpf.h>
-#include <scx/common.h>
-#include "scx_pair.h"
-#include "scx_pair.bpf.skel.h"
-
-const char help_fmt[] =
-"A demo sched_ext core-scheduler which always makes every sibling CPU pair\n"
-"execute from the same CPU cgroup.\n"
-"\n"
-"See the top-level comment in .bpf.c for more details.\n"
-"\n"
-"Usage: %s [-S STRIDE] [-p]\n"
-"\n"
-"  -S STRIDE     Override CPU pair stride (default: nr_cpus_ids / 2)\n"
-"  -p            Switch only tasks on SCHED_EXT policy intead of all\n"
-"  -h            Display this help and exit\n";
-
-static volatile int exit_req;
-
-static void sigint_handler(int dummy)
-{
-	exit_req = 1;
-}
-
-int main(int argc, char **argv)
-{
-	struct scx_pair *skel;
-	struct bpf_link *link;
-	__u64 seq = 0;
-	__s32 stride, i, opt, outer_fd;
-
-	signal(SIGINT, sigint_handler);
-	signal(SIGTERM, sigint_handler);
-
-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
-
-	skel = scx_pair__open();
-	SCX_BUG_ON(!skel, "Failed to open skel");
-
-	skel->rodata->nr_cpu_ids = libbpf_num_possible_cpus();
-
-	/* pair up the earlier half to the latter by default, override with -s */
-	stride = skel->rodata->nr_cpu_ids / 2;
-
-	while ((opt = getopt(argc, argv, "S:ph")) != -1) {
-		switch (opt) {
-		case 'S':
-			stride = strtoul(optarg, NULL, 0);
-			break;
-		case 'p':
-			skel->rodata->switch_partial = true;
-			break;
-		default:
-			fprintf(stderr, help_fmt, basename(argv[0]));
-			return opt != 'h';
-		}
-	}
-
-	bpf_map__set_max_entries(skel->maps.pair_ctx, skel->rodata->nr_cpu_ids / 2);
-
-	/* Resize arrays so their element count is equal to cpu count. */
-	RESIZE_ARRAY(rodata, pair_cpu, skel->rodata->nr_cpu_ids);
-	RESIZE_ARRAY(rodata, pair_id, skel->rodata->nr_cpu_ids);
-	RESIZE_ARRAY(rodata, in_pair_idx, skel->rodata->nr_cpu_ids);
-
-	for (i = 0; i < skel->rodata->nr_cpu_ids; i++)
-		skel->rodata_pair_cpu->pair_cpu[i] = -1;
-
-	printf("Pairs: ");
-	for (i = 0; i < skel->rodata->nr_cpu_ids; i++) {
-		int j = (i + stride) % skel->rodata->nr_cpu_ids;
-
-		if (skel->rodata_pair_cpu->pair_cpu[i] >= 0)
-			continue;
-
-		SCX_BUG_ON(i == j,
-			   "Invalid stride %d - CPU%d wants to be its own pair",
-			   stride, i);
-
-		SCX_BUG_ON(skel->rodata_pair_cpu->pair_cpu[j] >= 0,
-			   "Invalid stride %d - three CPUs (%d, %d, %d) want to be a pair",
-			   stride, i, j, skel->rodata_pair_cpu->pair_cpu[j]);
-
-		skel->rodata_pair_cpu->pair_cpu[i] = j;
-		skel->rodata_pair_cpu->pair_cpu[j] = i;
-		skel->rodata_pair_id->pair_id[i] = i;
-		skel->rodata_pair_id->pair_id[j] = i;
-		skel->rodata_in_pair_idx->in_pair_idx[i] = 0;
-		skel->rodata_in_pair_idx->in_pair_idx[j] = 1;
-
-		printf("[%d, %d] ", i, j);
-	}
-	printf("\n");
-
-	SCX_BUG_ON(scx_pair__load(skel), "Failed to load skel");
-
-	/*
-	 * Populate the cgrp_q_arr map which is an array containing per-cgroup
-	 * queues. It'd probably be better to do this from BPF but there are too
-	 * many to initialize statically and there's no way to dynamically
-	 * populate from BPF.
-	 */
-	outer_fd = bpf_map__fd(skel->maps.cgrp_q_arr);
-	SCX_BUG_ON(outer_fd < 0, "Failed to get outer_fd: %d", outer_fd);
-
-	printf("Initializing");
-        for (i = 0; i < MAX_CGRPS; i++) {
-		__s32 inner_fd;
-
-		if (exit_req)
-			break;
-
-		inner_fd = bpf_map_create(BPF_MAP_TYPE_QUEUE, NULL, 0,
-					  sizeof(__u32), MAX_QUEUED, NULL);
-		SCX_BUG_ON(inner_fd < 0, "Failed to get inner_fd: %d",
-			   inner_fd);
-		SCX_BUG_ON(bpf_map_update_elem(outer_fd, &i, &inner_fd, BPF_ANY),
-			   "Failed to set inner map");
-		close(inner_fd);
-
-		if (!(i % 10))
-			printf(".");
-		fflush(stdout);
-        }
-	printf("\n");
-
-	/*
-	 * Fully initialized, attach and run.
-	 */
-	link = bpf_map__attach_struct_ops(skel->maps.pair_ops);
-	SCX_BUG_ON(!link, "Failed to attach struct_ops");
-
-	while (!exit_req && !uei_exited(&skel->bss->uei)) {
-		printf("[SEQ %llu]\n", seq++);
-		printf(" total:%10" PRIu64 " dispatch:%10" PRIu64 "   missing:%10" PRIu64 "\n",
-		       skel->bss->nr_total,
-		       skel->bss->nr_dispatched,
-		       skel->bss->nr_missing);
-		printf(" kicks:%10" PRIu64 " preemptions:%7" PRIu64 "\n",
-		       skel->bss->nr_kicks,
-		       skel->bss->nr_preemptions);
-		printf("   exp:%10" PRIu64 " exp_wait:%10" PRIu64 " exp_empty:%10" PRIu64 "\n",
-		       skel->bss->nr_exps,
-		       skel->bss->nr_exp_waits,
-		       skel->bss->nr_exp_empty);
-		printf("cgnext:%10" PRIu64 "   cgcoll:%10" PRIu64 "   cgempty:%10" PRIu64 "\n",
-		       skel->bss->nr_cgrp_next,
-		       skel->bss->nr_cgrp_coll,
-		       skel->bss->nr_cgrp_empty);
-		fflush(stdout);
-		sleep(1);
-	}
-
-	bpf_link__destroy(link);
-	uei_print(&skel->bss->uei);
-	scx_pair__destroy(skel);
-	return 0;
-}
diff --git a/tools/sched_ext/scx_pair.h b/tools/sched_ext/scx_pair.h
deleted file mode 100644
index d9666a447..000000000
--- a/tools/sched_ext/scx_pair.h
+++ /dev/null
@@ -1,9 +0,0 @@
-#ifndef __SCX_EXAMPLE_PAIR_H
-#define __SCX_EXAMPLE_PAIR_H
-
-enum {
-	MAX_QUEUED		= 4096,
-	MAX_CGRPS		= 4096,
-};
-
-#endif /* __SCX_EXAMPLE_PAIR_H */
diff --git a/tools/sched_ext/scx_rusty/.gitignore b/tools/sched_ext/scx_rusty/.gitignore
deleted file mode 100644
index 186dba259..000000000
--- a/tools/sched_ext/scx_rusty/.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-src/bpf/.output
-Cargo.lock
-target
diff --git a/tools/sched_ext/scx_rusty/Cargo.toml b/tools/sched_ext/scx_rusty/Cargo.toml
deleted file mode 100644
index a8b4231d1..000000000
--- a/tools/sched_ext/scx_rusty/Cargo.toml
+++ /dev/null
@@ -1,27 +0,0 @@
-[package]
-name = "scx_rusty"
-version = "0.5.3"
-authors = ["Dan Schatzberg <dschatzberg@meta.com>", "Meta"]
-edition = "2021"
-description = "Userspace scheduling with BPF"
-license = "GPL-2.0-only"
-
-[dependencies]
-anyhow = "1.0.65"
-bitvec = { version = "1.0", features = ["serde"] }
-clap = { version = "4.1", features = ["derive", "env", "unicode", "wrap_help"] }
-ctrlc = { version = "3.1", features = ["termination"] }
-fb_procfs = "0.7.0"
-hex = "0.4.3"
-libbpf-rs = "0.22.0"
-libc = "0.2.137"
-log = "0.4.17"
-ordered-float = "3.4.0"
-scx_utils = "0.5"
-simplelog = "0.12.0"
-
-[build-dependencies]
-scx_utils = "0.5"
-
-[features]
-enable_backtrace = []
diff --git a/tools/sched_ext/scx_rusty/README.md b/tools/sched_ext/scx_rusty/README.md
deleted file mode 100644
index 990e51aaf..000000000
--- a/tools/sched_ext/scx_rusty/README.md
+++ /dev/null
@@ -1,36 +0,0 @@
-# scx_rusty
-
-This is a single user-defined scheduler used within [sched_ext](https://github.com/sched-ext/scx/tree/main), which is a Linux kernel feature which enables implementing kernel thread schedulers in BPF and dynamically loading them. [Read more about sched_ext](https://github.com/sched-ext/scx/tree/main).
-
-## Overview
-
-A multi-domain, BPF / user space hybrid scheduler. The BPF portion of the
-scheduler does a simple round robin in each domain, and the user space portion
-(written in Rust) calculates the load factor of each domain, and informs BPF of
-how tasks should be load balanced accordingly.
-
-## How To Install
-
-Available as a [Rust crate](https://crates.io/crates/scx_rusty): `cargo add scx_rusty`
-
-## Typical Use Case
-
-Rusty is designed to be flexible, and accommodate different architectures and
-workloads. Various load balancing thresholds (e.g. greediness, frequenty, etc),
-as well as how Rusty should partition the system into scheduling domains, can
-be tuned to achieve the optimal configuration for any given system or workload.
-
-## Production Ready?
-
-Yes. If tuned correctly, rusty should be performant across various CPU
-architectures and workloads. Rusty by default creates a separate scheduling
-domain per-LLC, so its default configuration may be performant as well. Note
-however that scx_rusty does not yet disambiguate between LLCs in different NUMA
-nodes, so it may perform better on multi-CCX machines where all the LLCs share
-the same socket, as opposed to multi-socket machines.
-
-Note as well that you may run into an issue with infeasible weights, where a
-task with a very high weight may cause the scheduler to incorrectly leave cores
-idle because it thinks they're necessary to accommodate the compute for a
-single task. This can also happen in CFS, and should soon be addressed for
-scx_rusty.
diff --git a/tools/sched_ext/scx_rusty/build.rs b/tools/sched_ext/scx_rusty/build.rs
deleted file mode 100644
index d26db839c..000000000
--- a/tools/sched_ext/scx_rusty/build.rs
+++ /dev/null
@@ -1,13 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-//
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-
-fn main() {
-    scx_utils::BpfBuilder::new()
-        .unwrap()
-        .enable_intf("src/bpf/intf.h", "bpf_intf.rs")
-        .enable_skel("src/bpf/main.bpf.c", "bpf")
-        .build()
-        .unwrap();
-}
diff --git a/tools/sched_ext/scx_rusty/rustfmt.toml b/tools/sched_ext/scx_rusty/rustfmt.toml
deleted file mode 100644
index b7258ed0a..000000000
--- a/tools/sched_ext/scx_rusty/rustfmt.toml
+++ /dev/null
@@ -1,8 +0,0 @@
-# Get help on options with `rustfmt --help=config`
-# Please keep these in alphabetical order.
-edition = "2021"
-group_imports = "StdExternalCrate"
-imports_granularity = "Item"
-merge_derives = false
-use_field_init_shorthand = true
-version = "Two"
diff --git a/tools/sched_ext/scx_rusty/src/bpf/intf.h b/tools/sched_ext/scx_rusty/src/bpf/intf.h
deleted file mode 100644
index f29569510..000000000
--- a/tools/sched_ext/scx_rusty/src/bpf/intf.h
+++ /dev/null
@@ -1,97 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-#ifndef __INTF_H
-#define __INTF_H
-
-#include <stdbool.h>
-#ifndef __kptr
-#ifdef __KERNEL__
-#error "__kptr_ref not defined in the kernel"
-#endif
-#define __kptr
-#endif
-
-#ifndef __KERNEL__
-typedef unsigned char u8;
-typedef unsigned int u32;
-typedef unsigned long long u64;
-#endif
-
-#include <scx/ravg.bpf.h>
-
-enum consts {
-	MAX_CPUS		= 512,
-	MAX_DOMS		= 64,	/* limited to avoid complex bitmask ops */
-	CACHELINE_SIZE		= 64,
-
-	/*
-	 * When userspace load balancer is trying to determine the tasks to push
-	 * out from an overloaded domain, it looks at the the following number
-	 * of recently active tasks of the domain. While this may lead to
-	 * spurious migration victim selection failures in pathological cases,
-	 * this isn't a practical problem as the LB rounds are best-effort
-	 * anyway and will be retried until loads are balanced.
-	 */
-	MAX_DOM_ACTIVE_PIDS	= 1024,
-};
-
-/* Statistics */
-enum stat_idx {
-	/* The following fields add up to all dispatched tasks */
-	RUSTY_STAT_WAKE_SYNC,
-	RUSTY_STAT_PREV_IDLE,
-	RUSTY_STAT_GREEDY_IDLE,
-	RUSTY_STAT_PINNED,
-	RUSTY_STAT_DIRECT_DISPATCH,
-	RUSTY_STAT_DIRECT_GREEDY,
-	RUSTY_STAT_DIRECT_GREEDY_FAR,
-	RUSTY_STAT_DSQ_DISPATCH,
-	RUSTY_STAT_GREEDY,
-
-	/* Extra stats that don't contribute to total */
-	RUSTY_STAT_REPATRIATE,
-	RUSTY_STAT_KICK_GREEDY,
-	RUSTY_STAT_LOAD_BALANCE,
-
-	/* Errors */
-	RUSTY_STAT_TASK_GET_ERR,
-
-	RUSTY_NR_STATS,
-};
-
-struct task_ctx {
-	/* The domains this task can run on */
-	u64 dom_mask;
-
-	struct bpf_cpumask __kptr *cpumask;
-	u32 dom_id;
-	u32 weight;
-	bool runnable;
-	u64 dom_active_pids_gen;
-	u64 running_at;
-
-	/* The task is a workqueue worker thread */
-	bool is_kworker;
-
-	/* Allowed on all CPUs and eligible for DIRECT_GREEDY optimization */
-	bool all_cpus;
-
-	/* select_cpu() telling enqueue() to queue directly on the DSQ */
-	bool dispatch_local;
-
-	struct ravg_data dcyc_rd;
-};
-
-struct dom_ctx {
-	u64 vtime_now;
-	struct bpf_cpumask __kptr *cpumask;
-	struct bpf_cpumask __kptr *direct_greedy_cpumask;
-
-	u64 load;
-	struct ravg_data load_rd;
-	u64 dbg_load_printed_at;
-};
-
-#endif /* __INTF_H */
diff --git a/tools/sched_ext/scx_rusty/src/bpf/main.bpf.c b/tools/sched_ext/scx_rusty/src/bpf/main.bpf.c
deleted file mode 100644
index fe4de979f..000000000
--- a/tools/sched_ext/scx_rusty/src/bpf/main.bpf.c
+++ /dev/null
@@ -1,1164 +0,0 @@
-/* Copyright (c) Meta Platforms, Inc. and affiliates. */
-/*
- * This software may be used and distributed according to the terms of the
- * GNU General Public License version 2.
- *
- * scx_rusty is a multi-domain BPF / userspace hybrid scheduler where the BPF
- * part does simple round robin in each domain and the userspace part
- * calculates the load factor of each domain and tells the BPF part how to load
- * balance the domains.
- *
- * Every task has an entry in the task_data map which lists which domain the
- * task belongs to. When a task first enters the system (rusty_prep_enable),
- * they are round-robined to a domain.
- *
- * rusty_select_cpu is the primary scheduling logic, invoked when a task
- * becomes runnable. The lb_data map is populated by userspace to inform the BPF
- * scheduler that a task should be migrated to a new domain. Otherwise, the task
- * is scheduled in priority order as follows:
- * * The current core if the task was woken up synchronously and there are idle
- *   cpus in the system
- * * The previous core, if idle
- * * The pinned-to core if the task is pinned to a specific core
- * * Any idle cpu in the domain
- *
- * If none of the above conditions are met, then the task is enqueued to a
- * dispatch queue corresponding to the domain (rusty_enqueue).
- *
- * rusty_dispatch will attempt to consume a task from its domain's
- * corresponding dispatch queue (this occurs after scheduling any tasks directly
- * assigned to it due to the logic in rusty_select_cpu). If no task is found,
- * then greedy load stealing will attempt to find a task on another dispatch
- * queue to run.
- *
- * Load balancing is almost entirely handled by userspace. BPF populates the
- * task weight, dom mask and current dom in the task_data map and executes the
- * load balance based on userspace populating the lb_data map.
- */
-#include <scx/common.bpf.h>
-#include <scx/ravg_impl.bpf.h>
-#include "intf.h"
-
-#include <errno.h>
-#include <stdbool.h>
-#include <string.h>
-#include <bpf/bpf_core_read.h>
-#include <bpf/bpf_helpers.h>
-#include <bpf/bpf_tracing.h>
-
-char _license[] SEC("license") = "GPL";
-
-/*
- * const volatiles are set during initialization and treated as consts by the
- * jit compiler.
- */
-
-/*
- * Domains and cpus
- */
-const volatile u32 nr_doms = 32;	/* !0 for veristat, set during init */
-const volatile u32 nr_cpus = 64;	/* !0 for veristat, set during init */
-const volatile u32 cpu_dom_id_map[MAX_CPUS];
-const volatile u64 dom_cpumasks[MAX_DOMS][MAX_CPUS / 64];
-const volatile u32 load_half_life = 1000000000	/* 1s */;
-
-const volatile bool kthreads_local;
-const volatile bool fifo_sched;
-const volatile bool switch_partial;
-const volatile u32 greedy_threshold;
-const volatile u32 debug;
-
-/* base slice duration */
-const volatile u64 slice_ns = SCX_SLICE_DFL;
-
-/*
- * Exit info
- */
-int exit_kind = SCX_EXIT_NONE;
-char exit_msg[SCX_EXIT_MSG_LEN];
-
-/*
- * Per-CPU context
- */
-struct pcpu_ctx {
-	u32 dom_rr_cur; /* used when scanning other doms */
-
-	/* libbpf-rs does not respect the alignment, so pad out the struct explicitly */
-	u8 _padding[CACHELINE_SIZE - sizeof(u32)];
-} __attribute__((aligned(CACHELINE_SIZE)));
-
-struct pcpu_ctx pcpu_ctx[MAX_CPUS];
-
-/*
- * Domain context
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__type(key, u32);
-	__type(value, struct dom_ctx);
-	__uint(max_entries, MAX_DOMS);
-	__uint(map_flags, 0);
-} dom_data SEC(".maps");
-
-struct lock_wrapper {
-	struct bpf_spin_lock lock;
-};
-
-struct {
-	__uint(type, BPF_MAP_TYPE_ARRAY);
-	__type(key, u32);
-	__type(value, struct lock_wrapper);
-	__uint(max_entries, MAX_DOMS);
-	__uint(map_flags, 0);
-} dom_load_locks SEC(".maps");
-
-struct dom_active_pids {
-	u64 gen;
-	u64 read_idx;
-	u64 write_idx;
-	s32 pids[MAX_DOM_ACTIVE_PIDS];
-};
-
-struct dom_active_pids dom_active_pids[MAX_DOMS];
-
-const u64 ravg_1 = 1 << RAVG_FRAC_BITS;
-
-static void dom_load_adj(u32 dom_id, s64 adj, u64 now)
-{
-	struct dom_ctx *domc;
-	struct lock_wrapper *lockw;
-
-	domc = bpf_map_lookup_elem(&dom_data, &dom_id);
-	lockw = bpf_map_lookup_elem(&dom_load_locks, &dom_id);
-
-	if (!domc || !lockw) {
-		scx_bpf_error("dom_ctx / lock lookup failed");
-		return;
-	}
-
-	bpf_spin_lock(&lockw->lock);
-	domc->load += adj;
-	ravg_accumulate(&domc->load_rd, domc->load, now, load_half_life);
-	bpf_spin_unlock(&lockw->lock);
-
-	if (adj < 0 && (s64)domc->load < 0)
-		scx_bpf_error("cpu%d dom%u load underflow (load=%lld adj=%lld)",
-			      bpf_get_smp_processor_id(), dom_id, domc->load, adj);
-
-	if (debug >=2 &&
-	    (!domc->dbg_load_printed_at || now - domc->dbg_load_printed_at >= 1000000000)) {
-		bpf_printk("LOAD ADJ dom=%u adj=%lld load=%llu",
-			   dom_id,
-			   adj,
-			   ravg_read(&domc->load_rd, now, load_half_life) >> RAVG_FRAC_BITS);
-		domc->dbg_load_printed_at = now;
-	}
-}
-
-static void dom_load_xfer_task(struct task_struct *p, struct task_ctx *taskc,
-			       u32 from_dom_id, u32 to_dom_id, u64 now)
-{
-	struct dom_ctx *from_domc, *to_domc;
-	struct lock_wrapper *from_lockw, *to_lockw;
-	struct ravg_data task_load_rd;
-	u64 from_load[2], to_load[2], task_load;
-
-	from_domc = bpf_map_lookup_elem(&dom_data, &from_dom_id);
-	from_lockw = bpf_map_lookup_elem(&dom_load_locks, &from_dom_id);
-	to_domc = bpf_map_lookup_elem(&dom_data, &to_dom_id);
-	to_lockw = bpf_map_lookup_elem(&dom_load_locks, &to_dom_id);
-	if (!from_domc || !from_lockw || !to_domc || !to_lockw) {
-		scx_bpf_error("dom_ctx / lock lookup failed");
-		return;
-	}
-
-	/*
-	 * @p is moving from @from_dom_id to @to_dom_id. Its load contribution
-	 * should be moved together. We only track duty cycle for tasks. Scale
-	 * it by weight to get load_rd.
-	 */
-	ravg_accumulate(&taskc->dcyc_rd, taskc->runnable, now, load_half_life);
-	task_load_rd = taskc->dcyc_rd;
-	ravg_scale(&task_load_rd, p->scx.weight, 0);
-
-	if (debug >= 2)
-		task_load = ravg_read(&task_load_rd, now, load_half_life);
-
-	/* transfer out of @from_dom_id */
-	bpf_spin_lock(&from_lockw->lock);
-	if (taskc->runnable)
-		from_domc->load -= p->scx.weight;
-
-	if (debug >= 2)
-		from_load[0] = ravg_read(&from_domc->load_rd, now, load_half_life);
-
-	ravg_transfer(&from_domc->load_rd, from_domc->load,
-		      &task_load_rd, taskc->runnable, load_half_life, false);
-
-	if (debug >= 2)
-		from_load[1] = ravg_read(&from_domc->load_rd, now, load_half_life);
-
-	bpf_spin_unlock(&from_lockw->lock);
-
-	/* transfer into @to_dom_id */
-	bpf_spin_lock(&to_lockw->lock);
-	if (taskc->runnable)
-		to_domc->load += p->scx.weight;
-
-	if (debug >= 2)
-		to_load[0] = ravg_read(&to_domc->load_rd, now, load_half_life);
-
-	ravg_transfer(&to_domc->load_rd, to_domc->load,
-		      &task_load_rd, taskc->runnable, load_half_life, true);
-
-	if (debug >= 2)
-		to_load[1] = ravg_read(&to_domc->load_rd, now, load_half_life);
-
-	bpf_spin_unlock(&to_lockw->lock);
-
-	if (debug >= 2)
-		bpf_printk("XFER dom%u->%u task=%lu from=%lu->%lu to=%lu->%lu",
-			   from_dom_id, to_dom_id,
-			   task_load >> RAVG_FRAC_BITS,
-			   from_load[0] >> RAVG_FRAC_BITS,
-			   from_load[1] >> RAVG_FRAC_BITS,
-			   to_load[0] >> RAVG_FRAC_BITS,
-			   to_load[1] >> RAVG_FRAC_BITS);
-}
-
-/*
- * Statistics
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
-	__uint(key_size, sizeof(u32));
-	__uint(value_size, sizeof(u64));
-	__uint(max_entries, RUSTY_NR_STATS);
-} stats SEC(".maps");
-
-static inline void stat_add(enum stat_idx idx, u64 addend)
-{
-	u32 idx_v = idx;
-
-	u64 *cnt_p = bpf_map_lookup_elem(&stats, &idx_v);
-	if (cnt_p)
-		(*cnt_p) += addend;
-}
-
-/* Map pid -> task_ctx */
-struct {
-	__uint(type, BPF_MAP_TYPE_HASH);
-	__type(key, pid_t);
-	__type(value, struct task_ctx);
-	__uint(max_entries, 1000000);
-	__uint(map_flags, 0);
-} task_data SEC(".maps");
-
-struct task_ctx *lookup_task_ctx(struct task_struct *p)
-{
-	struct task_ctx *taskc;
-	s32 pid = p->pid;
-
-	if ((taskc = bpf_map_lookup_elem(&task_data, &pid))) {
-		return taskc;
-	} else {
-		scx_bpf_error("task_ctx lookup failed for pid %d", p->pid);
-		return NULL;
-	}
-}
-
-/*
- * This is populated from userspace to indicate which pids should be reassigned
- * to new doms.
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_HASH);
-	__type(key, pid_t);
-	__type(value, u32);
-	__uint(max_entries, 1000);
-	__uint(map_flags, 0);
-} lb_data SEC(".maps");
-
-/*
- * Userspace tuner will frequently update the following struct with tuning
- * parameters and bump its gen. refresh_tune_params() converts them into forms
- * that can be used directly in the scheduling paths.
- */
-struct tune_input{
-	u64 gen;
-	u64 direct_greedy_cpumask[MAX_CPUS / 64];
-	u64 kick_greedy_cpumask[MAX_CPUS / 64];
-} tune_input;
-
-u64 tune_params_gen;
-private(A) struct bpf_cpumask __kptr *all_cpumask;
-private(A) struct bpf_cpumask __kptr *direct_greedy_cpumask;
-private(A) struct bpf_cpumask __kptr *kick_greedy_cpumask;
-
-static inline bool vtime_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
-static u32 cpu_to_dom_id(s32 cpu)
-{
-	const volatile u32 *dom_idp;
-
-	if (nr_doms <= 1)
-		return 0;
-
-	dom_idp = MEMBER_VPTR(cpu_dom_id_map, [cpu]);
-	if (!dom_idp)
-		return MAX_DOMS;
-
-	return *dom_idp;
-}
-
-static void refresh_tune_params(void)
-{
-	s32 cpu;
-
-	if (tune_params_gen == tune_input.gen)
-		return;
-
-	tune_params_gen = tune_input.gen;
-
-	bpf_for(cpu, 0, nr_cpus) {
-		u32 dom_id = cpu_to_dom_id(cpu);
-		struct dom_ctx *domc;
-
-		if (!(domc = bpf_map_lookup_elem(&dom_data, &dom_id))) {
-			scx_bpf_error("Failed to lookup dom[%u]", dom_id);
-			return;
-		}
-
-		if (tune_input.direct_greedy_cpumask[cpu / 64] & (1LLU << (cpu % 64))) {
-			if (direct_greedy_cpumask)
-				bpf_cpumask_set_cpu(cpu, direct_greedy_cpumask);
-			if (domc->direct_greedy_cpumask)
-				bpf_cpumask_set_cpu(cpu, domc->direct_greedy_cpumask);
-		} else {
-			if (direct_greedy_cpumask)
-				bpf_cpumask_clear_cpu(cpu, direct_greedy_cpumask);
-			if (domc->direct_greedy_cpumask)
-				bpf_cpumask_clear_cpu(cpu, domc->direct_greedy_cpumask);
-		}
-
-		if (tune_input.kick_greedy_cpumask[cpu / 64] & (1LLU << (cpu % 64))) {
-			if (kick_greedy_cpumask)
-				bpf_cpumask_set_cpu(cpu, kick_greedy_cpumask);
-		} else {
-			if (kick_greedy_cpumask)
-				bpf_cpumask_clear_cpu(cpu, kick_greedy_cpumask);
-		}
-	}
-}
-
-static bool task_set_domain(struct task_ctx *taskc, struct task_struct *p,
-			    u32 new_dom_id, bool init_dsq_vtime)
-{
-	struct dom_ctx *old_domc, *new_domc;
-	struct bpf_cpumask *d_cpumask, *t_cpumask;
-	u32 old_dom_id = taskc->dom_id;
-	s64 vtime_delta;
-
-	old_domc = bpf_map_lookup_elem(&dom_data, &old_dom_id);
-	if (!old_domc) {
-		scx_bpf_error("Failed to lookup old dom%u", old_dom_id);
-		return false;
-	}
-
-	if (init_dsq_vtime)
-		vtime_delta = 0;
-	else
-		vtime_delta = p->scx.dsq_vtime - old_domc->vtime_now;
-
-	new_domc = bpf_map_lookup_elem(&dom_data, &new_dom_id);
-	if (!new_domc) {
-		scx_bpf_error("Failed to lookup new dom%u", new_dom_id);
-		return false;
-	}
-
-	d_cpumask = new_domc->cpumask;
-	if (!d_cpumask) {
-		scx_bpf_error("Failed to get dom%u cpumask kptr",
-			      new_dom_id);
-		return false;
-	}
-
-	t_cpumask = taskc->cpumask;
-	if (!t_cpumask) {
-		scx_bpf_error("Failed to look up task cpumask");
-		return false;
-	}
-
-	/*
-	 * set_cpumask might have happened between userspace requesting LB and
-	 * here and @p might not be able to run in @dom_id anymore. Verify.
-	 */
-	if (bpf_cpumask_intersects((const struct cpumask *)d_cpumask,
-				   p->cpus_ptr)) {
-		u64 now = bpf_ktime_get_ns();
-
-		dom_load_xfer_task(p, taskc, taskc->dom_id, new_dom_id, now);
-
-		p->scx.dsq_vtime = new_domc->vtime_now + vtime_delta;
-		taskc->dom_id = new_dom_id;
-		bpf_cpumask_and(t_cpumask, (const struct cpumask *)d_cpumask,
-				p->cpus_ptr);
-	}
-
-	return taskc->dom_id == new_dom_id;
-}
-
-s32 BPF_STRUCT_OPS(rusty_select_cpu, struct task_struct *p, s32 prev_cpu,
-		   u64 wake_flags)
-{
-	const struct cpumask *idle_smtmask = scx_bpf_get_idle_smtmask();
-	struct task_ctx *taskc;
-	struct bpf_cpumask *p_cpumask;
-	bool prev_domestic, has_idle_cores;
-	s32 cpu;
-
-	refresh_tune_params();
-
-	if (!(taskc = lookup_task_ctx(p)) || !(p_cpumask = taskc->cpumask))
-		goto enoent;
-
-	if (p->nr_cpus_allowed == 1) {
-		cpu = prev_cpu;
-		if (kthreads_local && (p->flags & PF_KTHREAD)) {
-			stat_add(RUSTY_STAT_DIRECT_DISPATCH, 1);
-		} else {
-			stat_add(RUSTY_STAT_PINNED, 1);
-		}
-		goto direct;
-	}
-
-	/*
-	 * If WAKE_SYNC and the machine isn't fully saturated, wake up @p to the
-	 * local dsq of the waker.
-	 */
-	if (wake_flags & SCX_WAKE_SYNC) {
-		struct task_struct *current = (void *)bpf_get_current_task();
-
-		if (!(BPF_CORE_READ(current, flags) & PF_EXITING) &&
-		    taskc->dom_id < MAX_DOMS) {
-			struct dom_ctx *domc;
-			struct bpf_cpumask *d_cpumask;
-			const struct cpumask *idle_cpumask;
-			bool has_idle;
-
-			domc = bpf_map_lookup_elem(&dom_data, &taskc->dom_id);
-			if (!domc) {
-				scx_bpf_error("Failed to find dom%u", taskc->dom_id);
-				goto enoent;
-			}
-			d_cpumask = domc->cpumask;
-			if (!d_cpumask) {
-				scx_bpf_error("Failed to acquire dom%u cpumask kptr",
-					      taskc->dom_id);
-				goto enoent;
-			}
-
-			idle_cpumask = scx_bpf_get_idle_cpumask();
-
-			has_idle = bpf_cpumask_intersects((const struct cpumask *)d_cpumask,
-							  idle_cpumask);
-
-			scx_bpf_put_idle_cpumask(idle_cpumask);
-
-			if (has_idle) {
-				cpu = bpf_get_smp_processor_id();
-				if (bpf_cpumask_test_cpu(cpu, p->cpus_ptr)) {
-					stat_add(RUSTY_STAT_WAKE_SYNC, 1);
-					goto direct;
-				}
-			}
-		}
-	}
-
-	has_idle_cores = !bpf_cpumask_empty(idle_smtmask);
-
-	/* did @p get pulled out to a foreign domain by e.g. greedy execution? */
-	prev_domestic = bpf_cpumask_test_cpu(prev_cpu,
-					     (const struct cpumask *)p_cpumask);
-
-	/*
-	 * See if we want to keep @prev_cpu. We want to keep @prev_cpu if the
-	 * whole physical core is idle. If the sibling[s] are busy, it's likely
-	 * more advantageous to look for wholly idle cores first.
-	 */
-	if (prev_domestic) {
-		if (bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
-		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
-			stat_add(RUSTY_STAT_PREV_IDLE, 1);
-			cpu = prev_cpu;
-			goto direct;
-		}
-	} else {
-		/*
-		 * @prev_cpu is foreign. Linger iff the domain isn't too busy as
-		 * indicated by direct_greedy_cpumask. There may also be an idle
-		 * CPU in the domestic domain
-		 */
-		if (direct_greedy_cpumask &&
-		    bpf_cpumask_test_cpu(prev_cpu, (const struct cpumask *)
-					 direct_greedy_cpumask) &&
-		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
-		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
-			stat_add(RUSTY_STAT_GREEDY_IDLE, 1);
-			cpu = prev_cpu;
-			goto direct;
-		}
-	}
-
-	/*
-	 * @prev_cpu didn't work out. Let's see whether there's an idle CPU @p
-	 * can be directly dispatched to. We'll first try to find the best idle
-	 * domestic CPU and then move onto foreign.
-	 */
-
-	/* If there is a domestic idle core, dispatch directly */
-	if (has_idle_cores) {
-		cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)p_cpumask,
-					    SCX_PICK_IDLE_CORE);
-		if (cpu >= 0) {
-			stat_add(RUSTY_STAT_DIRECT_DISPATCH, 1);
-			goto direct;
-		}
-	}
-
-	/*
-	 * If @prev_cpu was domestic and is idle itself even though the core
-	 * isn't, picking @prev_cpu may improve L1/2 locality.
-	 */
-	if (prev_domestic && scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
-		stat_add(RUSTY_STAT_DIRECT_DISPATCH, 1);
-		cpu = prev_cpu;
-		goto direct;
-	}
-
-	/* If there is any domestic idle CPU, dispatch directly */
-	cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)p_cpumask, 0);
-	if (cpu >= 0) {
-		stat_add(RUSTY_STAT_DIRECT_DISPATCH, 1);
-		goto direct;
-	}
-
-	/*
-	 * Domestic domain is fully booked. If there are CPUs which are idle and
-	 * under-utilized, ignore domain boundaries and push the task there. Try
-	 * to find an idle core first.
-	 */
-	if (taskc->all_cpus && direct_greedy_cpumask &&
-	    !bpf_cpumask_empty((const struct cpumask *)direct_greedy_cpumask)) {
-		u32 dom_id = cpu_to_dom_id(prev_cpu);
-		struct dom_ctx *domc;
-
-		if (!(domc = bpf_map_lookup_elem(&dom_data, &dom_id))) {
-			scx_bpf_error("Failed to lookup dom[%u]", dom_id);
-			goto enoent;
-		}
-
-		/* Try to find an idle core in the previous and then any domain */
-		if (has_idle_cores) {
-			if (domc->direct_greedy_cpumask) {
-				cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)
-							    domc->direct_greedy_cpumask,
-							    SCX_PICK_IDLE_CORE);
-				if (cpu >= 0) {
-					stat_add(RUSTY_STAT_DIRECT_GREEDY, 1);
-					goto direct;
-				}
-			}
-
-			if (direct_greedy_cpumask) {
-				cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)
-							    direct_greedy_cpumask,
-							    SCX_PICK_IDLE_CORE);
-				if (cpu >= 0) {
-					stat_add(RUSTY_STAT_DIRECT_GREEDY_FAR, 1);
-					goto direct;
-				}
-			}
-		}
-
-		/*
-		 * No idle core. Is there any idle CPU?
-		 */
-		if (domc->direct_greedy_cpumask) {
-			cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)
-						    domc->direct_greedy_cpumask, 0);
-			if (cpu >= 0) {
-				stat_add(RUSTY_STAT_DIRECT_GREEDY, 1);
-				goto direct;
-			}
-		}
-
-		if (direct_greedy_cpumask) {
-			cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)
-						    direct_greedy_cpumask, 0);
-			if (cpu >= 0) {
-				stat_add(RUSTY_STAT_DIRECT_GREEDY_FAR, 1);
-				goto direct;
-			}
-		}
-	}
-
-	/*
-	 * We're going to queue on the domestic domain's DSQ. @prev_cpu may be
-	 * in a different domain. Returning an out-of-domain CPU can lead to
-	 * stalls as all in-domain CPUs may be idle by the time @p gets
-	 * enqueued.
-	 */
-	if (prev_domestic)
-		cpu = prev_cpu;
-	else
-		cpu = scx_bpf_pick_any_cpu((const struct cpumask *)p_cpumask, 0);
-
-	scx_bpf_put_idle_cpumask(idle_smtmask);
-	return cpu;
-
-direct:
-	taskc->dispatch_local = true;
-	scx_bpf_put_idle_cpumask(idle_smtmask);
-	return cpu;
-
-enoent:
-	scx_bpf_put_idle_cpumask(idle_smtmask);
-	return -ENOENT;
-}
-
-void BPF_STRUCT_OPS(rusty_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	struct task_ctx *taskc;
-	struct bpf_cpumask *p_cpumask;
-	pid_t pid = p->pid;
-	u32 *new_dom;
-	s32 cpu;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-	if (!(p_cpumask = taskc->cpumask)) {
-		scx_bpf_error("NULL cpmask");
-		return;
-	}
-
-	/*
-	 * Migrate @p to a new domain if requested by userland through lb_data.
-	 */
-	new_dom = bpf_map_lookup_elem(&lb_data, &pid);
-	if (new_dom && *new_dom != taskc->dom_id &&
-	    task_set_domain(taskc, p, *new_dom, false)) {
-		stat_add(RUSTY_STAT_LOAD_BALANCE, 1);
-		taskc->dispatch_local = false;
-		cpu = scx_bpf_pick_any_cpu((const struct cpumask *)p_cpumask, 0);
-		if (cpu >= 0)
-			scx_bpf_kick_cpu(cpu, 0);
-		goto dom_queue;
-	}
-
-	if (taskc->dispatch_local) {
-		taskc->dispatch_local = false;
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, enq_flags);
-		return;
-	}
-
-	/*
-	 * @p is about to be queued on its domain's dsq. However, @p may be on a
-	 * foreign CPU due to a greedy execution and not have gone through
-	 * ->select_cpu() if it's being enqueued e.g. after slice exhaustion. If
-	 * so, @p would be queued on its domain's dsq but none of the CPUs in
-	 * the domain would be woken up which can induce temporary execution
-	 * stalls. Kick a domestic CPU if @p is on a foreign domain.
-	 */
-	if (!bpf_cpumask_test_cpu(scx_bpf_task_cpu(p), (const struct cpumask *)p_cpumask)) {
-		cpu = scx_bpf_pick_any_cpu((const struct cpumask *)p_cpumask, 0);
-		scx_bpf_kick_cpu(cpu, 0);
-		stat_add(RUSTY_STAT_REPATRIATE, 1);
-	}
-
-dom_queue:
-	if (fifo_sched) {
-		scx_bpf_dispatch(p, taskc->dom_id, slice_ns, enq_flags);
-	} else {
-		u64 vtime = p->scx.dsq_vtime;
-		u32 dom_id = taskc->dom_id;
-		struct dom_ctx *domc;
-
-		domc = bpf_map_lookup_elem(&dom_data, &dom_id);
-		if (!domc) {
-			scx_bpf_error("Failed to lookup dom[%u]", dom_id);
-			return;
-		}
-
-		/*
-		 * Limit the amount of budget that an idling task can accumulate
-		 * to one slice.
-		 */
-		if (vtime_before(vtime, domc->vtime_now - slice_ns))
-			vtime = domc->vtime_now - slice_ns;
-
-		scx_bpf_dispatch_vtime(p, taskc->dom_id, slice_ns, vtime, enq_flags);
-	}
-
-	/*
-	 * If there are CPUs which are idle and not saturated, wake them up to
-	 * see whether they'd be able to steal the just queued task. This path
-	 * is taken only if DIRECT_GREEDY didn't trigger in select_cpu().
-	 *
-	 * While both mechanisms serve very similar purposes, DIRECT_GREEDY
-	 * emplaces the task in a foreign CPU directly while KICK_GREEDY just
-	 * wakes up a foreign CPU which will then first try to execute from its
-	 * domestic domain first before snooping foreign ones.
-	 *
-	 * While KICK_GREEDY is a more expensive way of accelerating greedy
-	 * execution, DIRECT_GREEDY shows negative performance impacts when the
-	 * CPUs are highly loaded while KICK_GREEDY doesn't. Even under fairly
-	 * high utilization, KICK_GREEDY can slightly improve work-conservation.
-	 */
-	if (taskc->all_cpus && kick_greedy_cpumask) {
-		cpu = scx_bpf_pick_idle_cpu((const struct cpumask *)
-					    kick_greedy_cpumask, 0);
-		if (cpu >= 0) {
-			stat_add(RUSTY_STAT_KICK_GREEDY, 1);
-			scx_bpf_kick_cpu(cpu, 0);
-		}
-	}
-}
-
-static bool cpumask_intersects_domain(const struct cpumask *cpumask, u32 dom_id)
-{
-	s32 cpu;
-
-	if (dom_id >= MAX_DOMS)
-		return false;
-
-	bpf_for(cpu, 0, nr_cpus) {
-		if (bpf_cpumask_test_cpu(cpu, cpumask) &&
-		    (dom_cpumasks[dom_id][cpu / 64] & (1LLU << (cpu % 64))))
-			return true;
-	}
-	return false;
-}
-
-static u32 dom_rr_next(s32 cpu)
-{
-	struct pcpu_ctx *pcpuc;
-	u32 dom_id;
-
-	pcpuc = MEMBER_VPTR(pcpu_ctx, [cpu]);
-	if (!pcpuc)
-		return 0;
-
-	dom_id = (pcpuc->dom_rr_cur + 1) % nr_doms;
-
-	if (dom_id == cpu_to_dom_id(cpu))
-		dom_id = (dom_id + 1) % nr_doms;
-
-	pcpuc->dom_rr_cur = dom_id;
-	return dom_id;
-}
-
-void BPF_STRUCT_OPS(rusty_dispatch, s32 cpu, struct task_struct *prev)
-{
-	u32 dom = cpu_to_dom_id(cpu);
-
-	if (scx_bpf_consume(dom)) {
-		stat_add(RUSTY_STAT_DSQ_DISPATCH, 1);
-		return;
-	}
-
-	if (!greedy_threshold)
-		return;
-
-	bpf_repeat(nr_doms - 1) {
-		u32 dom_id = dom_rr_next(cpu);
-
-		if (scx_bpf_dsq_nr_queued(dom_id) >= greedy_threshold &&
-		    scx_bpf_consume(dom_id)) {
-			stat_add(RUSTY_STAT_GREEDY, 1);
-			break;
-		}
-	}
-}
-
-void BPF_STRUCT_OPS(rusty_runnable, struct task_struct *p, u64 enq_flags)
-{
-	u64 now = bpf_ktime_get_ns();
-	struct task_ctx *taskc;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	taskc->runnable = true;
-	taskc->is_kworker = p->flags & PF_WQ_WORKER;
-
-	ravg_accumulate(&taskc->dcyc_rd, taskc->runnable, now, load_half_life);
-	dom_load_adj(taskc->dom_id, p->scx.weight, now);
-}
-
-void BPF_STRUCT_OPS(rusty_running, struct task_struct *p)
-{
-	struct task_ctx *taskc;
-	struct dom_ctx *domc;
-	u32 dom_id, dap_gen;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	taskc->running_at = bpf_ktime_get_ns();
-	dom_id = taskc->dom_id;
-	if (dom_id >= MAX_DOMS) {
-		scx_bpf_error("Invalid dom ID");
-		return;
-	}
-
-	/*
-	 * Record that @p has been active in @domc. Load balancer will only
-	 * consider recently active tasks. Access synchronization rules aren't
-	 * strict. We just need to be right most of the time.
-	 */
-	dap_gen = dom_active_pids[dom_id].gen;
-	if (taskc->dom_active_pids_gen != dap_gen) {
-		u64 idx = __sync_fetch_and_add(&dom_active_pids[dom_id].write_idx, 1) %
-			MAX_DOM_ACTIVE_PIDS;
-		s32 *pidp;
-
-		pidp = MEMBER_VPTR(dom_active_pids, [dom_id].pids[idx]);
-		if (!pidp) {
-			scx_bpf_error("dom_active_pids[%u][%llu] indexing failed",
-				      dom_id, idx);
-			return;
-		}
-
-		*pidp = p->pid;
-		taskc->dom_active_pids_gen = dap_gen;
-	}
-
-	if (fifo_sched)
-		return;
-
-	domc = bpf_map_lookup_elem(&dom_data, &dom_id);
-	if (!domc) {
-		scx_bpf_error("Failed to lookup dom[%u]", dom_id);
-		return;
-	}
-
-	/*
-	 * Global vtime always progresses forward as tasks start executing. The
-	 * test and update can be performed concurrently from multiple CPUs and
-	 * thus racy. Any error should be contained and temporary. Let's just
-	 * live with it.
-	 */
-	if (vtime_before(domc->vtime_now, p->scx.dsq_vtime))
-		domc->vtime_now = p->scx.dsq_vtime;
-}
-
-void BPF_STRUCT_OPS(rusty_stopping, struct task_struct *p, bool runnable)
-{
-	struct task_ctx *taskc;
-
-	if (fifo_sched)
-		return;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	/* scale the execution time by the inverse of the weight and charge */
-	p->scx.dsq_vtime +=
-		(bpf_ktime_get_ns() - taskc->running_at) * 100 / p->scx.weight;
-}
-
-void BPF_STRUCT_OPS(rusty_quiescent, struct task_struct *p, u64 deq_flags)
-{
-	u64 now = bpf_ktime_get_ns();
-	struct task_ctx *taskc;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	taskc->runnable = false;
-
-	ravg_accumulate(&taskc->dcyc_rd, taskc->runnable, now, load_half_life);
-	dom_load_adj(taskc->dom_id, -(s64)p->scx.weight, now);
-}
-
-void BPF_STRUCT_OPS(rusty_set_weight, struct task_struct *p, u32 weight)
-{
-	struct task_ctx *taskc;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	taskc->weight = weight;
-}
-
-static u32 task_pick_domain(struct task_ctx *taskc, struct task_struct *p,
-			    const struct cpumask *cpumask)
-{
-	s32 cpu = bpf_get_smp_processor_id();
-	u32 first_dom = MAX_DOMS, dom;
-
-	if (cpu < 0 || cpu >= MAX_CPUS)
-		return MAX_DOMS;
-
-	taskc->dom_mask = 0;
-
-	dom = pcpu_ctx[cpu].dom_rr_cur++;
-	bpf_repeat(nr_doms) {
-		dom = (dom + 1) % nr_doms;
-		if (cpumask_intersects_domain(cpumask, dom)) {
-			taskc->dom_mask |= 1LLU << dom;
-			/*
-			 * AsThe starting point is round-robin'd and the first
-			 * match should be spread across all the domains.
-			 */
-			if (first_dom == MAX_DOMS)
-				first_dom = dom;
-		}
-	}
-
-	return first_dom;
-}
-
-static void task_pick_and_set_domain(struct task_ctx *taskc,
-				     struct task_struct *p,
-				     const struct cpumask *cpumask,
-				     bool init_dsq_vtime)
-{
-	u32 dom_id = 0;
-
-	if (nr_doms > 1)
-		dom_id = task_pick_domain(taskc, p, cpumask);
-
-	if (!task_set_domain(taskc, p, dom_id, init_dsq_vtime))
-		scx_bpf_error("Failed to set dom%d for %s[%d]",
-			      dom_id, p->comm, p->pid);
-}
-
-void BPF_STRUCT_OPS(rusty_set_cpumask, struct task_struct *p,
-		    const struct cpumask *cpumask)
-{
-	struct task_ctx *taskc;
-
-	if (!(taskc = lookup_task_ctx(p)))
-		return;
-
-	task_pick_and_set_domain(taskc, p, cpumask, false);
-	if (all_cpumask)
-		taskc->all_cpus =
-			bpf_cpumask_subset((const struct cpumask *)all_cpumask, cpumask);
-}
-
-s32 BPF_STRUCT_OPS(rusty_init_task, struct task_struct *p,
-		   struct scx_init_task_args *args)
-{
-	struct bpf_cpumask *cpumask;
-	struct task_ctx taskc = { .dom_active_pids_gen = -1 };
-	struct task_ctx *map_value;
-	long ret;
-	pid_t pid;
-
-	pid = p->pid;
-
-	/*
-	 * XXX - We want BPF_NOEXIST but bpf_map_delete_elem() in .disable() may
-	 * fail spuriously due to BPF recursion protection triggering
-	 * unnecessarily.
-	 */
-	ret = bpf_map_update_elem(&task_data, &pid, &taskc, 0 /*BPF_NOEXIST*/);
-	if (ret) {
-		stat_add(RUSTY_STAT_TASK_GET_ERR, 1);
-		return ret;
-	}
-
-	/*
-	 * Read the entry from the map immediately so we can add the cpumask
-	 * with bpf_kptr_xchg().
-	 */
-	map_value = bpf_map_lookup_elem(&task_data, &pid);
-	if (!map_value)
-		/* Should never happen -- it was just inserted above. */
-		return -EINVAL;
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask) {
-		bpf_map_delete_elem(&task_data, &pid);
-		return -ENOMEM;
-	}
-
-	cpumask = bpf_kptr_xchg(&map_value->cpumask, cpumask);
-	if (cpumask) {
-		/* Should never happen as we just inserted it above. */
-		bpf_cpumask_release(cpumask);
-		bpf_map_delete_elem(&task_data, &pid);
-		return -EINVAL;
-	}
-
-	task_pick_and_set_domain(map_value, p, p->cpus_ptr, true);
-
-	return 0;
-}
-
-void BPF_STRUCT_OPS(rusty_exit_task, struct task_struct *p,
-		    struct scx_exit_task_args *args)
-{
-	pid_t pid = p->pid;
-	long ret;
-
-	/*
-	 * XXX - There's no reason delete should fail here but BPF's recursion
-	 * protection can unnecessarily fail the operation. The fact that
-	 * deletions aren't reliable means that we sometimes leak task_ctx and
-	 * can't use BPF_NOEXIST on allocation in .prep_enable().
-	 */
-	ret = bpf_map_delete_elem(&task_data, &pid);
-	if (ret) {
-		stat_add(RUSTY_STAT_TASK_GET_ERR, 1);
-		return;
-	}
-}
-
-static s32 create_dom(u32 dom_id)
-{
-	struct dom_ctx domc_init = {}, *domc;
-	struct bpf_cpumask *cpumask;
-	u32 cpu;
-	s32 ret;
-
-	ret = scx_bpf_create_dsq(dom_id, -1);
-	if (ret < 0) {
-		scx_bpf_error("Failed to create dsq %u (%d)", dom_id, ret);
-		return ret;
-	}
-
-	ret = bpf_map_update_elem(&dom_data, &dom_id, &domc_init, 0);
-	if (ret) {
-		scx_bpf_error("Failed to add dom_ctx entry %u (%d)", dom_id, ret);
-		return ret;
-	}
-
-	domc = bpf_map_lookup_elem(&dom_data, &dom_id);
-	if (!domc) {
-		/* Should never happen, we just inserted it above. */
-		scx_bpf_error("No dom%u", dom_id);
-		return -ENOENT;
-	}
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask) {
-		scx_bpf_error("Failed to create BPF cpumask for domain %u", dom_id);
-		return -ENOMEM;
-	}
-
-	for (cpu = 0; cpu < MAX_CPUS; cpu++) {
-		const volatile u64 *dmask;
-
-		dmask = MEMBER_VPTR(dom_cpumasks, [dom_id][cpu / 64]);
-		if (!dmask) {
-			scx_bpf_error("array index error");
-			bpf_cpumask_release(cpumask);
-			return -ENOENT;
-		}
-
-		if (*dmask & (1LLU << (cpu % 64))) {
-			bpf_cpumask_set_cpu(cpu, cpumask);
-
-			bpf_rcu_read_lock();
-			if (all_cpumask)
-				bpf_cpumask_set_cpu(cpu, all_cpumask);
-			bpf_rcu_read_unlock();
-		}
-	}
-
-	cpumask = bpf_kptr_xchg(&domc->cpumask, cpumask);
-	if (cpumask) {
-		scx_bpf_error("Domain %u cpumask already present", dom_id);
-		bpf_cpumask_release(cpumask);
-		return -EEXIST;
-	}
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask) {
-		scx_bpf_error("Failed to create BPF cpumask for domain %u",
-			      dom_id);
-		return -ENOMEM;
-	}
-
-	cpumask = bpf_kptr_xchg(&domc->direct_greedy_cpumask, cpumask);
-	if (cpumask) {
-		scx_bpf_error("Domain %u direct_greedy_cpumask already present",
-			      dom_id);
-		bpf_cpumask_release(cpumask);
-		return -EEXIST;
-	}
-
-	return 0;
-}
-
-s32 BPF_STRUCT_OPS_SLEEPABLE(rusty_init)
-{
-	struct bpf_cpumask *cpumask;
-	s32 i, ret;
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask)
-		return -ENOMEM;
-	cpumask = bpf_kptr_xchg(&all_cpumask, cpumask);
-	if (cpumask)
-		bpf_cpumask_release(cpumask);
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask)
-		return -ENOMEM;
-	cpumask = bpf_kptr_xchg(&direct_greedy_cpumask, cpumask);
-	if (cpumask)
-		bpf_cpumask_release(cpumask);
-
-	cpumask = bpf_cpumask_create();
-	if (!cpumask)
-		return -ENOMEM;
-	cpumask = bpf_kptr_xchg(&kick_greedy_cpumask, cpumask);
-	if (cpumask)
-		bpf_cpumask_release(cpumask);
-
-	if (!switch_partial)
-		scx_bpf_switch_all();
-
-	bpf_for(i, 0, nr_doms) {
-		ret = create_dom(i);
-		if (ret)
-			return ret;
-	}
-
-	bpf_for(i, 0, nr_cpus)
-		pcpu_ctx[i].dom_rr_cur = i;
-
-	return 0;
-}
-
-void BPF_STRUCT_OPS(rusty_exit, struct scx_exit_info *ei)
-{
-	bpf_probe_read_kernel_str(exit_msg, sizeof(exit_msg), ei->msg);
-	exit_kind = ei->kind;
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops rusty = {
-	.select_cpu		= (void *)rusty_select_cpu,
-	.enqueue		= (void *)rusty_enqueue,
-	.dispatch		= (void *)rusty_dispatch,
-	.runnable		= (void *)rusty_runnable,
-	.running		= (void *)rusty_running,
-	.stopping		= (void *)rusty_stopping,
-	.quiescent		= (void *)rusty_quiescent,
-	.set_weight		= (void *)rusty_set_weight,
-	.set_cpumask		= (void *)rusty_set_cpumask,
-	.init_task		= (void *)rusty_init_task,
-	.exit_task		= (void *)rusty_exit_task,
-	.init			= (void *)rusty_init,
-	.exit			= (void *)rusty_exit,
-	.name			= "rusty",
-};
diff --git a/tools/sched_ext/scx_rusty/src/bpf_intf.rs b/tools/sched_ext/scx_rusty/src/bpf_intf.rs
deleted file mode 100644
index 0ed31f8e0..000000000
--- a/tools/sched_ext/scx_rusty/src/bpf_intf.rs
+++ /dev/null
@@ -1,10 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-#![allow(non_upper_case_globals)]
-#![allow(non_camel_case_types)]
-#![allow(non_snake_case)]
-#![allow(dead_code)]
-
-include!(concat!(env!("OUT_DIR"), "/bpf_intf.rs"));
diff --git a/tools/sched_ext/scx_rusty/src/bpf_skel.rs b/tools/sched_ext/scx_rusty/src/bpf_skel.rs
deleted file mode 100644
index 063ccf896..000000000
--- a/tools/sched_ext/scx_rusty/src/bpf_skel.rs
+++ /dev/null
@@ -1,12 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-
-// We can't directly include the generated skeleton in main.rs as it may
-// contain compiler attributes that can't be `include!()`ed via macro and we
-// can't use the `#[path = "..."]` because `concat!(env!("OUT_DIR"),
-// "/bpf.skel.rs")` does not work inside the path attribute yet (see
-// https://github.com/rust-lang/rust/pull/83366).
-
-include!(concat!(env!("OUT_DIR"), "/bpf_skel.rs"));
diff --git a/tools/sched_ext/scx_rusty/src/main.rs b/tools/sched_ext/scx_rusty/src/main.rs
deleted file mode 100644
index 3192ee049..000000000
--- a/tools/sched_ext/scx_rusty/src/main.rs
+++ /dev/null
@@ -1,1271 +0,0 @@
-// Copyright (c) Meta Platforms, Inc. and affiliates.
-
-// This software may be used and distributed according to the terms of the
-// GNU General Public License version 2.
-mod bpf_skel;
-pub use bpf_skel::*;
-pub mod bpf_intf;
-
-use std::cell::Cell;
-use std::collections::BTreeMap;
-use std::collections::BTreeSet;
-use std::ffi::CStr;
-use std::ops::Bound::Included;
-use std::ops::Bound::Unbounded;
-use std::sync::atomic::AtomicBool;
-use std::sync::atomic::Ordering;
-use std::sync::Arc;
-use std::time::Duration;
-use std::time::Instant;
-
-use ::fb_procfs as procfs;
-use anyhow::anyhow;
-use anyhow::bail;
-use anyhow::Context;
-use anyhow::Result;
-use bitvec::prelude::*;
-use clap::Parser;
-use libbpf_rs::skel::OpenSkel as _;
-use libbpf_rs::skel::Skel as _;
-use libbpf_rs::skel::SkelBuilder as _;
-use log::debug;
-use log::info;
-use log::trace;
-use log::warn;
-use ordered_float::OrderedFloat;
-use scx_utils::ravg::ravg_read;
-
-const RAVG_FRAC_BITS: u32 = bpf_intf::ravg_consts_RAVG_FRAC_BITS;
-const MAX_DOMS: usize = bpf_intf::consts_MAX_DOMS as usize;
-const MAX_CPUS: usize = bpf_intf::consts_MAX_CPUS as usize;
-
-/// scx_rusty: A multi-domain BPF / userspace hybrid scheduler
-///
-/// The BPF part does simple vtime or round robin scheduling in each domain
-/// while tracking average load of each domain and duty cycle of each task.
-///
-/// The userspace part performs two roles. First, it makes higher frequency
-/// (100ms) tuning decisions. It identifies CPUs which are not too heavily
-/// loaded and mark them so that they can pull tasks from other overloaded
-/// domains on the fly.
-///
-/// Second, it drives lower frequency (2s) load balancing. It determines
-/// whether load balancing is necessary by comparing domain load averages.
-/// If there are large enough load differences, it examines upto 1024
-/// recently active tasks on the domain to determine which should be
-/// migrated.
-///
-/// The overhead of userspace operations is low. Load balancing is not
-/// performed frequently but work-conservation is still maintained through
-/// tuning and greedy execution. Load balancing itself is not that expensive
-/// either. It only accesses per-domain load metrics to determine the
-/// domains that need load balancing and limited number of per-task metrics
-/// for each pushing domain.
-///
-/// An earlier variant of this scheduler was used to balance across six
-/// domains, each representing a chiplet in a six-chiplet AMD processor, and
-/// could match the performance of production setup using CFS.
-///
-/// WARNING: Very high weight (low nice value) tasks can throw off load
-/// balancing due to infeasible weight problem. This problem will be solved
-/// in the near future.
-///
-/// WARNING: scx_rusty currently assumes that all domains have equal
-/// processing power and at similar distances from each other. This
-/// limitation will be removed in the future.
-#[derive(Debug, Parser)]
-struct Opts {
-    /// Scheduling slice duration in microseconds.
-    #[clap(short = 's', long, default_value = "20000")]
-    slice_us: u64,
-
-    /// Monitoring and load balance interval in seconds.
-    #[clap(short = 'i', long, default_value = "2.0")]
-    interval: f64,
-
-    /// Tuner runs at higher frequency than the load balancer to dynamically
-    /// tune scheduling behavior. Tuning interval in seconds.
-    #[clap(short = 'I', long, default_value = "0.1")]
-    tune_interval: f64,
-
-    /// The half-life of task and domain load running averages in seconds.
-    #[clap(short = 'l', long, default_value = "1.0")]
-    load_half_life: f64,
-
-    /// Build domains according to how CPUs are grouped at this cache level
-    /// as determined by /sys/devices/system/cpu/cpuX/cache/indexI/id.
-    #[clap(short = 'c', long, default_value = "3")]
-    cache_level: u32,
-
-    /// Instead of using cache locality, set the cpumask for each domain
-    /// manually, provide multiple --cpumasks, one for each domain. E.g.
-    /// --cpumasks 0xff_00ff --cpumasks 0xff00 will create two domains with
-    /// the corresponding CPUs belonging to each domain. Each CPU must
-    /// belong to precisely one domain.
-    #[clap(short = 'C', long, num_args = 1.., conflicts_with = "cache_level")]
-    cpumasks: Vec<String>,
-
-    /// When non-zero, enable greedy task stealing. When a domain is idle, a
-    /// cpu will attempt to steal tasks from a domain with at least
-    /// greedy_threshold tasks enqueued. These tasks aren't permanently
-    /// stolen from the domain.
-    #[clap(short = 'g', long, default_value = "1")]
-    greedy_threshold: u32,
-
-    /// Disable load balancing. Unless disabled, periodically userspace will
-    /// calculate the load factor of each domain and instruct BPF which
-    /// processes to move.
-    #[clap(long, action = clap::ArgAction::SetTrue)]
-    no_load_balance: bool,
-
-    /// Put per-cpu kthreads directly into local dsq's.
-    #[clap(short = 'k', long, action = clap::ArgAction::SetTrue)]
-    kthreads_local: bool,
-
-    /// In recent kernels (>=v6.6), the kernel is responsible for balancing
-    /// kworkers across L3 cache domains. Exclude them from load-balancing
-    /// to avoid conflicting operations. Greedy executions still apply.
-    #[clap(short = 'b', long, action = clap::ArgAction::SetTrue)]
-    balanced_kworkers: bool,
-
-    /// Use FIFO scheduling instead of weighted vtime scheduling.
-    #[clap(short = 'f', long, action = clap::ArgAction::SetTrue)]
-    fifo_sched: bool,
-
-    /// Idle CPUs with utilization lower than this will get remote tasks
-    /// directly pushed on them. 0 disables, 100 enables always.
-    #[clap(short = 'D', long, default_value = "90.0")]
-    direct_greedy_under: f64,
-
-    /// Idle CPUs with utilization lower than this may get kicked to
-    /// accelerate stealing when a task is queued on a saturated remote
-    /// domain. 0 disables, 100 enables always.
-    #[clap(short = 'K', long, default_value = "100.0")]
-    kick_greedy_under: f64,
-
-    /// If specified, only tasks which have their scheduling policy set to
-    /// SCHED_EXT using sched_setscheduler(2) are switched. Otherwise, all
-    /// tasks are switched.
-    #[clap(short = 'p', long, action = clap::ArgAction::SetTrue)]
-    partial: bool,
-
-    /// Enable verbose output including libbpf details. Specify multiple
-    /// times to increase verbosity.
-    #[clap(short = 'v', long, action = clap::ArgAction::Count)]
-    verbose: u8,
-}
-
-fn now_monotonic() -> u64 {
-    let mut time = libc::timespec {
-        tv_sec: 0,
-        tv_nsec: 0,
-    };
-    let ret = unsafe { libc::clock_gettime(libc::CLOCK_MONOTONIC, &mut time) };
-    assert!(ret == 0);
-    time.tv_sec as u64 * 1_000_000_000 + time.tv_nsec as u64
-}
-
-fn clear_map(map: &libbpf_rs::Map) {
-    for key in map.keys() {
-        let _ = map.delete(&key);
-    }
-}
-
-fn format_cpumask(cpumask: &[u64], nr_cpus: usize) -> String {
-    cpumask
-        .iter()
-        .take((nr_cpus + 64) / 64)
-        .rev()
-        .fold(String::new(), |acc, x| format!("{} {:016X}", acc, x))
-}
-
-fn read_total_cpu(reader: &procfs::ProcReader) -> Result<procfs::CpuStat> {
-    reader
-        .read_stat()
-        .context("Failed to read procfs")?
-        .total_cpu
-        .ok_or_else(|| anyhow!("Could not read total cpu stat in proc"))
-}
-
-fn sub_or_zero(curr: &u64, prev: &u64) -> u64
-{
-    if let Some(res) = curr.checked_sub(*prev) {
-        res
-    } else {
-        0
-    }
-}
-
-fn calc_util(curr: &procfs::CpuStat, prev: &procfs::CpuStat) -> Result<f64> {
-    match (curr, prev) {
-        (
-            procfs::CpuStat {
-                user_usec: Some(curr_user),
-                nice_usec: Some(curr_nice),
-                system_usec: Some(curr_system),
-                idle_usec: Some(curr_idle),
-                iowait_usec: Some(curr_iowait),
-                irq_usec: Some(curr_irq),
-                softirq_usec: Some(curr_softirq),
-                stolen_usec: Some(curr_stolen),
-                ..
-            },
-            procfs::CpuStat {
-                user_usec: Some(prev_user),
-                nice_usec: Some(prev_nice),
-                system_usec: Some(prev_system),
-                idle_usec: Some(prev_idle),
-                iowait_usec: Some(prev_iowait),
-                irq_usec: Some(prev_irq),
-                softirq_usec: Some(prev_softirq),
-                stolen_usec: Some(prev_stolen),
-                ..
-            },
-        ) => {
-            let idle_usec = sub_or_zero(curr_idle, prev_idle);
-            let iowait_usec = sub_or_zero(curr_iowait, prev_iowait);
-            let user_usec = sub_or_zero(curr_user, prev_user);
-            let system_usec = sub_or_zero(curr_system, prev_system);
-            let nice_usec = sub_or_zero(curr_nice, prev_nice);
-            let irq_usec = sub_or_zero(curr_irq, prev_irq);
-            let softirq_usec = sub_or_zero(curr_softirq, prev_softirq);
-            let stolen_usec = sub_or_zero(curr_stolen, prev_stolen);
-
-            let busy_usec =
-                user_usec + system_usec + nice_usec + irq_usec + softirq_usec + stolen_usec;
-            let total_usec = idle_usec + busy_usec + iowait_usec;
-            if total_usec > 0 {
-                Ok(((busy_usec as f64) / (total_usec as f64)).clamp(0.0, 1.0))
-            } else {
-                Ok(1.0)
-            }
-        }
-        _ => {
-            bail!("Missing stats in cpustat");
-        }
-    }
-}
-
-#[derive(Debug)]
-struct Topology {
-    nr_cpus: usize,
-    nr_doms: usize,
-    dom_cpus: Vec<BitVec<u64, Lsb0>>,
-    cpu_dom: Vec<Option<usize>>,
-}
-
-impl Topology {
-    fn from_cpumasks(cpumasks: &[String], nr_cpus: usize) -> Result<Self> {
-        if cpumasks.len() > MAX_DOMS {
-            bail!(
-                "Number of requested domains ({}) is greater than MAX_DOMS ({})",
-                cpumasks.len(),
-                MAX_DOMS
-            );
-        }
-        let mut cpu_dom = vec![None; nr_cpus];
-        let mut dom_cpus = vec![bitvec![u64, Lsb0; 0; MAX_CPUS]; cpumasks.len()];
-        for (dom, cpumask) in cpumasks.iter().enumerate() {
-            let hex_str = {
-                let mut tmp_str = cpumask
-                    .strip_prefix("0x")
-                    .unwrap_or(cpumask)
-                    .replace('_', "");
-                if tmp_str.len() % 2 != 0 {
-                    tmp_str = "0".to_string() + &tmp_str;
-                }
-                tmp_str
-            };
-            let byte_vec = hex::decode(&hex_str)
-                .with_context(|| format!("Failed to parse cpumask: {}", cpumask))?;
-
-            for (index, &val) in byte_vec.iter().rev().enumerate() {
-                let mut v = val;
-                while v != 0 {
-                    let lsb = v.trailing_zeros() as usize;
-                    v &= !(1 << lsb);
-                    let cpu = index * 8 + lsb;
-                    if cpu > nr_cpus {
-                        bail!(
-                            concat!(
-                                "Found cpu ({}) in cpumask ({}) which is larger",
-                                " than the number of cpus on the machine ({})"
-                            ),
-                            cpu,
-                            cpumask,
-                            nr_cpus
-                        );
-                    }
-                    if let Some(other_dom) = cpu_dom[cpu] {
-                        bail!(
-                            "Found cpu ({}) with domain ({}) but also in cpumask ({})",
-                            cpu,
-                            other_dom,
-                            cpumask
-                        );
-                    }
-                    cpu_dom[cpu] = Some(dom);
-                    dom_cpus[dom].set(cpu, true);
-                }
-            }
-            dom_cpus[dom].set_uninitialized(false);
-        }
-
-        for (cpu, dom) in cpu_dom.iter().enumerate() {
-            if dom.is_none() {
-                bail!(
-                    "CPU {} not assigned to any domain. Make sure it is covered by some --cpumasks argument.",
-                    cpu
-                );
-            }
-        }
-
-        Ok(Self {
-            nr_cpus,
-            nr_doms: dom_cpus.len(),
-            dom_cpus,
-            cpu_dom,
-        })
-    }
-
-    fn from_cache_level(level: u32, nr_cpus: usize) -> Result<Self> {
-        let mut cpu_to_cache = vec![]; // (cpu_id, Option<cache_id>)
-        let mut cache_ids = BTreeSet::<usize>::new();
-        let mut nr_offline = 0;
-
-        // Build cpu -> cache ID mapping.
-        for cpu in 0..nr_cpus {
-            let path = format!("/sys/devices/system/cpu/cpu{}/cache/index{}/id", cpu, level);
-            let id = match std::fs::read_to_string(&path) {
-                Ok(val) => Some(val.trim().parse::<usize>().with_context(|| {
-                    format!("Failed to parse {:?}'s content {:?}", &path, &val)
-                })?),
-                Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
-                    nr_offline += 1;
-                    None
-                }
-                Err(e) => return Err(e).with_context(|| format!("Failed to open {:?}", &path)),
-            };
-
-            cpu_to_cache.push(id);
-            if let Some(id) = id {
-                cache_ids.insert(id);
-            }
-        }
-
-        info!(
-            "CPUs: online/possible = {}/{}",
-            nr_cpus - nr_offline,
-            nr_cpus
-        );
-
-        // Cache IDs may have holes. Assign consecutive domain IDs to
-        // existing cache IDs.
-        let mut cache_to_dom = BTreeMap::<usize, usize>::new();
-        let mut nr_doms = 0;
-        for cache_id in cache_ids.iter() {
-            cache_to_dom.insert(*cache_id, nr_doms);
-            nr_doms += 1;
-        }
-
-        if nr_doms > MAX_DOMS {
-            bail!(
-                "Total number of doms {} is greater than MAX_DOMS ({})",
-                nr_doms,
-                MAX_DOMS
-            );
-        }
-
-        // Build and return dom -> cpumask and cpu -> dom mappings.
-        let mut dom_cpus = vec![bitvec![u64, Lsb0; 0; MAX_CPUS]; nr_doms];
-        let mut cpu_dom = vec![];
-
-        for (cpu, cache) in cpu_to_cache.iter().enumerate().take(nr_cpus) {
-            match cache {
-                Some(cache_id) => {
-                    let dom_id = cache_to_dom[cache_id];
-                    dom_cpus[dom_id].set(cpu, true);
-                    cpu_dom.push(Some(dom_id));
-                }
-                None => {
-                    dom_cpus[0].set(cpu, true);
-                    cpu_dom.push(None);
-                }
-            }
-        }
-
-        Ok(Self {
-            nr_cpus,
-            nr_doms: dom_cpus.len(),
-            dom_cpus,
-            cpu_dom,
-        })
-    }
-}
-
-struct Tuner {
-    top: Arc<Topology>,
-    direct_greedy_under: f64,
-    kick_greedy_under: f64,
-    proc_reader: procfs::ProcReader,
-    prev_cpu_stats: BTreeMap<u32, procfs::CpuStat>,
-    dom_utils: Vec<f64>,
-}
-
-impl Tuner {
-    fn new(top: Arc<Topology>, opts: &Opts) -> Result<Self> {
-        let proc_reader = procfs::ProcReader::new();
-        let prev_cpu_stats = proc_reader
-            .read_stat()?
-            .cpus_map
-            .ok_or_else(|| anyhow!("Expected cpus_map to exist"))?;
-        Ok(Self {
-            direct_greedy_under: opts.direct_greedy_under / 100.0,
-            kick_greedy_under: opts.kick_greedy_under / 100.0,
-            proc_reader,
-            prev_cpu_stats,
-            dom_utils: vec![0.0; top.nr_doms],
-            top,
-        })
-    }
-
-    fn step(&mut self, skel: &mut BpfSkel) -> Result<()> {
-        let curr_cpu_stats = self
-            .proc_reader
-            .read_stat()?
-            .cpus_map
-            .ok_or_else(|| anyhow!("Expected cpus_map to exist"))?;
-        let ti = &mut skel.bss_mut().tune_input;
-        let mut dom_nr_cpus = vec![0; self.top.nr_doms];
-        let mut dom_util_sum = vec![0.0; self.top.nr_doms];
-
-        for cpu in 0..self.top.nr_cpus {
-            let cpu32 = cpu as u32;
-            // None domain indicates the CPU was offline during
-            // initialization and None CpuStat indicates the CPU has gone
-            // down since then. Ignore both.
-            if let (Some(dom), Some(curr), Some(prev)) = (
-                self.top.cpu_dom[cpu],
-                curr_cpu_stats.get(&cpu32),
-                self.prev_cpu_stats.get(&cpu32),
-            ) {
-                dom_nr_cpus[dom] += 1;
-                dom_util_sum[dom] += calc_util(curr, prev)?;
-            }
-        }
-
-        for dom in 0..self.top.nr_doms {
-            // Calculate the domain avg util. If there are no active CPUs,
-            // it doesn't really matter. Go with 0.0 as that's less likely
-            // to confuse users.
-            let util = match dom_nr_cpus[dom] {
-                0 => 0.0,
-                nr => dom_util_sum[dom] / nr as f64,
-            };
-
-            self.dom_utils[dom] = util;
-
-            // This could be implemented better.
-            let update_dom_bits = |target: &mut [u64; 8], val: bool| {
-                for cpu in 0..self.top.nr_cpus {
-                    if let Some(cdom) = self.top.cpu_dom[cpu] {
-                        if cdom == dom {
-                            if val {
-                                target[cpu / 64] |= 1u64 << (cpu % 64);
-                            } else {
-                                target[cpu / 64] &= !(1u64 << (cpu % 64));
-                            }
-                        }
-                    }
-                }
-            };
-
-            update_dom_bits(
-                &mut ti.direct_greedy_cpumask,
-                self.direct_greedy_under > 0.99999 || util < self.direct_greedy_under,
-            );
-            update_dom_bits(
-                &mut ti.kick_greedy_cpumask,
-                self.kick_greedy_under > 0.99999 || util < self.kick_greedy_under,
-            );
-        }
-
-        ti.gen += 1;
-        self.prev_cpu_stats = curr_cpu_stats;
-        Ok(())
-    }
-}
-
-#[derive(Debug)]
-struct TaskInfo {
-    pid: i32,
-    dom_mask: u64,
-    migrated: Cell<bool>,
-    is_kworker: bool,
-}
-
-struct LoadBalancer<'a, 'b, 'c> {
-    skel: &'a mut BpfSkel<'b>,
-    top: Arc<Topology>,
-    skip_kworkers: bool,
-
-    tasks_by_load: Vec<Option<BTreeMap<OrderedFloat<f64>, TaskInfo>>>,
-    load_avg: f64,
-    dom_loads: Vec<f64>,
-
-    imbal: Vec<f64>,
-    doms_to_push: BTreeMap<OrderedFloat<f64>, u32>,
-    doms_to_pull: BTreeMap<OrderedFloat<f64>, u32>,
-
-    nr_lb_data_errors: &'c mut u64,
-}
-
-impl<'a, 'b, 'c> LoadBalancer<'a, 'b, 'c> {
-    // If imbalance gets higher than this ratio, try to balance the loads.
-    const LOAD_IMBAL_HIGH_RATIO: f64 = 0.10;
-
-    // Aim to transfer this fraction of the imbalance on each round. We want
-    // to be gradual to avoid unnecessary oscillations. While this can delay
-    // convergence, greedy execution should be able to bridge the temporary
-    // gap.
-    const LOAD_IMBAL_XFER_TARGET_RATIO: f64 = 0.50;
-
-    // Don't push out more than this ratio of load on each round. While this
-    // overlaps with XFER_TARGET_RATIO, XFER_TARGET_RATIO only defines the
-    // target and doesn't limit the total load. As long as the transfer
-    // reduces load imbalance between the two involved domains, it'd happily
-    // transfer whatever amount that can be transferred. This limit is used
-    // as the safety cap to avoid draining a given domain too much in a
-    // single round.
-    const LOAD_IMBAL_PUSH_MAX_RATIO: f64 = 0.50;
-
-    fn new(
-        skel: &'a mut BpfSkel<'b>,
-        top: Arc<Topology>,
-        skip_kworkers: bool,
-        nr_lb_data_errors: &'c mut u64,
-    ) -> Self {
-        Self {
-            skel,
-            skip_kworkers,
-
-            tasks_by_load: (0..top.nr_doms).map(|_| None).collect(),
-            load_avg: 0f64,
-            dom_loads: vec![0.0; top.nr_doms],
-
-            imbal: vec![0.0; top.nr_doms],
-            doms_to_pull: BTreeMap::new(),
-            doms_to_push: BTreeMap::new(),
-
-            nr_lb_data_errors,
-
-            top,
-        }
-    }
-
-    fn read_dom_loads(&mut self) -> Result<()> {
-        let now_mono = now_monotonic();
-        let load_half_life = self.skel.rodata().load_half_life;
-        let maps = self.skel.maps();
-        let dom_data = maps.dom_data();
-        let mut load_sum = 0.0f64;
-
-        for i in 0..self.top.nr_doms {
-            let key = unsafe { std::mem::transmute::<u32, [u8; 4]>(i as u32) };
-
-            if let Some(dom_ctx_map_elem) = dom_data
-                .lookup(&key, libbpf_rs::MapFlags::ANY)
-                .context("Failed to lookup dom_ctx")?
-            {
-                let dom_ctx =
-                    unsafe { &*(dom_ctx_map_elem.as_slice().as_ptr() as *const bpf_intf::dom_ctx) };
-
-                let rd = &dom_ctx.load_rd;
-                self.dom_loads[i] = ravg_read(
-                    rd.val,
-                    rd.val_at,
-                    rd.old,
-                    rd.cur,
-                    now_mono,
-                    load_half_life,
-                    RAVG_FRAC_BITS,
-                );
-
-                load_sum += self.dom_loads[i];
-            }
-        }
-
-        self.load_avg = load_sum / self.top.nr_doms as f64;
-
-        Ok(())
-    }
-
-    /// To balance dom loads, identify doms with lower and higher load than
-    /// average.
-    fn calculate_dom_load_balance(&mut self) -> Result<()> {
-        for (dom, dom_load) in self.dom_loads.iter().enumerate() {
-            let imbal = dom_load - self.load_avg;
-            if imbal.abs() >= self.load_avg * Self::LOAD_IMBAL_HIGH_RATIO {
-                if imbal > 0f64 {
-                    self.doms_to_push.insert(OrderedFloat(imbal), dom as u32);
-                } else {
-                    self.doms_to_pull.insert(OrderedFloat(-imbal), dom as u32);
-                }
-                self.imbal[dom] = imbal;
-            }
-        }
-        Ok(())
-    }
-
-    /// @dom needs to push out tasks to balance loads. Make sure its
-    /// tasks_by_load is populated so that the victim tasks can be picked.
-    fn populate_tasks_by_load(&mut self, dom: u32) -> Result<()> {
-        if self.tasks_by_load[dom as usize].is_some() {
-            return Ok(());
-        }
-
-        // Read active_pids and update write_idx and gen.
-        //
-        // XXX - We can't read task_ctx inline because self.skel.bss()
-        // borrows mutably and thus conflicts with self.skel.maps().
-        const MAX_PIDS: u64 = bpf_intf::consts_MAX_DOM_ACTIVE_PIDS as u64;
-        let active_pids = &mut self.skel.bss_mut().dom_active_pids[dom as usize];
-        let mut pids = vec![];
-
-        let (mut ridx, widx) = (active_pids.read_idx, active_pids.write_idx);
-        if widx - ridx > MAX_PIDS {
-            ridx = widx - MAX_PIDS;
-        }
-
-        for idx in ridx..widx {
-            let pid = active_pids.pids[(idx % MAX_PIDS) as usize];
-            pids.push(pid);
-        }
-
-        active_pids.read_idx = active_pids.write_idx;
-        active_pids.gen += 1;
-
-        // Read task_ctx and load.
-        let load_half_life = self.skel.rodata().load_half_life;
-        let maps = self.skel.maps();
-        let task_data = maps.task_data();
-        let now_mono = now_monotonic();
-        let mut tasks_by_load = BTreeMap::new();
-
-        for pid in pids.iter() {
-            let key = unsafe { std::mem::transmute::<i32, [u8; 4]>(*pid) };
-
-            if let Some(task_data_elem) = task_data.lookup(&key, libbpf_rs::MapFlags::ANY)? {
-                let task_ctx =
-                    unsafe { &*(task_data_elem.as_slice().as_ptr() as *const bpf_intf::task_ctx) };
-
-                if task_ctx.dom_id != dom {
-                    continue;
-                }
-
-                let rd = &task_ctx.dcyc_rd;
-                let load = task_ctx.weight as f64
-                    * ravg_read(
-                        rd.val,
-                        rd.val_at,
-                        rd.old,
-                        rd.cur,
-                        now_mono,
-                        load_half_life,
-                        RAVG_FRAC_BITS,
-                    );
-
-                tasks_by_load.insert(
-                    OrderedFloat(load),
-                    TaskInfo {
-                        pid: *pid,
-                        dom_mask: task_ctx.dom_mask,
-                        migrated: Cell::new(false),
-                        is_kworker: task_ctx.is_kworker,
-                    },
-                );
-            }
-        }
-
-        debug!(
-            "DOM[{:02}] read load for {} tasks",
-            dom,
-            &tasks_by_load.len(),
-        );
-        trace!("DOM[{:02}] tasks_by_load={:?}", dom, &tasks_by_load);
-
-        self.tasks_by_load[dom as usize] = Some(tasks_by_load);
-        Ok(())
-    }
-
-    // Find the first candidate pid which hasn't already been migrated and
-    // can run in @pull_dom.
-    fn find_first_candidate<'d, I>(
-        tasks_by_load: I,
-        pull_dom: u32,
-        skip_kworkers: bool,
-    ) -> Option<(f64, &'d TaskInfo)>
-    where
-        I: IntoIterator<Item = (&'d OrderedFloat<f64>, &'d TaskInfo)>,
-    {
-        match tasks_by_load
-            .into_iter()
-            .skip_while(|(_, task)| {
-                task.migrated.get()
-                    || (task.dom_mask & (1 << pull_dom) == 0)
-                    || (skip_kworkers && task.is_kworker)
-            })
-            .next()
-        {
-            Some((OrderedFloat(load), task)) => Some((*load, task)),
-            None => None,
-        }
-    }
-
-    fn pick_victim(
-        &mut self,
-        (push_dom, to_push): (u32, f64),
-        (pull_dom, to_pull): (u32, f64),
-    ) -> Result<Option<(&TaskInfo, f64)>> {
-        let to_xfer = to_pull.min(to_push) * Self::LOAD_IMBAL_XFER_TARGET_RATIO;
-
-        debug!(
-            "considering dom {}@{:.2} -> {}@{:.2}",
-            push_dom, to_push, pull_dom, to_pull
-        );
-
-        let calc_new_imbal = |xfer: f64| (to_push - xfer).abs() + (to_pull - xfer).abs();
-
-        self.populate_tasks_by_load(push_dom)?;
-
-        // We want to pick a task to transfer from push_dom to pull_dom to
-        // reduce the load imbalance between the two closest to $to_xfer.
-        // IOW, pick a task which has the closest load value to $to_xfer
-        // that can be migrated. Find such task by locating the first
-        // migratable task while scanning left from $to_xfer and the
-        // counterpart while scanning right and picking the better of the
-        // two.
-        let (load, task, new_imbal) = match (
-            Self::find_first_candidate(
-                self.tasks_by_load[push_dom as usize]
-                    .as_ref()
-                    .unwrap()
-                    .range((Unbounded, Included(&OrderedFloat(to_xfer))))
-                    .rev(),
-                pull_dom,
-                self.skip_kworkers,
-            ),
-            Self::find_first_candidate(
-                self.tasks_by_load[push_dom as usize]
-                    .as_ref()
-                    .unwrap()
-                    .range((Included(&OrderedFloat(to_xfer)), Unbounded)),
-                pull_dom,
-                self.skip_kworkers,
-            ),
-        ) {
-            (None, None) => return Ok(None),
-            (Some((load, task)), None) | (None, Some((load, task))) => {
-                (load, task, calc_new_imbal(load))
-            }
-            (Some((load0, task0)), Some((load1, task1))) => {
-                let (new_imbal0, new_imbal1) = (calc_new_imbal(load0), calc_new_imbal(load1));
-                if new_imbal0 <= new_imbal1 {
-                    (load0, task0, new_imbal0)
-                } else {
-                    (load1, task1, new_imbal1)
-                }
-            }
-        };
-
-        // If the best candidate can't reduce the imbalance, there's nothing
-        // to do for this pair.
-        let old_imbal = to_push + to_pull;
-        if old_imbal < new_imbal {
-            debug!(
-                "skipping pid {}, dom {} -> {} won't improve imbal {:.2} -> {:.2}",
-                task.pid, push_dom, pull_dom, old_imbal, new_imbal
-            );
-            return Ok(None);
-        }
-
-        debug!(
-            "migrating pid {}, dom {} -> {}, imbal={:.2} -> {:.2}",
-            task.pid, push_dom, pull_dom, old_imbal, new_imbal,
-        );
-
-        Ok(Some((task, load)))
-    }
-
-    // Actually execute the load balancing. Concretely this writes pid -> dom
-    // entries into the lb_data map for bpf side to consume.
-    fn load_balance(&mut self) -> Result<()> {
-        clear_map(self.skel.maps().lb_data());
-
-        debug!("imbal={:?}", &self.imbal);
-        debug!("doms_to_push={:?}", &self.doms_to_push);
-        debug!("doms_to_pull={:?}", &self.doms_to_pull);
-
-        // Push from the most imbalanced to least.
-        while let Some((OrderedFloat(mut to_push), push_dom)) = self.doms_to_push.pop_last() {
-            let push_max = self.dom_loads[push_dom as usize] * Self::LOAD_IMBAL_PUSH_MAX_RATIO;
-            let mut pushed = 0f64;
-
-            // Transfer tasks from push_dom to reduce imbalance.
-            loop {
-                let last_pushed = pushed;
-
-                // Pull from the most imbalaned to least.
-                let mut doms_to_pull = BTreeMap::<_, _>::new();
-                std::mem::swap(&mut self.doms_to_pull, &mut doms_to_pull);
-                let mut pull_doms = doms_to_pull.into_iter().rev().collect::<Vec<(_, _)>>();
-
-                for (to_pull, pull_dom) in pull_doms.iter_mut() {
-                    if let Some((task, load)) =
-                        self.pick_victim((push_dom, to_push), (*pull_dom, f64::from(*to_pull)))?
-                    {
-                        // Execute migration.
-                        task.migrated.set(true);
-                        to_push -= load;
-                        *to_pull -= load;
-                        pushed += load;
-
-                        // Ask BPF code to execute the migration.
-                        let pid = task.pid;
-                        let cpid = (pid as libc::pid_t).to_ne_bytes();
-                        if let Err(e) = self.skel.maps_mut().lb_data().update(
-                            &cpid,
-                            &pull_dom.to_ne_bytes(),
-                            libbpf_rs::MapFlags::NO_EXIST,
-                        ) {
-                            warn!(
-                                "Failed to update lb_data map for pid={} error={:?}",
-                                pid, &e
-                            );
-                            *self.nr_lb_data_errors += 1;
-                        }
-
-                        // Always break after a successful migration so that
-                        // the pulling domains are always considered in the
-                        // descending imbalance order.
-                        break;
-                    }
-                }
-
-                pull_doms
-                    .into_iter()
-                    .map(|(k, v)| self.doms_to_pull.insert(k, v))
-                    .count();
-
-                // Stop repeating if nothing got transferred or pushed enough.
-                if pushed == last_pushed || pushed >= push_max {
-                    break;
-                }
-            }
-        }
-        Ok(())
-    }
-}
-
-struct Scheduler<'a> {
-    skel: BpfSkel<'a>,
-    struct_ops: Option<libbpf_rs::Link>,
-
-    sched_interval: Duration,
-    tune_interval: Duration,
-    balance_load: bool,
-    balanced_kworkers: bool,
-
-    top: Arc<Topology>,
-    proc_reader: procfs::ProcReader,
-
-    prev_at: Instant,
-    prev_total_cpu: procfs::CpuStat,
-
-    nr_lb_data_errors: u64,
-
-    tuner: Tuner,
-}
-
-impl<'a> Scheduler<'a> {
-    fn init(opts: &Opts) -> Result<Self> {
-        // Open the BPF prog first for verification.
-        let mut skel_builder = BpfSkelBuilder::default();
-        skel_builder.obj_builder.debug(opts.verbose > 0);
-        let mut skel = skel_builder.open().context("Failed to open BPF program")?;
-
-        let nr_cpus = libbpf_rs::num_possible_cpus().unwrap();
-        if nr_cpus > MAX_CPUS {
-            bail!(
-                "nr_cpus ({}) is greater than MAX_CPUS ({})",
-                nr_cpus,
-                MAX_CPUS
-            );
-        }
-
-        // Initialize skel according to @opts.
-        let top = Arc::new(if !opts.cpumasks.is_empty() {
-            Topology::from_cpumasks(&opts.cpumasks, nr_cpus)?
-        } else {
-            Topology::from_cache_level(opts.cache_level, nr_cpus)?
-        });
-
-        skel.rodata_mut().nr_doms = top.nr_doms as u32;
-        skel.rodata_mut().nr_cpus = top.nr_cpus as u32;
-
-        for (cpu, dom) in top.cpu_dom.iter().enumerate() {
-            skel.rodata_mut().cpu_dom_id_map[cpu] = dom.unwrap_or(0) as u32;
-        }
-
-        for (dom, cpus) in top.dom_cpus.iter().enumerate() {
-            let raw_cpus_slice = cpus.as_raw_slice();
-            let dom_cpumask_slice = &mut skel.rodata_mut().dom_cpumasks[dom];
-            let (left, _) = dom_cpumask_slice.split_at_mut(raw_cpus_slice.len());
-            left.clone_from_slice(cpus.as_raw_slice());
-            info!(
-                "DOM[{:02}] cpumask{} ({} cpus)",
-                dom,
-                &format_cpumask(dom_cpumask_slice, nr_cpus),
-                cpus.count_ones()
-            );
-        }
-
-        skel.rodata_mut().slice_ns = opts.slice_us * 1000;
-        skel.rodata_mut().load_half_life = (opts.load_half_life * 1000000000.0) as u32;
-        skel.rodata_mut().kthreads_local = opts.kthreads_local;
-        skel.rodata_mut().fifo_sched = opts.fifo_sched;
-        skel.rodata_mut().switch_partial = opts.partial;
-        skel.rodata_mut().greedy_threshold = opts.greedy_threshold;
-        skel.rodata_mut().debug = opts.verbose as u32;
-
-        // Attach.
-        let mut skel = skel.load().context("Failed to load BPF program")?;
-        skel.attach().context("Failed to attach BPF program")?;
-        let struct_ops = Some(
-            skel.maps_mut()
-                .rusty()
-                .attach_struct_ops()
-                .context("Failed to attach rusty struct ops")?,
-        );
-        info!("Rusty Scheduler Attached");
-
-        // Other stuff.
-        let proc_reader = procfs::ProcReader::new();
-        let prev_total_cpu = read_total_cpu(&proc_reader)?;
-
-        Ok(Self {
-            skel,
-            struct_ops, // should be held to keep it attached
-
-            sched_interval: Duration::from_secs_f64(opts.interval),
-            tune_interval: Duration::from_secs_f64(opts.tune_interval),
-            balance_load: !opts.no_load_balance,
-            balanced_kworkers: opts.balanced_kworkers,
-
-            top: top.clone(),
-            proc_reader,
-
-            prev_at: Instant::now(),
-            prev_total_cpu,
-
-            nr_lb_data_errors: 0,
-
-            tuner: Tuner::new(top, opts)?,
-        })
-    }
-
-    fn get_cpu_busy(&mut self) -> Result<f64> {
-        let total_cpu = read_total_cpu(&self.proc_reader)?;
-        let busy = match (&self.prev_total_cpu, &total_cpu) {
-            (
-                procfs::CpuStat {
-                    user_usec: Some(prev_user),
-                    nice_usec: Some(prev_nice),
-                    system_usec: Some(prev_system),
-                    idle_usec: Some(prev_idle),
-                    iowait_usec: Some(prev_iowait),
-                    irq_usec: Some(prev_irq),
-                    softirq_usec: Some(prev_softirq),
-                    stolen_usec: Some(prev_stolen),
-                    guest_usec: _,
-                    guest_nice_usec: _,
-                },
-                procfs::CpuStat {
-                    user_usec: Some(curr_user),
-                    nice_usec: Some(curr_nice),
-                    system_usec: Some(curr_system),
-                    idle_usec: Some(curr_idle),
-                    iowait_usec: Some(curr_iowait),
-                    irq_usec: Some(curr_irq),
-                    softirq_usec: Some(curr_softirq),
-                    stolen_usec: Some(curr_stolen),
-                    guest_usec: _,
-                    guest_nice_usec: _,
-                },
-            ) => {
-                let idle_usec = sub_or_zero(curr_idle, prev_idle);
-                let iowait_usec = sub_or_zero(curr_iowait, prev_iowait);
-                let user_usec = sub_or_zero(curr_user, prev_user);
-                let system_usec = sub_or_zero(curr_system, prev_system);
-                let nice_usec = sub_or_zero(curr_nice, prev_nice);
-                let irq_usec = sub_or_zero(curr_irq, prev_irq);
-                let softirq_usec = sub_or_zero(curr_softirq, prev_softirq);
-                let stolen_usec = sub_or_zero(curr_stolen, prev_stolen);
-
-                let busy_usec =
-                    user_usec + system_usec + nice_usec + irq_usec + softirq_usec + stolen_usec;
-                let total_usec = idle_usec + busy_usec + iowait_usec;
-                busy_usec as f64 / total_usec as f64
-            }
-            _ => {
-                bail!("Some procfs stats are not populated!");
-            }
-        };
-
-        self.prev_total_cpu = total_cpu;
-        Ok(busy)
-    }
-
-    fn read_bpf_stats(&mut self) -> Result<Vec<u64>> {
-        let mut maps = self.skel.maps_mut();
-        let stats_map = maps.stats();
-        let mut stats: Vec<u64> = Vec::new();
-        let zero_vec = vec![vec![0u8; stats_map.value_size() as usize]; self.top.nr_cpus];
-
-        for stat in 0..bpf_intf::stat_idx_RUSTY_NR_STATS {
-            let cpu_stat_vec = stats_map
-                .lookup_percpu(&stat.to_ne_bytes(), libbpf_rs::MapFlags::ANY)
-                .with_context(|| format!("Failed to lookup stat {}", stat))?
-                .expect("per-cpu stat should exist");
-            let sum = cpu_stat_vec
-                .iter()
-                .map(|val| {
-                    u64::from_ne_bytes(
-                        val.as_slice()
-                            .try_into()
-                            .expect("Invalid value length in stat map"),
-                    )
-                })
-                .sum();
-            stats_map
-                .update_percpu(&stat.to_ne_bytes(), &zero_vec, libbpf_rs::MapFlags::ANY)
-                .context("Failed to zero stat")?;
-            stats.push(sum);
-        }
-        Ok(stats)
-    }
-
-    fn report(
-        &mut self,
-        stats: &[u64],
-        cpu_busy: f64,
-        processing_dur: Duration,
-        load_avg: f64,
-        dom_loads: &[f64],
-        imbal: &[f64],
-    ) {
-        let stat = |idx| stats[idx as usize];
-        let total = stat(bpf_intf::stat_idx_RUSTY_STAT_WAKE_SYNC)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_PREV_IDLE)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_GREEDY_IDLE)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_PINNED)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_DISPATCH)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_GREEDY)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_GREEDY_FAR)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_DSQ_DISPATCH)
-            + stat(bpf_intf::stat_idx_RUSTY_STAT_GREEDY);
-
-        info!(
-            "cpu={:7.2} bal={} load_avg={:8.2} task_err={} lb_data_err={} proc={:?}ms",
-            cpu_busy * 100.0,
-            stats[bpf_intf::stat_idx_RUSTY_STAT_LOAD_BALANCE as usize],
-            load_avg,
-            stats[bpf_intf::stat_idx_RUSTY_STAT_TASK_GET_ERR as usize],
-            self.nr_lb_data_errors,
-            processing_dur.as_millis(),
-        );
-
-        let stat_pct = |idx| stat(idx) as f64 / total as f64 * 100.0;
-
-        info!(
-            "tot={:7} wsync={:5.2} prev_idle={:5.2} greedy_idle={:5.2} pin={:5.2}",
-            total,
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_WAKE_SYNC),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_PREV_IDLE),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_GREEDY_IDLE),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_PINNED),
-        );
-
-        info!(
-            "dir={:5.2} dir_greedy={:5.2} dir_greedy_far={:5.2}",
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_DISPATCH),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_GREEDY),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_DIRECT_GREEDY_FAR),
-        );
-
-        info!(
-            "dsq={:5.2} greedy={:5.2} kick_greedy={:5.2} rep={:5.2}",
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_DSQ_DISPATCH),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_GREEDY),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_KICK_GREEDY),
-            stat_pct(bpf_intf::stat_idx_RUSTY_STAT_REPATRIATE),
-        );
-
-        let ti = &self.skel.bss().tune_input;
-        info!(
-            "direct_greedy_cpumask={}",
-            format_cpumask(&ti.direct_greedy_cpumask, self.top.nr_cpus)
-        );
-        info!(
-            "  kick_greedy_cpumask={}",
-            format_cpumask(&ti.kick_greedy_cpumask, self.top.nr_cpus)
-        );
-
-        for i in 0..self.top.nr_doms {
-            info!(
-                "DOM[{:02}] util={:6.2} load={:8.2} imbal={}",
-                i,
-                self.tuner.dom_utils[i] * 100.0,
-                dom_loads[i],
-                if imbal[i] == 0.0 {
-                    format!("{:9.2}", 0.0)
-                } else {
-                    format!("{:+9.2}", imbal[i])
-                },
-            );
-        }
-    }
-
-    fn lb_step(&mut self) -> Result<()> {
-        let started_at = Instant::now();
-        let bpf_stats = self.read_bpf_stats()?;
-        let cpu_busy = self.get_cpu_busy()?;
-
-        let mut lb = LoadBalancer::new(
-            &mut self.skel,
-            self.top.clone(),
-            self.balanced_kworkers,
-            &mut self.nr_lb_data_errors,
-        );
-
-        lb.read_dom_loads()?;
-        lb.calculate_dom_load_balance()?;
-
-        if self.balance_load {
-            lb.load_balance()?;
-        }
-
-        // Extract fields needed for reporting and drop lb to release
-        // mutable borrows.
-        let (load_avg, dom_loads, imbal) = (lb.load_avg, lb.dom_loads, lb.imbal);
-
-        self.report(
-            &bpf_stats,
-            cpu_busy,
-            Instant::now().duration_since(started_at),
-            load_avg,
-            &dom_loads,
-            &imbal,
-        );
-
-        self.prev_at = started_at;
-        Ok(())
-    }
-
-    fn read_bpf_exit_kind(&mut self) -> i32 {
-        unsafe { std::ptr::read_volatile(&self.skel.bss().exit_kind as *const _) }
-    }
-
-    fn report_bpf_exit_kind(&mut self) -> Result<()> {
-        // Report msg if EXT_OPS_EXIT_ERROR.
-        match self.read_bpf_exit_kind() {
-            0 => Ok(()),
-            etype if etype == 2 => {
-                let cstr = unsafe { CStr::from_ptr(self.skel.bss().exit_msg.as_ptr() as *const _) };
-                let msg = cstr
-                    .to_str()
-                    .context("Failed to convert exit msg to string")
-                    .unwrap();
-                bail!("BPF exit_kind={} msg={}", etype, msg);
-            }
-            etype => {
-                info!("BPF exit_kind={}", etype);
-                Ok(())
-            }
-        }
-    }
-
-    fn run(&mut self, shutdown: Arc<AtomicBool>) -> Result<()> {
-        let now = Instant::now();
-        let mut next_tune_at = now + self.tune_interval;
-        let mut next_sched_at = now + self.sched_interval;
-
-        while !shutdown.load(Ordering::Relaxed) && self.read_bpf_exit_kind() == 0 {
-            let now = Instant::now();
-
-            if now >= next_tune_at {
-                self.tuner.step(&mut self.skel)?;
-                next_tune_at += self.tune_interval;
-                if next_tune_at < now {
-                    next_tune_at = now + self.tune_interval;
-                }
-            }
-
-            if now >= next_sched_at {
-                self.lb_step()?;
-                next_sched_at += self.sched_interval;
-                if next_sched_at < now {
-                    next_sched_at = now + self.sched_interval;
-                }
-            }
-
-            std::thread::sleep(
-                next_sched_at
-                    .min(next_tune_at)
-                    .duration_since(Instant::now()),
-            );
-        }
-
-        self.report_bpf_exit_kind()
-    }
-}
-
-impl<'a> Drop for Scheduler<'a> {
-    fn drop(&mut self) {
-        if let Some(struct_ops) = self.struct_ops.take() {
-            drop(struct_ops);
-        }
-    }
-}
-
-fn main() -> Result<()> {
-    let opts = Opts::parse();
-
-    let llv = match opts.verbose {
-        0 => simplelog::LevelFilter::Info,
-        1 => simplelog::LevelFilter::Debug,
-        _ => simplelog::LevelFilter::Trace,
-    };
-    let mut lcfg = simplelog::ConfigBuilder::new();
-    lcfg.set_time_level(simplelog::LevelFilter::Error)
-        .set_location_level(simplelog::LevelFilter::Off)
-        .set_target_level(simplelog::LevelFilter::Off)
-        .set_thread_level(simplelog::LevelFilter::Off);
-    simplelog::TermLogger::init(
-        llv,
-        lcfg.build(),
-        simplelog::TerminalMode::Stderr,
-        simplelog::ColorChoice::Auto,
-    )?;
-
-    let mut sched = Scheduler::init(&opts)?;
-
-    let shutdown = Arc::new(AtomicBool::new(false));
-    let shutdown_clone = shutdown.clone();
-    ctrlc::set_handler(move || {
-        shutdown_clone.store(true, Ordering::Relaxed);
-    })
-    .context("Error setting Ctrl-C handler")?;
-
-    sched.run(shutdown)
-}
diff --git a/tools/sched_ext/scx_userland.bpf.c b/tools/sched_ext/scx_userland.bpf.c
deleted file mode 100644
index 4cdc3a6fb..000000000
--- a/tools/sched_ext/scx_userland.bpf.c
+++ /dev/null
@@ -1,349 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * A minimal userland scheduler.
- *
- * In terms of scheduling, this provides two different types of behaviors:
- * 1. A global FIFO scheduling order for _any_ tasks that have CPU affinity.
- *    All such tasks are direct-dispatched from the kernel, and are never
- *    enqueued in user space.
- * 2. A primitive vruntime scheduler that is implemented in user space, for all
- *    other tasks.
- *
- * Some parts of this example user space scheduler could be implemented more
- * efficiently using more complex and sophisticated data structures. For
- * example, rather than using BPF_MAP_TYPE_QUEUE's,
- * BPF_MAP_TYPE_{USER_}RINGBUF's could be used for exchanging messages between
- * user space and kernel space. Similarly, we use a simple vruntime-sorted list
- * in user space, but an rbtree could be used instead.
- *
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#include <scx/common.bpf.h>
-#include "scx_userland.h"
-
-/*
- * Maximum amount of tasks enqueued/dispatched between kernel and user-space.
- */
-#define MAX_ENQUEUED_TASKS 4096
-
-char _license[] SEC("license") = "GPL";
-
-const volatile bool switch_partial;
-const volatile s32 usersched_pid;
-
-/* !0 for veristat, set during init */
-const volatile u32 num_possible_cpus = 64;
-
-/* Stats that are printed by user space. */
-u64 nr_failed_enqueues, nr_kernel_enqueues, nr_user_enqueues;
-
-/*
- * Number of tasks that are queued for scheduling.
- *
- * This number is incremented by the BPF component when a task is queued to the
- * user-space scheduler and it must be decremented by the user-space scheduler
- * when a task is consumed.
- */
-volatile u64 nr_queued;
-
-/*
- * Number of tasks that are waiting for scheduling.
- *
- * This number must be updated by the user-space scheduler to keep track if
- * there is still some scheduling work to do.
- */
-volatile u64 nr_scheduled;
-
-struct user_exit_info uei;
-
-/*
- * The map containing tasks that are enqueued in user space from the kernel.
- *
- * This map is drained by the user space scheduler.
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_QUEUE);
-	__uint(max_entries, MAX_ENQUEUED_TASKS);
-	__type(value, struct scx_userland_enqueued_task);
-} enqueued SEC(".maps");
-
-/*
- * The map containing tasks that are dispatched to the kernel from user space.
- *
- * Drained by the kernel in userland_dispatch().
- */
-struct {
-	__uint(type, BPF_MAP_TYPE_QUEUE);
-	__uint(max_entries, MAX_ENQUEUED_TASKS);
-	__type(value, s32);
-} dispatched SEC(".maps");
-
-/* Per-task scheduling context */
-struct task_ctx {
-	bool force_local; /* Dispatch directly to local DSQ */
-};
-
-/* Map that contains task-local storage. */
-struct {
-	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
-	__uint(map_flags, BPF_F_NO_PREALLOC);
-	__type(key, int);
-	__type(value, struct task_ctx);
-} task_ctx_stor SEC(".maps");
-
-/*
- * Flag used to wake-up the user-space scheduler.
- */
-static volatile u32 usersched_needed;
-
-/*
- * Set user-space scheduler wake-up flag (equivalent to an atomic release
- * operation).
- */
-static void set_usersched_needed(void)
-{
-	__sync_fetch_and_or(&usersched_needed, 1);
-}
-
-/*
- * Check and clear user-space scheduler wake-up flag (equivalent to an atomic
- * acquire operation).
- */
-static bool test_and_clear_usersched_needed(void)
-{
-	return __sync_fetch_and_and(&usersched_needed, 0) == 1;
-}
-
-static bool is_usersched_task(const struct task_struct *p)
-{
-	return p->pid == usersched_pid;
-}
-
-static bool keep_in_kernel(const struct task_struct *p)
-{
-	return p->nr_cpus_allowed < num_possible_cpus;
-}
-
-static struct task_struct *usersched_task(void)
-{
-	struct task_struct *p;
-
-	p = bpf_task_from_pid(usersched_pid);
-	/*
-	 * Should never happen -- the usersched task should always be managed
-	 * by sched_ext.
-	 */
-	if (!p)
-		scx_bpf_error("Failed to find usersched task %d", usersched_pid);
-
-	return p;
-}
-
-s32 BPF_STRUCT_OPS(userland_select_cpu, struct task_struct *p,
-		   s32 prev_cpu, u64 wake_flags)
-{
-	if (keep_in_kernel(p)) {
-		s32 cpu;
-		struct task_ctx *tctx;
-
-		tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
-		if (!tctx) {
-			scx_bpf_error("Failed to look up task-local storage for %s", p->comm);
-			return -ESRCH;
-		}
-
-		if (p->nr_cpus_allowed == 1 ||
-		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
-			tctx->force_local = true;
-			return prev_cpu;
-		}
-
-		cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
-		if (cpu >= 0) {
-			tctx->force_local = true;
-			return cpu;
-		}
-	}
-
-	return prev_cpu;
-}
-
-static void dispatch_user_scheduler(void)
-{
-	struct task_struct *p;
-
-	p = usersched_task();
-	if (p) {
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, 0);
-		bpf_task_release(p);
-	}
-}
-
-static void enqueue_task_in_user_space(struct task_struct *p, u64 enq_flags)
-{
-	struct scx_userland_enqueued_task task = {};
-
-	task.pid = p->pid;
-	task.sum_exec_runtime = p->se.sum_exec_runtime;
-	task.weight = p->scx.weight;
-
-	if (bpf_map_push_elem(&enqueued, &task, 0)) {
-		/*
-		 * If we fail to enqueue the task in user space, put it
-		 * directly on the global DSQ.
-		 */
-		__sync_fetch_and_add(&nr_failed_enqueues, 1);
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
-	} else {
-		__sync_fetch_and_add(&nr_user_enqueues, 1);
-		set_usersched_needed();
-	}
-}
-
-void BPF_STRUCT_OPS(userland_enqueue, struct task_struct *p, u64 enq_flags)
-{
-	if (keep_in_kernel(p)) {
-		u64 dsq_id = SCX_DSQ_GLOBAL;
-		struct task_ctx *tctx;
-
-		tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
-		if (!tctx) {
-			scx_bpf_error("Failed to lookup task ctx for %s", p->comm);
-			return;
-		}
-
-		if (tctx->force_local)
-			dsq_id = SCX_DSQ_LOCAL;
-		tctx->force_local = false;
-		scx_bpf_dispatch(p, dsq_id, SCX_SLICE_DFL, enq_flags);
-		__sync_fetch_and_add(&nr_kernel_enqueues, 1);
-		return;
-	} else if (!is_usersched_task(p)) {
-		enqueue_task_in_user_space(p, enq_flags);
-	}
-}
-
-void BPF_STRUCT_OPS(userland_dispatch, s32 cpu, struct task_struct *prev)
-{
-	if (test_and_clear_usersched_needed())
-		dispatch_user_scheduler();
-
-	bpf_repeat(MAX_ENQUEUED_TASKS) {
-		s32 pid;
-		struct task_struct *p;
-
-		if (bpf_map_pop_elem(&dispatched, &pid))
-			break;
-
-		/*
-		 * The task could have exited by the time we get around to
-		 * dispatching it. Treat this as a normal occurrence, and simply
-		 * move onto the next iteration.
-		 */
-		p = bpf_task_from_pid(pid);
-		if (!p)
-			continue;
-
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, 0);
-		bpf_task_release(p);
-	}
-}
-
-/*
- * A CPU is about to change its idle state. If the CPU is going idle, ensure
- * that the user-space scheduler has a chance to run if there is any remaining
- * work to do.
- */
-void BPF_STRUCT_OPS(userland_update_idle, s32 cpu, bool idle)
-{
-	/*
-	 * Don't do anything if we exit from and idle state, a CPU owner will
-	 * be assigned in .running().
-	 */
-	if (!idle)
-		return;
-	/*
-	 * A CPU is now available, notify the user-space scheduler that tasks
-	 * can be dispatched, if there is at least one task waiting to be
-	 * scheduled, either queued (accounted in nr_queued) or scheduled
-	 * (accounted in nr_scheduled).
-	 *
-	 * NOTE: nr_queued is incremented by the BPF component, more exactly in
-	 * enqueue(), when a task is sent to the user-space scheduler, then
-	 * the scheduler drains the queued tasks (updating nr_queued) and adds
-	 * them to its internal data structures / state; at this point tasks
-	 * become "scheduled" and the user-space scheduler will take care of
-	 * updating nr_scheduled accordingly; lastly tasks will be dispatched
-	 * and the user-space scheduler will update nr_scheduled again.
-	 *
-	 * Checking both counters allows to determine if there is still some
-	 * pending work to do for the scheduler: new tasks have been queued
-	 * since last check, or there are still tasks "queued" or "scheduled"
-	 * since the previous user-space scheduler run. If the counters are
-	 * both zero it is pointless to wake-up the scheduler (even if a CPU
-	 * becomes idle), because there is nothing to do.
-	 *
-	 * Keep in mind that update_idle() doesn't run concurrently with the
-	 * user-space scheduler (that is single-threaded): this function is
-	 * naturally serialized with the user-space scheduler code, therefore
-	 * this check here is also safe from a concurrency perspective.
-	 */
-	if (nr_queued || nr_scheduled) {
-		/*
-		 * Kick the CPU to make it immediately ready to accept
-		 * dispatched tasks.
-		 */
-		set_usersched_needed();
-		scx_bpf_kick_cpu(cpu, 0);
-	}
-}
-
-s32 BPF_STRUCT_OPS(userland_init_task, struct task_struct *p,
-		   struct scx_init_task_args *args)
-{
-	if (bpf_task_storage_get(&task_ctx_stor, p, 0,
-				 BPF_LOCAL_STORAGE_GET_F_CREATE))
-		return 0;
-	else
-		return -ENOMEM;
-}
-
-s32 BPF_STRUCT_OPS(userland_init)
-{
-	if (num_possible_cpus == 0) {
-		scx_bpf_error("User scheduler # CPUs uninitialized (%d)",
-			      num_possible_cpus);
-		return -EINVAL;
-	}
-
-	if (usersched_pid <= 0) {
-		scx_bpf_error("User scheduler pid uninitialized (%d)",
-			      usersched_pid);
-		return -EINVAL;
-	}
-
-	if (!switch_partial)
-		scx_bpf_switch_all();
-	return 0;
-}
-
-void BPF_STRUCT_OPS(userland_exit, struct scx_exit_info *ei)
-{
-	uei_record(&uei, ei);
-}
-
-SEC(".struct_ops.link")
-struct sched_ext_ops userland_ops = {
-	.select_cpu		= (void *)userland_select_cpu,
-	.enqueue		= (void *)userland_enqueue,
-	.dispatch		= (void *)userland_dispatch,
-	.update_idle		= (void *)userland_update_idle,
-	.init_task		= (void *)userland_init_task,
-	.init			= (void *)userland_init,
-	.exit			= (void *)userland_exit,
-	.flags			= SCX_OPS_ENQ_LAST | SCX_OPS_KEEP_BUILTIN_IDLE,
-	.timeout_ms		= 3000,
-	.name			= "userland",
-};
diff --git a/tools/sched_ext/scx_userland.c b/tools/sched_ext/scx_userland.c
deleted file mode 100644
index 368acd0b3..000000000
--- a/tools/sched_ext/scx_userland.c
+++ /dev/null
@@ -1,423 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * A demo sched_ext user space scheduler which provides vruntime semantics
- * using a simple ordered-list implementation.
- *
- * Each CPU in the system resides in a single, global domain. This precludes
- * the need to do any load balancing between domains. The scheduler could
- * easily be extended to support multiple domains, with load balancing
- * happening in user space.
- *
- * Any task which has any CPU affinity is scheduled entirely in BPF. This
- * program only schedules tasks which may run on any CPU.
- *
- * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
- * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
- * Copyright (c) 2022 David Vernet <dvernet@meta.com>
- */
-#include <stdio.h>
-#include <unistd.h>
-#include <sched.h>
-#include <signal.h>
-#include <assert.h>
-#include <libgen.h>
-#include <pthread.h>
-#include <bpf/bpf.h>
-#include <sys/mman.h>
-#include <sys/queue.h>
-#include <sys/syscall.h>
-
-#include <scx/common.h>
-#include "scx_userland.h"
-#include "scx_userland.bpf.skel.h"
-
-const char help_fmt[] =
-"A minimal userland sched_ext scheduler.\n"
-"\n"
-"See the top-level comment in .bpf.c for more details.\n"
-"\n"
-"Try to reduce `sysctl kernel.pid_max` if this program triggers OOMs.\n"
-"\n"
-"Usage: %s [-b BATCH] [-p]\n"
-"\n"
-"  -b BATCH      The number of tasks to batch when dispatching (default: 8)\n"
-"  -p            Don't switch all, switch only tasks on SCHED_EXT policy\n"
-"  -h            Display this help and exit\n";
-
-/* Defined in UAPI */
-#define SCHED_EXT 7
-
-/* Number of tasks to batch when dispatching to user space. */
-static __u32 batch_size = 8;
-
-static volatile int exit_req;
-static int enqueued_fd, dispatched_fd;
-
-static struct scx_userland *skel;
-static struct bpf_link *ops_link;
-
-/* Stats collected in user space. */
-static __u64 nr_vruntime_enqueues, nr_vruntime_dispatches, nr_vruntime_failed;
-
-/* Number of tasks currently enqueued. */
-static __u64 nr_curr_enqueued;
-
-/* The data structure containing tasks that are enqueued in user space. */
-struct enqueued_task {
-	LIST_ENTRY(enqueued_task) entries;
-	__u64 sum_exec_runtime;
-	double vruntime;
-};
-
-/*
- * Use a vruntime-sorted list to store tasks. This could easily be extended to
- * a more optimal data structure, such as an rbtree as is done in CFS. We
- * currently elect to use a sorted list to simplify the example for
- * illustrative purposes.
- */
-LIST_HEAD(listhead, enqueued_task);
-
-/*
- * A vruntime-sorted list of tasks. The head of the list contains the task with
- * the lowest vruntime. That is, the task that has the "highest" claim to be
- * scheduled.
- */
-static struct listhead vruntime_head = LIST_HEAD_INITIALIZER(vruntime_head);
-
-/*
- * The main array of tasks. The array is allocated all at once during
- * initialization, based on /proc/sys/kernel/pid_max, to avoid having to
- * dynamically allocate memory on the enqueue path, which could cause a
- * deadlock. A more substantive user space scheduler could e.g. provide a hook
- * for newly enabled tasks that are passed to the scheduler from the
- * .prep_enable() callback to allows the scheduler to allocate on safe paths.
- */
-struct enqueued_task *tasks;
-static int pid_max;
-
-static double min_vruntime;
-
-static void sigint_handler(int userland)
-{
-	exit_req = 1;
-}
-
-static int get_pid_max(void)
-{
-	FILE *fp;
-	int pid_max;
-
-	fp = fopen("/proc/sys/kernel/pid_max", "r");
-	if (fp == NULL) {
-		fprintf(stderr, "Error opening /proc/sys/kernel/pid_max\n");
-		return -1;
-	}
-	if (fscanf(fp, "%d", &pid_max) != 1) {
-		fprintf(stderr, "Error reading from /proc/sys/kernel/pid_max\n");
-		fclose(fp);
-		return -1;
-	}
-	fclose(fp);
-
-	return pid_max;
-}
-
-static int init_tasks(void)
-{
-	pid_max = get_pid_max();
-	if (pid_max < 0)
-		return pid_max;
-
-	tasks = calloc(pid_max, sizeof(*tasks));
-	if (!tasks) {
-		fprintf(stderr, "Error allocating tasks array\n");
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static __u32 task_pid(const struct enqueued_task *task)
-{
-	return ((uintptr_t)task - (uintptr_t)tasks) / sizeof(*task);
-}
-
-static int dispatch_task(__s32 pid)
-{
-	int err;
-
-	err = bpf_map_update_elem(dispatched_fd, NULL, &pid, 0);
-	if (err) {
-		nr_vruntime_failed++;
-	} else {
-		nr_vruntime_dispatches++;
-	}
-
-	return err;
-}
-
-static struct enqueued_task *get_enqueued_task(__s32 pid)
-{
-	if (pid >= pid_max)
-		return NULL;
-
-	return &tasks[pid];
-}
-
-static double calc_vruntime_delta(__u64 weight, __u64 delta)
-{
-	double weight_f = (double)weight / 100.0;
-	double delta_f = (double)delta;
-
-	return delta_f / weight_f;
-}
-
-static void update_enqueued(struct enqueued_task *enqueued, const struct scx_userland_enqueued_task *bpf_task)
-{
-	__u64 delta;
-
-	delta = bpf_task->sum_exec_runtime - enqueued->sum_exec_runtime;
-
-	enqueued->vruntime += calc_vruntime_delta(bpf_task->weight, delta);
-	if (min_vruntime > enqueued->vruntime)
-		enqueued->vruntime = min_vruntime;
-	enqueued->sum_exec_runtime = bpf_task->sum_exec_runtime;
-}
-
-static int vruntime_enqueue(const struct scx_userland_enqueued_task *bpf_task)
-{
-	struct enqueued_task *curr, *enqueued, *prev;
-
-	curr = get_enqueued_task(bpf_task->pid);
-	if (!curr)
-		return ENOENT;
-
-	update_enqueued(curr, bpf_task);
-	nr_vruntime_enqueues++;
-	nr_curr_enqueued++;
-
-	/*
-	 * Enqueue the task in a vruntime-sorted list. A more optimal data
-	 * structure such as an rbtree could easily be used as well. We elect
-	 * to use a list here simply because it's less code, and thus the
-	 * example is less convoluted and better serves to illustrate what a
-	 * user space scheduler could look like.
-	 */
-
-	if (LIST_EMPTY(&vruntime_head)) {
-		LIST_INSERT_HEAD(&vruntime_head, curr, entries);
-		return 0;
-	}
-
-	LIST_FOREACH(enqueued, &vruntime_head, entries) {
-		if (curr->vruntime <= enqueued->vruntime) {
-			LIST_INSERT_BEFORE(enqueued, curr, entries);
-			return 0;
-		}
-		prev = enqueued;
-	}
-
-	LIST_INSERT_AFTER(prev, curr, entries);
-
-	return 0;
-}
-
-static void drain_enqueued_map(void)
-{
-	while (1) {
-		struct scx_userland_enqueued_task task;
-		int err;
-
-		if (bpf_map_lookup_and_delete_elem(enqueued_fd, NULL, &task)) {
-			skel->bss->nr_queued = 0;
-			skel->bss->nr_scheduled = nr_curr_enqueued;
-			return;
-		}
-
-		err = vruntime_enqueue(&task);
-		if (err) {
-			fprintf(stderr, "Failed to enqueue task %d: %s\n",
-				task.pid, strerror(err));
-			exit_req = 1;
-			return;
-		}
-	}
-}
-
-static void dispatch_batch(void)
-{
-	__u32 i;
-
-	for (i = 0; i < batch_size; i++) {
-		struct enqueued_task *task;
-		int err;
-		__s32 pid;
-
-		task = LIST_FIRST(&vruntime_head);
-		if (!task)
-			break;
-
-		min_vruntime = task->vruntime;
-		pid = task_pid(task);
-		LIST_REMOVE(task, entries);
-		err = dispatch_task(pid);
-		if (err) {
-			/*
-			 * If we fail to dispatch, put the task back to the
-			 * vruntime_head list and stop dispatching additional
-			 * tasks in this batch.
-			 */
-			LIST_INSERT_HEAD(&vruntime_head, task, entries);
-			break;
-		}
-		nr_curr_enqueued--;
-	}
-	skel->bss->nr_scheduled = nr_curr_enqueued;
-}
-
-static void *run_stats_printer(void *arg)
-{
-	while (!exit_req) {
-		__u64 nr_failed_enqueues, nr_kernel_enqueues, nr_user_enqueues, total;
-
-		nr_failed_enqueues = skel->bss->nr_failed_enqueues;
-		nr_kernel_enqueues = skel->bss->nr_kernel_enqueues;
-		nr_user_enqueues = skel->bss->nr_user_enqueues;
-		total = nr_failed_enqueues + nr_kernel_enqueues + nr_user_enqueues;
-
-		printf("o-----------------------o\n");
-		printf("| BPF ENQUEUES          |\n");
-		printf("|-----------------------|\n");
-		printf("|  kern:     %10llu |\n", nr_kernel_enqueues);
-		printf("|  user:     %10llu |\n", nr_user_enqueues);
-		printf("|  failed:   %10llu |\n", nr_failed_enqueues);
-		printf("|  -------------------- |\n");
-		printf("|  total:    %10llu |\n", total);
-		printf("|                       |\n");
-		printf("|-----------------------|\n");
-		printf("| VRUNTIME / USER       |\n");
-		printf("|-----------------------|\n");
-		printf("|  enq:      %10llu |\n", nr_vruntime_enqueues);
-		printf("|  disp:     %10llu |\n", nr_vruntime_dispatches);
-		printf("|  failed:   %10llu |\n", nr_vruntime_failed);
-		printf("o-----------------------o\n");
-		printf("\n\n");
-		fflush(stdout);
-		sleep(1);
-	}
-
-	return NULL;
-}
-
-static int spawn_stats_thread(void)
-{
-	pthread_t stats_printer;
-
-	return pthread_create(&stats_printer, NULL, run_stats_printer, NULL);
-}
-
-static void bootstrap(int argc, char **argv)
-{
-	int err;
-	__u32 opt;
-	struct sched_param sched_param = {
-		.sched_priority = sched_get_priority_max(SCHED_EXT),
-	};
-	bool switch_partial = false;
-
-	err = init_tasks();
-	if (err)
-		exit(err);
-
-	signal(SIGINT, sigint_handler);
-	signal(SIGTERM, sigint_handler);
-	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
-
-	/*
-	 * Enforce that the user scheduler task is managed by sched_ext. The
-	 * task eagerly drains the list of enqueued tasks in its main work
-	 * loop, and then yields the CPU. The BPF scheduler only schedules the
-	 * user space scheduler task when at least one other task in the system
-	 * needs to be scheduled.
-	 */
-	err = syscall(__NR_sched_setscheduler, getpid(), SCHED_EXT, &sched_param);
-	SCX_BUG_ON(err, "Failed to set scheduler to SCHED_EXT");
-
-	while ((opt = getopt(argc, argv, "b:ph")) != -1) {
-		switch (opt) {
-		case 'b':
-			batch_size = strtoul(optarg, NULL, 0);
-			break;
-		case 'p':
-			switch_partial = true;
-			break;
-		default:
-			fprintf(stderr, help_fmt, basename(argv[0]));
-			exit(opt != 'h');
-		}
-	}
-
-	/*
-	 * It's not always safe to allocate in a user space scheduler, as an
-	 * enqueued task could hold a lock that we require in order to be able
-	 * to allocate.
-	 */
-	err = mlockall(MCL_CURRENT | MCL_FUTURE);
-	SCX_BUG_ON(err, "Failed to prefault and lock address space");
-
-	skel = scx_userland__open();
-	SCX_BUG_ON(!skel, "Failed to open skel");
-
-	skel->rodata->num_possible_cpus = libbpf_num_possible_cpus();
-	assert(skel->rodata->num_possible_cpus > 0);
-	skel->rodata->usersched_pid = getpid();
-	assert(skel->rodata->usersched_pid > 0);
-	skel->rodata->switch_partial = switch_partial;
-
-	SCX_BUG_ON(scx_userland__load(skel), "Failed to load skel");
-
-	enqueued_fd = bpf_map__fd(skel->maps.enqueued);
-	dispatched_fd = bpf_map__fd(skel->maps.dispatched);
-	assert(enqueued_fd > 0);
-	assert(dispatched_fd > 0);
-
-	SCX_BUG_ON(spawn_stats_thread(), "Failed to spawn stats thread");
-
-	ops_link = bpf_map__attach_struct_ops(skel->maps.userland_ops);
-	SCX_BUG_ON(!ops_link, "Failed to attach struct_ops");
-}
-
-static void sched_main_loop(void)
-{
-	while (!exit_req) {
-		/*
-		 * Perform the following work in the main user space scheduler
-		 * loop:
-		 *
-		 * 1. Drain all tasks from the enqueued map, and enqueue them
-		 *    to the vruntime sorted list.
-		 *
-		 * 2. Dispatch a batch of tasks from the vruntime sorted list
-		 *    down to the kernel.
-		 *
-		 * 3. Yield the CPU back to the system. The BPF scheduler will
-		 *    reschedule the user space scheduler once another task has
-		 *    been enqueued to user space.
-		 */
-		drain_enqueued_map();
-		dispatch_batch();
-		sched_yield();
-	}
-}
-
-int main(int argc, char **argv)
-{
-	bootstrap(argc, argv);
-	sched_main_loop();
-
-	exit_req = 1;
-	bpf_link__destroy(ops_link);
-	uei_print(&skel->bss->uei);
-	scx_userland__destroy(skel);
-	return 0;
-}
diff --git a/tools/sched_ext/scx_userland.h b/tools/sched_ext/scx_userland.h
deleted file mode 100644
index 684fb2dd5..000000000
--- a/tools/sched_ext/scx_userland.h
+++ /dev/null
@@ -1,17 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Copyright (c) 2022 Meta, Inc */
-
-#ifndef __SCX_USERLAND_COMMON_H
-#define __SCX_USERLAND_COMMON_H
-
-/*
- * An instance of a task that has been enqueued by the kernel for consumption
- * by a user space global scheduler thread.
- */
-struct scx_userland_enqueued_task {
-	__s32 pid;
-	u64 sum_exec_runtime;
-	u64 weight;
-};
-
-#endif  // __SCX_USERLAND_COMMON_H
-- 
2.43.0.232.ge79552d197

