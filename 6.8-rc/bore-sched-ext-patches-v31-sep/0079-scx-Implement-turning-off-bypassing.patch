From 103345af6057417c8a5b5105d92b836445542fa1 Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Mon, 8 Jan 2024 10:21:23 -1000
Subject: [PATCH 079/148] scx: Implement turning off bypassing

Bypassing overrides ops.enqueue() and .dispatch() to force global FIFO
behavior. However, this was an irreversible action making it impossible to
turn off bypassing. Instead, add behaviors conditional on
scx_ops_bypassing() to implement global FIFO behavior while bypassing. This
adds two condition checks to hot paths but they're easily predictable and
shouldn't add noticeable overhead.

Signed-off-by: Tejun Heo <tj@kernel.org>
---
 kernel/sched/ext.c | 35 +++++++++++++++--------------------
 1 file changed, 15 insertions(+), 20 deletions(-)

diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index 1256778a6..c7e00baec 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -889,6 +889,13 @@ static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,
 	if (unlikely(!test_rq_online(rq)))
 		goto local;
 
+	if (scx_ops_bypassing()) {
+		if (enq_flags & SCX_ENQ_LAST)
+			goto local;
+		else
+			goto global;
+	}
+
 	/* see %SCX_OPS_ENQ_EXITING */
 	if (!static_branch_unlikely(&scx_ops_enq_exiting) &&
 	    unlikely(p->flags & PF_EXITING))
@@ -1609,7 +1616,7 @@ static int balance_one(struct rq *rq, struct task_struct *prev,
 	if (consume_dispatch_q(rq, rf, &scx_dsq_global))
 		return 1;
 
-	if (!SCX_HAS_OP(dispatch))
+	if (!SCX_HAS_OP(dispatch) || scx_ops_bypassing())
 		return 0;
 
 	dspc->rq = rq;
@@ -3026,16 +3033,6 @@ bool task_should_scx(struct task_struct *p)
 	return p->policy == SCHED_EXT;
 }
 
-static void scx_ops_fallback_enqueue(struct task_struct *p, u64 enq_flags)
-{
-	if (enq_flags & SCX_ENQ_LAST)
-		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, enq_flags);
-	else
-		scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
-}
-
-static void scx_ops_fallback_dispatch(s32 cpu, struct task_struct *prev) {}
-
 /**
  * scx_ops_bypass - [Un]bypass scx_ops and guarantee forward progress
  *
@@ -3048,16 +3045,17 @@ static void scx_ops_fallback_dispatch(s32 cpu, struct task_struct *prev) {}
  * the DISABLING state and then cycling the tasks through dequeue/enqueue to
  * force global FIFO scheduling.
  *
- * a. ops.enqueue() and .dispatch() are overridden for simple global FIFO
- *    scheduling.
+ * a. ops.enqueue() is ignored and tasks are queued in simple global FIFO order.
  *
- * b. balance_scx() never sets %SCX_TASK_BAL_KEEP as the slice value can't be
+ * b. ops.dispatch() is ignored.
+ *
+ * c. balance_scx() never sets %SCX_TASK_BAL_KEEP as the slice value can't be
  *    trusted. Whenever a tick triggers, the running task is rotated to the tail
  *    of the queue with core_sched_at touched.
  *
- * c. pick_next_task() suppresses zero slice warning.
+ * d. pick_next_task() suppresses zero slice warning.
  *
- * d. scx_prio_less() reverts to the default core_sched_at order.
+ * e. scx_prio_less() reverts to the default core_sched_at order.
  */
 static void scx_ops_bypass(bool bypass)
 {
@@ -3070,9 +3068,6 @@ static void scx_ops_bypass(bool bypass)
 		WARN_ON_ONCE(depth <= 0);
 		if (depth != 1)
 			return;
-
-		scx_ops.enqueue = scx_ops_fallback_enqueue;
-		scx_ops.dispatch = scx_ops_fallback_dispatch;
 	} else {
 		depth = atomic_dec_return(&scx_ops_bypass_depth);
 		WARN_ON_ONCE(depth < 0);
@@ -3086,7 +3081,7 @@ static void scx_ops_bypass(bool bypass)
 		if (READ_ONCE(p->__state) != TASK_DEAD) {
 			struct sched_enq_and_set_ctx ctx;
 
-			/* cycling deq/enq is enough, see above */
+			/* cycling deq/enq is enough, see the function comment */
 			sched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);
 			sched_enq_and_set_task(&ctx);
 		}
-- 
2.43.0.232.ge79552d197

