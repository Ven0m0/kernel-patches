From a5891781b9dbadbdac1b8231e0b7d713046a21e1 Mon Sep 17 00:00:00 2001
From: Masahito S <firelzrd@gmail.com>
Date: Fri, 23 May 2025 12:35:48 +0900
Subject: [PATCH 1/2] linux6.15.y-zram-ir-1.2

---
 drivers/block/zram/zram_drv.c | 90 ++++++++++++++++++++++++++++-------
 1 file changed, 72 insertions(+), 18 deletions(-)

diff --git a/drivers/block/zram/zram_drv.c b/drivers/block/zram/zram_drv.c
index fda7d8624..abdf95306 100644
--- a/drivers/block/zram/zram_drv.c
+++ b/drivers/block/zram/zram_drv.c
@@ -60,6 +60,23 @@ static void zram_free_page(struct zram *zram, size_t index);
 static int zram_read_from_zspool(struct zram *zram, struct page *page,
 				 u32 index);
 
+#ifdef CONFIG_ZRAM_MULTI_COMP
+u8 __read_mostly sysctl_zram_recomp_immediate = 1;
+
+static const struct ctl_table zram_sysctl_table[] = {
+	{
+		.procname	= "zram_recomp_immediate",
+		.data		= &sysctl_zram_recomp_immediate,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler	= proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THREE,
+	},
+};
+static struct ctl_table_header *zram_sysctl_table_header;
+#endif //CONFIG_ZRAM_MULTI_COMP
+
 #define slot_dep_map(zram, index) (&(zram)->table[(index)].dep_map)
 
 static void zram_slot_lock_init(struct zram *zram, u32 index)
@@ -1682,7 +1699,7 @@ static int write_same_filled_page(struct zram *zram, unsigned long fill,
 }
 
 static int write_incompressible_page(struct zram *zram, struct page *page,
-				     u32 index)
+				     u32 index, u8 prio)
 {
 	unsigned long handle;
 	void *src;
@@ -1711,6 +1728,7 @@ static int write_incompressible_page(struct zram *zram, struct page *page,
 	zram_set_flag(zram, index, ZRAM_HUGE);
 	zram_set_handle(zram, index, handle);
 	zram_set_obj_size(zram, index, PAGE_SIZE);
+	zram_set_priority(zram, index, prio);
 	zram_slot_unlock(zram, index);
 
 	atomic64_add(PAGE_SIZE, &zram->stats.compr_data_size);
@@ -1727,9 +1745,13 @@ static int zram_write_page(struct zram *zram, struct page *page, u32 index)
 	unsigned long handle;
 	unsigned int comp_len;
 	void *mem;
-	struct zcomp_strm *zstrm;
+	struct zcomp_strm *zstrm = NULL;
 	unsigned long element;
 	bool same_filled;
+	u8 prio, prio_max = zram->num_active_comps;
+#ifdef CONFIG_ZRAM_MULTI_COMP
+	prio_max = min(prio_max, sysctl_zram_recomp_immediate + 1);
+#endif //CONFIG_ZRAM_MULTI_COMP
 
 	/* First, free memory allocated to this slot (if any) */
 	zram_slot_lock(zram, index);
@@ -1742,49 +1764,68 @@ static int zram_write_page(struct zram *zram, struct page *page, u32 index)
 	if (same_filled)
 		return write_same_filled_page(zram, element, index);
 
-	zstrm = zcomp_stream_get(zram->comps[ZRAM_PRIMARY_COMP]);
-	mem = kmap_local_page(page);
-	ret = zcomp_compress(zram->comps[ZRAM_PRIMARY_COMP], zstrm,
-			     mem, &comp_len);
-	kunmap_local(mem);
+	for (prio = ZRAM_PRIMARY_COMP; prio < prio_max; prio++) {
+		if (!zram->comps[prio])
+			continue;
+
+		zstrm = zcomp_stream_get(zram->comps[prio]);
+		mem = kmap_local_page(page);
+		ret = zcomp_compress(zram->comps[prio], zstrm,
+				     mem, &comp_len);
+		kunmap_local(mem);
+
+		if (unlikely(ret)) {
+			pr_err("Compression failed! err=%d\n", ret);
+			goto out;
+		}
+
+		if (comp_len < huge_class_size)
+			break;
 
-	if (unlikely(ret)) {
 		zcomp_stream_put(zstrm);
-		pr_err("Compression failed! err=%d\n", ret);
-		return ret;
+		zstrm = NULL;
 	}
 
-	if (comp_len >= huge_class_size) {
-		zcomp_stream_put(zstrm);
-		return write_incompressible_page(zram, page, index);
+	if (!zstrm) {
+		if (prio >= zram->num_active_comps) {
+			zram_slot_lock(zram, index);
+			zram_set_flag(zram, index, ZRAM_INCOMPRESSIBLE);
+			zram_slot_unlock(zram, index);
+		}
+
+		ret = write_incompressible_page(zram, page, index, prio - 1);
+		goto out;
 	}
 
 	handle = zs_malloc(zram->mem_pool, comp_len,
 			   GFP_NOIO | __GFP_NOWARN |
 			   __GFP_HIGHMEM | __GFP_MOVABLE);
 	if (IS_ERR_VALUE(handle)) {
-		zcomp_stream_put(zstrm);
-		return PTR_ERR((void *)handle);
+		ret = PTR_ERR((void *)handle);
+		goto out;
 	}
 
 	if (!zram_can_store_page(zram)) {
-		zcomp_stream_put(zstrm);
 		zs_free(zram->mem_pool, handle);
-		return -ENOMEM;
+		ret = -ENOMEM;
+		goto out;
 	}
 
 	zs_obj_write(zram->mem_pool, handle, zstrm->buffer, comp_len);
-	zcomp_stream_put(zstrm);
 
 	zram_slot_lock(zram, index);
 	zram_set_handle(zram, index, handle);
 	zram_set_obj_size(zram, index, comp_len);
+	zram_set_priority(zram, index, prio);
 	zram_slot_unlock(zram, index);
 
 	/* Update stats */
 	atomic64_inc(&zram->stats.pages_stored);
 	atomic64_add(comp_len, &zram->stats.compr_data_size);
 
+out:
+	if (zstrm)
+		zcomp_stream_put(zstrm);
 	return ret;
 }
 
@@ -2784,6 +2825,15 @@ static int __init zram_init(void)
 		num_devices--;
 	}
 
+#ifdef CONFIG_ZRAM_MULTI_COMP
+#define ZRAM_IR_VERSION "1.2"
+#define ZRAM_IR_PROGNAME "ZRAM Immediate Recompression (ZRAM-IR)"
+#define ZRAM_IR_AUTHOR   "Masahito Suzuki"
+	printk(KERN_INFO "%s %s by %s\n",
+		ZRAM_IR_PROGNAME, ZRAM_IR_VERSION, ZRAM_IR_AUTHOR);
+	zram_sysctl_table_header = register_sysctl("vm", zram_sysctl_table);
+#endif //CONFIG_ZRAM_MULTI_COMP
+
 	return 0;
 
 out_error:
@@ -2793,6 +2843,10 @@ static int __init zram_init(void)
 
 static void __exit zram_exit(void)
 {
+#ifdef CONFIG_ZRAM_MULTI_COMP
+	unregister_sysctl_table(zram_sysctl_table_header);
+#endif //CONFIG_ZRAM_MULTI_COMP
+
 	destroy_devices();
 }
 
-- 
2.49.0


From 8309d58031756d972806f9aeb04477c0c85d81b7 Mon Sep 17 00:00:00 2001
From: Masahito S <firelzrd@gmail.com>
Date: Mon, 2 Jun 2025 12:38:37 +0900
Subject: [PATCH 2/2] Parallel Swap

---
 include/linux/mmzone.h |   8 +++
 include/linux/swap.h   |   1 +
 mm/page_io.c           | 136 +++++++++++++++++++++++++++++++++++++++++
 mm/vmscan.c            |  14 +++++
 4 files changed, 159 insertions(+)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6ccec1bf2..5d577354f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,8 @@
 #include <linux/page-flags.h>
 #include <linux/local_lock.h>
 #include <linux/zswap.h>
+#include <linux/cpumask.h>
+#include <linux/mempool.h>
 #include <asm/page.h>
 
 /* Free memory management - zoned buddy allocator.  */
@@ -1398,6 +1400,9 @@ typedef struct pglist_data {
 
 	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */
 
+	struct workqueue_struct *swapout_wq;
+	atomic_t swapout_pending_works;
+
 #ifdef CONFIG_COMPACTION
 	int kcompactd_max_order;
 	enum zone_type kcompactd_highest_zoneidx;
@@ -1480,6 +1485,9 @@ typedef struct pglist_data {
 #endif
 } pg_data_t;
 
+extern int  swapout_wq_init(void);
+extern void swapout_wq_destroy(void);
+
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
 #define node_spanned_pages(nid)	(NODE_DATA(nid)->node_spanned_pages)
 
diff --git a/include/linux/swap.h b/include/linux/swap.h
index db46b25a6..1eeaa6a37 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -425,6 +425,7 @@ extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
+extern int sysctl_parallel_swap;
 long remove_mapping(struct address_space *mapping, struct folio *folio);
 
 #ifdef CONFIG_NUMA
diff --git a/mm/page_io.c b/mm/page_io.c
index 4bce19df5..924f322f5 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -233,6 +233,133 @@ static void swap_zeromap_folio_clear(struct folio *folio)
 	}
 }
 
+struct swapout_work {
+	struct work_struct work;
+	struct folio *folio;
+};
+
+mempool_t **swapout_mempool;
+
+int swapout_wq_init(void)
+{
+	int nid;
+	pg_data_t *pgdat;
+
+	swapout_mempool =
+		kcalloc(nr_node_ids, sizeof(mempool_t *), GFP_KERNEL);
+	if (!swapout_mempool)
+		return -ENOMEM;
+
+	for_each_node(nid) {
+		pgdat = NODE_DATA(nid);
+
+		char wq_name[32];
+		snprintf(wq_name, 32, "swapout%d", nid);
+
+		int max_swapout_workers;
+		max_swapout_workers =
+			max(1, (int)cpumask_weight(cpumask_of_node(nid)) - 1);
+		pgdat->swapout_wq = alloc_workqueue(wq_name,
+							WQ_UNBOUND | WQ_MEM_RECLAIM,
+							max_swapout_workers);
+		if (!pgdat->swapout_wq) {
+			pr_err("Failed to create swapout workqueue for node %d\n", nid);
+			goto error;
+		}
+
+		swapout_mempool[nid] = mempool_create_kmalloc_pool(
+			max_swapout_workers, sizeof(struct swapout_work));
+		if (!swapout_mempool[nid]) {
+			pr_err("Failed to create swapout mempool for node %d\n", nid);
+			goto error_wq;
+		}
+	}
+
+	return 0;
+
+error_wq:
+	destroy_workqueue(pgdat->swapout_wq);
+error:
+	kfree(swapout_mempool);
+	return -ENOMEM;
+}
+
+void swapout_wq_destroy(void)
+{
+    int nid;
+    for_each_node(nid) {
+        if (swapout_mempool[nid])
+            mempool_destroy(swapout_mempool[nid]);
+    }
+    kfree(swapout_mempool);
+}
+
+static void swapout_worker(struct work_struct *work)
+{
+	struct swapout_work *swapout_work =
+		container_of(work, struct swapout_work, work);
+	struct folio *folio = swapout_work->folio;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_NONE,
+	};
+
+	if (zswap_store(folio)) {
+		count_mthp_stat(folio_order(folio), MTHP_STAT_ZSWPOUT);
+		folio_unlock(folio);
+	} else {
+		__swap_writepage(folio, &wbc);
+	}
+
+	mempool_free(swapout_work, swapout_mempool[page_to_nid(&folio->page)]);
+
+	atomic_dec(&NODE_DATA(page_to_nid(&folio->page))->swapout_pending_works);
+}
+
+static bool swapout_sched_parallel(struct folio *folio)
+{
+	struct swap_info_struct *sis = swp_swap_info(folio->swap);
+	int nid = numa_node_id();
+	pg_data_t *pgdat = NODE_DATA(nid);
+	unsigned long current_pending;
+	int max_workers;
+	struct swapout_work *work;
+
+	if (!current_is_kswapd())
+		return false;
+
+	if (!folio_test_anon(folio))
+		return false;
+	/*
+	 * This case needs to synchronously return AOP_WRITEPAGE_ACTIVATE
+	 */
+	if (!mem_cgroup_zswap_writeback_enabled(folio_memcg(folio)))
+		return false;
+
+	sis = swp_swap_info(folio->swap);
+	if (!zswap_is_enabled() && !data_race(sis->flags & SWP_SYNCHRONOUS_IO))
+		return false;
+
+	current_pending = atomic_read(&pgdat->swapout_pending_works);
+
+	max_workers = clamp(sysctl_parallel_swap,
+		0, (int)cpumask_weight(cpumask_of_node(nid)) - 1);
+	if (current_pending >= max_workers)
+		return false;
+
+	work = mempool_alloc(swapout_mempool[nid], GFP_ATOMIC);
+	if (!work)
+		return false;
+
+	INIT_WORK(&work->work, swapout_worker);
+	work->folio = folio;
+
+	queue_work(pgdat->swapout_wq, &work->work);
+
+	atomic_inc(&pgdat->swapout_pending_works);
+
+	return true;
+}
+
 /*
  * We may have stale swap cache pages in memory: notice
  * them here and get rid of the unnecessary final write.
@@ -275,6 +402,15 @@ int swap_writepage(struct page *page, struct writeback_control *wbc)
 		 */
 		swap_zeromap_folio_clear(folio);
 	}
+
+	/*
+	 * Compression within zswap and zram might block rmap, unmap
+	 * of both file and anon pages, try to do compression parallel
+	 * if possible
+	 */
+	if (swapout_sched_parallel(folio))
+		return 0;
+
 	if (zswap_store(folio)) {
 		count_mthp_stat(folio_order(folio), MTHP_STAT_ZSWPOUT);
 		folio_unlock(folio);
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 3783e45bf..32ac3743e 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -183,6 +183,8 @@ struct scan_control {
 	struct reclaim_state reclaim_state;
 };
 
+int sysctl_parallel_swap = 1;
+
 #ifdef ARCH_HAS_PREFETCHW
 #define prefetchw_prev_lru_folio(_folio, _base, _field)			\
 	do {								\
@@ -7433,6 +7435,8 @@ void __meminit kswapd_run(int nid)
 		} else {
 			wake_up_process(pgdat->kswapd);
 		}
+
+    	swapout_wq_init();
 	}
 	pgdat_kswapd_unlock(pgdat);
 }
@@ -7451,6 +7455,8 @@ void __meminit kswapd_stop(int nid)
 	if (kswapd) {
 		kthread_stop(kswapd);
 		pgdat->kswapd = NULL;
+
+		swapout_wq_destroy();
 	}
 	pgdat_kswapd_unlock(pgdat);
 }
@@ -7465,6 +7471,14 @@ static const struct ctl_table vmscan_sysctl_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_TWO_HUNDRED,
 	},
+	{
+		.procname	= "parallel_swap",
+		.data		= &sysctl_parallel_swap,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler = proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
 #ifdef CONFIG_NUMA
 	{
 		.procname	= "zone_reclaim_mode",
-- 
2.49.0

