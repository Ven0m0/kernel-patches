From patchwork Tue Nov  8 19:32:03 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Nhat Pham <nphamcs@gmail.com>
X-Patchwork-Id: 13036773
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 37082C4167B
	for <linux-mm@archiver.kernel.org>; Tue,  8 Nov 2022 19:32:13 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id 921706B0072; Tue,  8 Nov 2022 14:32:11 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 8AE706B0075; Tue,  8 Nov 2022 14:32:11 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 65E1D8E0003; Tue,  8 Nov 2022 14:32:11 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0014.hostedemail.com
 [216.40.44.14])
	by kanga.kvack.org (Postfix) with ESMTP id 52DB16B0072
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 14:32:11 -0500 (EST)
Received: from smtpin27.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay07.hostedemail.com (Postfix) with ESMTP id 0B385160574
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:11 +0000 (UTC)
X-FDA: 80111270862.27.CA00442
Received: from mail-pf1-f182.google.com (mail-pf1-f182.google.com
 [209.85.210.182])
	by imf13.hostedemail.com (Postfix) with ESMTP id 7D9D720006
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:10 +0000 (UTC)
Received: by mail-pf1-f182.google.com with SMTP id i3so14663411pfc.11
        for <linux-mm@kvack.org>; Tue, 08 Nov 2022 11:32:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:from:to:cc:subject:date
         :message-id:reply-to;
        bh=TdXYXXujE5c6Z8uTdZ5KokARMRQa3FrJU+bkNzs2RLY=;
        b=HSThM6hFhhVtQ9XEuNUbTWYT7N7QyCqsOWJmCZ+Yc1HgTC5dVqVT5bIrBAjLJjYN4H
         DtZ4Od1NEvWEM7Q8mFdZTbfTJE3GcYDBXEa/wKhZksZQdEXvopPYjROUxytN+JRYnCAm
         PRXAIiGoz9m1Ca7Sa/Y36uQ24ak+Yw9WNj0uVQtUJNufX6MAtOUZYBw9F8TlnQraD6t7
         okXhYKtevjEUpjKI3L3YStKt6jPXNUGnPKtTSWoAEANPeEcaIj34eCx8kbh+b8JlXl30
         /fwr4LcBJksre4uH1ZMCiTg/wkn14O5eKPDZeoJmyIHSZ6IxnevkEL92goAY6m45eBPZ
         Y4KQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=TdXYXXujE5c6Z8uTdZ5KokARMRQa3FrJU+bkNzs2RLY=;
        b=JkxSr+ZxP6l3bE/9GKX3fzWdnK8AiJBEkGJ5vrvIZZ1bYkCUlo8AqMs6ASdkUy3ekz
         ukeGxsqJda/UGblbJxQFnOrD93i7TiHLM5104EW9a4aytGrHb+6waZxolKkm3wXQ/RsY
         02e+RE9J+u6h11eYkB+yt2Ik7YevrzhfRQAVEZ3/VrpJ59L2RLJho900G7GcThzY+V5f
         zBNXRtE7xsHcQ2KwT91IE5OhTwsEsnLrRZDrshRjdtTf95a0Wifm8vMsYXQXAdksSzbS
         uc0pJRagS3hDVyQFNKFsHVmXjE6dzl7LEyzONRWdZtw8HZ4NyffI9H0WeFdcK7QN7U1Q
         Ni8A==
X-Gm-Message-State: ACrzQf0OLtwNPjdhueowN1o2M6CaLmr6+2PPYC8w9Hd7M7eqaeebpr2H
	HeqWzxUOF0ddHqJ1yvwuYxg=
X-Google-Smtp-Source: 
 AMsMyM4hQi95Y9aEwO3gbncH+WrDysFLwztEEQpUWcNqQm7Mtby+jZc9482bPKMIM+RYTC65gLBLzQ==
X-Received: by 2002:a63:5a05:0:b0:434:23a5:a5ca with SMTP id
 o5-20020a635a05000000b0043423a5a5camr49494580pgb.515.1667935929482;
        Tue, 08 Nov 2022 11:32:09 -0800 (PST)
Received: from localhost (fwdproxy-prn-007.fbsv.net.
 [2a03:2880:ff:7::face:b00c])
        by smtp.gmail.com with ESMTPSA id
 a13-20020a63e84d000000b0046ae5cfc3d5sm6054230pgk.61.2022.11.08.11.32.08
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 08 Nov 2022 11:32:09 -0800 (PST)
From: Nhat Pham <nphamcs@gmail.com>
To: akpm@linux-foundation.org
Cc: hannes@cmpxchg.org,
	linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	minchan@kernel.org,
	ngupta@vflare.org,
	senozhatsky@chromium.org,
	sjenning@redhat.com,
	ddstreet@ieee.org,
	vitaly.wool@konsulko.com
Subject: [PATCH v3 1/5] zswap: fix writeback lock ordering for zsmalloc
Date: Tue,  8 Nov 2022 11:32:03 -0800
Message-Id: <20221108193207.3297327-2-nphamcs@gmail.com>
X-Mailer: git-send-email 2.30.2
In-Reply-To: <20221108193207.3297327-1-nphamcs@gmail.com>
References: <20221108193207.3297327-1-nphamcs@gmail.com>
MIME-Version: 1.0
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1667935930; a=rsa-sha256;
	cv=none;
	b=i9VEMXjWcGwsHYq96qVZakrhuSFRz54DrWC2q0KYTD4BmpuhSOJpwq/cRFNYBagovBMcgG
	dsSwJWrgqbQPntsMKJs8Bt+rnfe1f+v2kWVMA5jd/uK7r27TygJQiCSQF92rN1Wl1kflg4
	YfaUWbt+yM2lDa4zIWBK62C9Vgqjshs=
ARC-Authentication-Results: i=1;
	imf13.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=HSThM6hF;
	spf=pass (imf13.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.210.182 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1667935930;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references:dkim-signature;
	bh=TdXYXXujE5c6Z8uTdZ5KokARMRQa3FrJU+bkNzs2RLY=;
	b=7iJy6Ffk1arj6o33lTheEN9jIpK8k6ZrXv4RciWM/47qL5oZGvxyi1oj2VxQ9gYt33GO20
	PbvFaYJK3V/Hyp0IwSv/3lvAQLjE3vH0VoIHibuX0wxZ5JTNGBXAdvj0FRksLHjuvh/46Q
	8hz7cYoqIH7qPFYMM/ATfzHvj8q4eZg=
X-Stat-Signature: 7u1wz44yaqxar4mwo1xqyitutqcqejfu
X-Rspam-User: 
Authentication-Results: imf13.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=HSThM6hF;
	spf=pass (imf13.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.210.182 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
X-Rspamd-Queue-Id: 7D9D720006
X-Rspamd-Server: rspam09
X-HE-Tag: 1667935930-740021
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

From: Johannes Weiner <hannes@cmpxchg.org>

zswap's customary lock order is tree->lock before pool->lock, because
the tree->lock protects the entries' refcount, and the free callbacks in
the backends acquire their respective pool locks to dispatch the backing
object. zsmalloc's map callback takes the pool lock, so zswap must not
grab the tree->lock while a handle is mapped. This currently only
happens during writeback, which isn't implemented for zsmalloc. In
preparation for it, move the tree->lock section out of the mapped entry
section

Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
Signed-off-by: Nhat Pham <nphamcs@gmail.com>
---
 mm/zswap.c | 37 ++++++++++++++++++++-----------------
 1 file changed, 20 insertions(+), 17 deletions(-)

--
2.30.2

diff --git a/mm/zswap.c b/mm/zswap.c
index 2d48fd59cc7a..2d69c1d678fe 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -958,7 +958,7 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 	};

 	if (!zpool_can_sleep_mapped(pool)) {
-		tmp = kmalloc(PAGE_SIZE, GFP_ATOMIC);
+		tmp = kmalloc(PAGE_SIZE, GFP_KERNEL);
 		if (!tmp)
 			return -ENOMEM;
 	}
@@ -968,6 +968,7 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 	swpentry = zhdr->swpentry; /* here */
 	tree = zswap_trees[swp_type(swpentry)];
 	offset = swp_offset(swpentry);
+	zpool_unmap_handle(pool, handle);

 	/* find and ref zswap entry */
 	spin_lock(&tree->lock);
@@ -975,20 +976,12 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 	if (!entry) {
 		/* entry was invalidated */
 		spin_unlock(&tree->lock);
-		zpool_unmap_handle(pool, handle);
 		kfree(tmp);
 		return 0;
 	}
 	spin_unlock(&tree->lock);
 	BUG_ON(offset != entry->offset);

-	src = (u8 *)zhdr + sizeof(struct zswap_header);
-	if (!zpool_can_sleep_mapped(pool)) {
-		memcpy(tmp, src, entry->length);
-		src = tmp;
-		zpool_unmap_handle(pool, handle);
-	}
-
 	/* try to allocate swap cache page */
 	switch (zswap_get_swap_cache_page(swpentry, &page)) {
 	case ZSWAP_SWAPCACHE_FAIL: /* no memory or invalidate happened */
@@ -1006,6 +999,14 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 		acomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);
 		dlen = PAGE_SIZE;

+		zhdr = zpool_map_handle(pool, handle, ZPOOL_MM_RO);
+		src = (u8 *)zhdr + sizeof(struct zswap_header);
+		if (!zpool_can_sleep_mapped(pool)) {
+			memcpy(tmp, src, entry->length);
+			src = tmp;
+			zpool_unmap_handle(pool, handle);
+		}
+
 		mutex_lock(acomp_ctx->mutex);
 		sg_init_one(&input, src, entry->length);
 		sg_init_table(&output, 1);
@@ -1015,6 +1016,11 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 		dlen = acomp_ctx->req->dlen;
 		mutex_unlock(acomp_ctx->mutex);

+		if (!zpool_can_sleep_mapped(pool))
+			kfree(tmp);
+		else
+			zpool_unmap_handle(pool, handle);
+
 		BUG_ON(ret);
 		BUG_ON(dlen != PAGE_SIZE);

@@ -1045,7 +1051,11 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 		zswap_entry_put(tree, entry);
 	spin_unlock(&tree->lock);

-	goto end;
+	return ret;
+
+fail:
+	if (!zpool_can_sleep_mapped(pool))
+		kfree(tmp);

 	/*
 	* if we get here due to ZSWAP_SWAPCACHE_EXIST
@@ -1054,17 +1064,10 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 	* if we free the entry in the following put
 	* it is also okay to return !0
 	*/
-fail:
 	spin_lock(&tree->lock);
 	zswap_entry_put(tree, entry);
 	spin_unlock(&tree->lock);

-end:
-	if (zpool_can_sleep_mapped(pool))
-		zpool_unmap_handle(pool, handle);
-	else
-		kfree(tmp);
-
 	return ret;
 }


From patchwork Tue Nov  8 19:32:04 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Nhat Pham <nphamcs@gmail.com>
X-Patchwork-Id: 13036774
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 9942CC433FE
	for <linux-mm@archiver.kernel.org>; Tue,  8 Nov 2022 19:32:14 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id C7C438E0003; Tue,  8 Nov 2022 14:32:12 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id C545D8E0001; Tue,  8 Nov 2022 14:32:12 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id A64A58E0003; Tue,  8 Nov 2022 14:32:12 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0013.hostedemail.com
 [216.40.44.13])
	by kanga.kvack.org (Postfix) with ESMTP id 9697B8E0001
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 14:32:12 -0500 (EST)
Received: from smtpin13.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay01.hostedemail.com (Postfix) with ESMTP id 56D661C609A
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:12 +0000 (UTC)
X-FDA: 80111270904.13.8264C48
Received: from mail-pg1-f180.google.com (mail-pg1-f180.google.com
 [209.85.215.180])
	by imf22.hostedemail.com (Postfix) with ESMTP id DD655C000E
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:11 +0000 (UTC)
Received: by mail-pg1-f180.google.com with SMTP id s196so14256981pgs.3
        for <linux-mm@kvack.org>; Tue, 08 Nov 2022 11:32:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:from:to:cc:subject:date
         :message-id:reply-to;
        bh=Ub0OYe15qtfzmwShje8nnnhP69fp8NMaFxLpIwoOo2w=;
        b=E3PTLhswytnNvv9nQB34PVcBlwRSj8uZm4eaF0NgUYBbP7JCfjXpP86/ZCuqf6OCNo
         /n6cWg1QG9kKK2/+z0Lx2fceatk0v93g9xoc72bKj/KyoScvJGtoZU/H/hNoe9QUJdOl
         6vxiO6ef0OD09qpT3xAWLJ9rgo2NyGvBb4DFONpC8GsuUWXBZLCGO6crMXZG9nYQlXJT
         cfXxlCtUEkUKIer6L72fXjn/wasR8NmLkcAs7TTz5Jlmqlv05dRb6EN2WXtaveYmiukv
         hm2ZIWNd0NDceeW7PC7sPZ8dsZ0ZxgKiTw4+ji5hTujolTDZw1iKWk2QEwRPK9VhCTvQ
         tN4A==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=Ub0OYe15qtfzmwShje8nnnhP69fp8NMaFxLpIwoOo2w=;
        b=vrY0MdcnjG/1icfRyoy9YRhGWFicQ2aWZT/YlZN/gtyfZefvT0SMZ4uzIgx+9zrxES
         0bBYnD7xeFaiiG+PJGIUg+BE1cl8Si8FFCxPqrP+qfyrptRUjBExDDeBVDtqToGJG5+h
         SdtIjVoM66C0mvPo4Zn+Q6h8qd+A3FAxO03j4b3WSfYXB8NKsGATMlfb6UmczOAH0OOG
         x6Tq2YVkxdfK1cC0BPECq5R+zvZ2J49Hv4lwZunQvqhTI1O9KiC/W18D9mNfLDVL0t7t
         VGDzwoI5gcKfFxiN+KIzA+0f9eq5/bNBXKelgH4jorC4xApE1ad1skUgwE1NfcFXjiJ/
         hvnQ==
X-Gm-Message-State: ACrzQf3mgjpitiCGqz6WfNArnSdrsGJRSSv1d7/205KJaREowVLWHZv7
	8vc6moGyAtIOsu5FVkxnbPk=
X-Google-Smtp-Source: 
 AMsMyM6rr79u7WXn1dQdjRgbkmGb3wkYHYELr93nubNQ0VGjWd27thrzcITxY99m23zTFIKiqHmIiQ==
X-Received: by 2002:a63:5c56:0:b0:464:85bb:8fd9 with SMTP id
 n22-20020a635c56000000b0046485bb8fd9mr47624470pgm.188.1667935930743;
        Tue, 08 Nov 2022 11:32:10 -0800 (PST)
Received: from localhost (fwdproxy-prn-002.fbsv.net.
 [2a03:2880:ff:2::face:b00c])
        by smtp.gmail.com with ESMTPSA id
 t15-20020a17090a6a0f00b00212e60c7d9csm8198201pjj.41.2022.11.08.11.32.10
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 08 Nov 2022 11:32:10 -0800 (PST)
From: Nhat Pham <nphamcs@gmail.com>
To: akpm@linux-foundation.org
Cc: hannes@cmpxchg.org,
	linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	minchan@kernel.org,
	ngupta@vflare.org,
	senozhatsky@chromium.org,
	sjenning@redhat.com,
	ddstreet@ieee.org,
	vitaly.wool@konsulko.com
Subject: [PATCH v3 2/5] zsmalloc: Consolidate zs_pool's migrate_lock and
 size_class's locks
Date: Tue,  8 Nov 2022 11:32:04 -0800
Message-Id: <20221108193207.3297327-3-nphamcs@gmail.com>
X-Mailer: git-send-email 2.30.2
In-Reply-To: <20221108193207.3297327-1-nphamcs@gmail.com>
References: <20221108193207.3297327-1-nphamcs@gmail.com>
MIME-Version: 1.0
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1667935932; a=rsa-sha256;
	cv=none;
	b=V7Boi1IkN57M/E04Eseo44SXehMiY2WaNbLl0ii5UxhlO7XH/EYVcRp78CLJ4qdf1A4aXf
	91RuzG+s4w6kj4NntAjoMGs/iNcYAKeVsVz7MJv2fIqIFAH9pTrji/y6yRww2eV25stnCs
	VrFAAg+vOf9rE/pHyuMZqTxn/ApYRRo=
ARC-Authentication-Results: i=1;
	imf22.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=E3PTLhsw;
	spf=pass (imf22.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.215.180 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1667935931;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references:dkim-signature;
	bh=Ub0OYe15qtfzmwShje8nnnhP69fp8NMaFxLpIwoOo2w=;
	b=mOuZO76IODN7wIodPgmRJSlbToXH8D/GjX8JWM1mDITt/vRqNDJ8SV6FsHuIeY5l8sx9p0
	pRBXxM4wg4sfWEPK+SMblB7vGDsiKQa0Yvl3Fw0Aql/jCZ59D9Rkl9wKbMQLdtIvkyaTn/
	Ra9PdvYofKMfW/b1+KjG4kT4rgn4XWQ=
X-Stat-Signature: jygqskbj7jdfftcn9sg87fpgpggqf89k
X-Rspamd-Queue-Id: DD655C000E
Authentication-Results: imf22.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=E3PTLhsw;
	spf=pass (imf22.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.215.180 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
X-Rspam-User: 
X-Rspamd-Server: rspam11
X-HE-Tag: 1667935931-72435
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

Currently, zsmalloc has a hierarchy of locks, which includes a
pool-level migrate_lock, and a lock for each size class. We have to
obtain both locks in the hotpath in most cases anyway, except for
zs_malloc. This exception will no longer exist when we introduce a LRU
into the zs_pool for the new writeback functionality - we will need to
obtain a pool-level lock to synchronize LRU handling even in zs_malloc.

In preparation for zsmalloc writeback, consolidate these locks into a
single pool-level lock, which drastically reduces the complexity of
synchronization in zsmalloc.

We have also benchmarked the lock consolidation to see the performance
effect of this change on zram.

First, we ran a synthetic FS workload on a server machine with 36 cores
(same machine for all runs), using

fs_mark  -d  ../zram1mnt  -s  100000  -n  2500  -t  32  -k

before and after for btrfs and ext4 on zram (FS usage is 80%).

Here is the result (unit is file/second):

With lock consolidation (btrfs):
Average: 13520.2, Median: 13531.0, Stddev: 137.5961482019028

Without lock consolidation (btrfs):
Average: 13487.2, Median: 13575.0, Stddev: 309.08283679298665

With lock consolidation (ext4):
Average: 16824.4, Median: 16839.0, Stddev: 89.97388510006668

Without lock consolidation (ext4)
Average: 16958.0, Median: 16986.0, Stddev: 194.7370021336469

As you can see, we observe a 0.3% regression for btrfs, and a 0.9%
regression for ext4. This is a small, barely measurable difference in my
opinion.

For a more realistic scenario, we also tries building the kernel on zram.
Here is the time it takes (in seconds):

With lock consolidation (btrfs):
real
Average: 319.6, Median: 320.0, Stddev: 0.8944271909999159
user
Average: 6894.2, Median: 6895.0, Stddev: 25.528415540334656
sys
Average: 521.4, Median: 522.0, Stddev: 1.51657508881031

Without lock consolidation (btrfs):
real
Average: 319.8, Median: 320.0, Stddev: 0.8366600265340756
user
Average: 6896.6, Median: 6899.0, Stddev: 16.04057355583023
sys
Average: 520.6, Median: 521.0, Stddev: 1.140175425099138

With lock consolidation (ext4):
real
Average: 320.0, Median: 319.0, Stddev: 1.4142135623730951
user
Average: 6896.8, Median: 6878.0, Stddev: 28.621670111997307
sys
Average: 521.2, Median: 521.0, Stddev: 1.7888543819998317

Without lock consolidation (ext4)
real
Average: 319.6, Median: 319.0, Stddev: 0.8944271909999159
user
Average: 6886.2, Median: 6887.0, Stddev: 16.93221781102523
sys
Average: 520.4, Median: 520.0, Stddev: 1.140175425099138

The difference is entirely within the noise of a typical run on zram. This
hardly justifies the complexity of maintaining both the pool lock and
the class lock. In fact, for writeback, we would need to introduce yet
another lock to prevent data races on the pool's LRU, further
complicating the lock handling logic. IMHO, it is just better to
collapse all of these into a single pool-level lock.

Suggested-by: Johannes Weiner <hannes@cmpxchg.org>
Signed-off-by: Nhat Pham <nphamcs@gmail.com>
---
 mm/zsmalloc.c | 87 ++++++++++++++++++++++-----------------------------
 1 file changed, 37 insertions(+), 50 deletions(-)

--
2.30.2

diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index d03941cace2c..326faa751f0a 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -33,8 +33,7 @@
 /*
  * lock ordering:
  *	page_lock
- *	pool->migrate_lock
- *	class->lock
+ *	pool->lock
  *	zspage->lock
  */

@@ -192,7 +191,6 @@ static const int fullness_threshold_frac = 4;
 static size_t huge_class_size;

 struct size_class {
-	spinlock_t lock;
 	struct list_head fullness_list[NR_ZS_FULLNESS];
 	/*
 	 * Size of objects stored in this class. Must be multiple
@@ -247,8 +245,7 @@ struct zs_pool {
 #ifdef CONFIG_COMPACTION
 	struct work_struct free_work;
 #endif
-	/* protect page/zspage migration */
-	rwlock_t migrate_lock;
+	spinlock_t lock;
 };

 struct zspage {
@@ -355,7 +352,7 @@ static void cache_free_zspage(struct zs_pool *pool, struct zspage *zspage)
 	kmem_cache_free(pool->zspage_cachep, zspage);
 }

-/* class->lock(which owns the handle) synchronizes races */
+/* pool->lock(which owns the handle) synchronizes races */
 static void record_obj(unsigned long handle, unsigned long obj)
 {
 	*(unsigned long *)handle = obj;
@@ -452,7 +449,7 @@ static __maybe_unused int is_first_page(struct page *page)
 	return PagePrivate(page);
 }

-/* Protected by class->lock */
+/* Protected by pool->lock */
 static inline int get_zspage_inuse(struct zspage *zspage)
 {
 	return zspage->inuse;
@@ -597,13 +594,13 @@ static int zs_stats_size_show(struct seq_file *s, void *v)
 		if (class->index != i)
 			continue;

-		spin_lock(&class->lock);
+		spin_lock(&pool->lock);
 		class_almost_full = zs_stat_get(class, CLASS_ALMOST_FULL);
 		class_almost_empty = zs_stat_get(class, CLASS_ALMOST_EMPTY);
 		obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);
 		obj_used = zs_stat_get(class, OBJ_USED);
 		freeable = zs_can_compact(class);
-		spin_unlock(&class->lock);
+		spin_unlock(&pool->lock);

 		objs_per_zspage = class->objs_per_zspage;
 		pages_used = obj_allocated / objs_per_zspage *
@@ -916,7 +913,7 @@ static void __free_zspage(struct zs_pool *pool, struct size_class *class,

 	get_zspage_mapping(zspage, &class_idx, &fg);

-	assert_spin_locked(&class->lock);
+	assert_spin_locked(&pool->lock);

 	VM_BUG_ON(get_zspage_inuse(zspage));
 	VM_BUG_ON(fg != ZS_EMPTY);
@@ -1247,19 +1244,19 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 	BUG_ON(in_interrupt());

 	/* It guarantees it can get zspage from handle safely */
-	read_lock(&pool->migrate_lock);
+	spin_lock(&pool->lock);
 	obj = handle_to_obj(handle);
 	obj_to_location(obj, &page, &obj_idx);
 	zspage = get_zspage(page);

 	/*
-	 * migration cannot move any zpages in this zspage. Here, class->lock
+	 * migration cannot move any zpages in this zspage. Here, pool->lock
 	 * is too heavy since callers would take some time until they calls
 	 * zs_unmap_object API so delegate the locking from class to zspage
 	 * which is smaller granularity.
 	 */
 	migrate_read_lock(zspage);
-	read_unlock(&pool->migrate_lock);
+	spin_unlock(&pool->lock);

 	class = zspage_class(pool, zspage);
 	off = (class->size * obj_idx) & ~PAGE_MASK;
@@ -1412,8 +1409,8 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
 	size += ZS_HANDLE_SIZE;
 	class = pool->size_class[get_size_class_index(size)];

-	/* class->lock effectively protects the zpage migration */
-	spin_lock(&class->lock);
+	/* pool->lock effectively protects the zpage migration */
+	spin_lock(&pool->lock);
 	zspage = find_get_zspage(class);
 	if (likely(zspage)) {
 		obj = obj_malloc(pool, zspage, handle);
@@ -1421,12 +1418,12 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
 		fix_fullness_group(class, zspage);
 		record_obj(handle, obj);
 		class_stat_inc(class, OBJ_USED, 1);
-		spin_unlock(&class->lock);
+		spin_unlock(&pool->lock);

 		return handle;
 	}

-	spin_unlock(&class->lock);
+	spin_unlock(&pool->lock);

 	zspage = alloc_zspage(pool, class, gfp);
 	if (!zspage) {
@@ -1434,7 +1431,7 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
 		return (unsigned long)ERR_PTR(-ENOMEM);
 	}

-	spin_lock(&class->lock);
+	spin_lock(&pool->lock);
 	obj = obj_malloc(pool, zspage, handle);
 	newfg = get_fullness_group(class, zspage);
 	insert_zspage(class, zspage, newfg);
@@ -1447,7 +1444,7 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)

 	/* We completely set up zspage so mark them as movable */
 	SetZsPageMovable(pool, zspage);
-	spin_unlock(&class->lock);
+	spin_unlock(&pool->lock);

 	return handle;
 }
@@ -1491,16 +1488,14 @@ void zs_free(struct zs_pool *pool, unsigned long handle)
 		return;

 	/*
-	 * The pool->migrate_lock protects the race with zpage's migration
+	 * The pool->lock protects the race with zpage's migration
 	 * so it's safe to get the page from handle.
 	 */
-	read_lock(&pool->migrate_lock);
+	spin_lock(&pool->lock);
 	obj = handle_to_obj(handle);
 	obj_to_page(obj, &f_page);
 	zspage = get_zspage(f_page);
 	class = zspage_class(pool, zspage);
-	spin_lock(&class->lock);
-	read_unlock(&pool->migrate_lock);

 	obj_free(class->size, obj);
 	class_stat_dec(class, OBJ_USED, 1);
@@ -1510,7 +1505,7 @@ void zs_free(struct zs_pool *pool, unsigned long handle)

 	free_zspage(pool, class, zspage);
 out:
-	spin_unlock(&class->lock);
+	spin_unlock(&pool->lock);
 	cache_free_handle(pool, handle);
 }
 EXPORT_SYMBOL_GPL(zs_free);
@@ -1867,16 +1862,12 @@ static int zs_page_migrate(struct page *newpage, struct page *page,
 	pool = zspage->pool;

 	/*
-	 * The pool migrate_lock protects the race between zpage migration
+	 * The pool's lock protects the race between zpage migration
 	 * and zs_free.
 	 */
-	write_lock(&pool->migrate_lock);
+	spin_lock(&pool->lock);
 	class = zspage_class(pool, zspage);

-	/*
-	 * the class lock protects zpage alloc/free in the zspage.
-	 */
-	spin_lock(&class->lock);
 	/* the migrate_write_lock protects zpage access via zs_map_object */
 	migrate_write_lock(zspage);

@@ -1906,10 +1897,9 @@ static int zs_page_migrate(struct page *newpage, struct page *page,
 	replace_sub_page(class, zspage, newpage, page);
 	/*
 	 * Since we complete the data copy and set up new zspage structure,
-	 * it's okay to release migration_lock.
+	 * it's okay to release the pool's lock.
 	 */
-	write_unlock(&pool->migrate_lock);
-	spin_unlock(&class->lock);
+	spin_unlock(&pool->lock);
 	dec_zspage_isolation(zspage);
 	migrate_write_unlock(zspage);

@@ -1964,9 +1954,9 @@ static void async_free_zspage(struct work_struct *work)
 		if (class->index != i)
 			continue;

-		spin_lock(&class->lock);
+		spin_lock(&pool->lock);
 		list_splice_init(&class->fullness_list[ZS_EMPTY], &free_pages);
-		spin_unlock(&class->lock);
+		spin_unlock(&pool->lock);
 	}

 	list_for_each_entry_safe(zspage, tmp, &free_pages, list) {
@@ -1976,9 +1966,9 @@ static void async_free_zspage(struct work_struct *work)
 		get_zspage_mapping(zspage, &class_idx, &fullness);
 		VM_BUG_ON(fullness != ZS_EMPTY);
 		class = pool->size_class[class_idx];
-		spin_lock(&class->lock);
+		spin_lock(&pool->lock);
 		__free_zspage(pool, class, zspage);
-		spin_unlock(&class->lock);
+		spin_unlock(&pool->lock);
 	}
 };

@@ -2039,10 +2029,11 @@ static unsigned long __zs_compact(struct zs_pool *pool,
 	struct zspage *dst_zspage = NULL;
 	unsigned long pages_freed = 0;

-	/* protect the race between zpage migration and zs_free */
-	write_lock(&pool->migrate_lock);
-	/* protect zpage allocation/free */
-	spin_lock(&class->lock);
+	/*
+	 * protect the race between zpage migration and zs_free
+	 * as well as zpage allocation/free
+	 */
+	spin_lock(&pool->lock);
 	while ((src_zspage = isolate_zspage(class, true))) {
 		/* protect someone accessing the zspage(i.e., zs_map_object) */
 		migrate_write_lock(src_zspage);
@@ -2067,7 +2058,7 @@ static unsigned long __zs_compact(struct zs_pool *pool,
 			putback_zspage(class, dst_zspage);
 			migrate_write_unlock(dst_zspage);
 			dst_zspage = NULL;
-			if (rwlock_is_contended(&pool->migrate_lock))
+			if (spin_is_contended(&pool->lock))
 				break;
 		}

@@ -2084,11 +2075,9 @@ static unsigned long __zs_compact(struct zs_pool *pool,
 			pages_freed += class->pages_per_zspage;
 		} else
 			migrate_write_unlock(src_zspage);
-		spin_unlock(&class->lock);
-		write_unlock(&pool->migrate_lock);
+		spin_unlock(&pool->lock);
 		cond_resched();
-		write_lock(&pool->migrate_lock);
-		spin_lock(&class->lock);
+		spin_lock(&pool->lock);
 	}

 	if (src_zspage) {
@@ -2096,8 +2085,7 @@ static unsigned long __zs_compact(struct zs_pool *pool,
 		migrate_write_unlock(src_zspage);
 	}

-	spin_unlock(&class->lock);
-	write_unlock(&pool->migrate_lock);
+	spin_unlock(&pool->lock);

 	return pages_freed;
 }
@@ -2200,7 +2188,7 @@ struct zs_pool *zs_create_pool(const char *name)
 		return NULL;

 	init_deferred_free(pool);
-	rwlock_init(&pool->migrate_lock);
+	spin_lock_init(&pool->lock);

 	pool->name = kstrdup(name, GFP_KERNEL);
 	if (!pool->name)
@@ -2271,7 +2259,6 @@ struct zs_pool *zs_create_pool(const char *name)
 		class->index = i;
 		class->pages_per_zspage = pages_per_zspage;
 		class->objs_per_zspage = objs_per_zspage;
-		spin_lock_init(&class->lock);
 		pool->size_class[i] = class;
 		for (fullness = ZS_EMPTY; fullness < NR_ZS_FULLNESS;
 							fullness++)

From patchwork Tue Nov  8 19:32:05 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Nhat Pham <nphamcs@gmail.com>
X-Patchwork-Id: 13036775
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 0C3B4C43219
	for <linux-mm@archiver.kernel.org>; Tue,  8 Nov 2022 19:32:16 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id C7C408E0005; Tue,  8 Nov 2022 14:32:13 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id C06098E0001; Tue,  8 Nov 2022 14:32:13 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id AA63E8E0005; Tue,  8 Nov 2022 14:32:13 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0010.hostedemail.com
 [216.40.44.10])
	by kanga.kvack.org (Postfix) with ESMTP id 9C5F28E0001
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 14:32:13 -0500 (EST)
Received: from smtpin02.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay04.hostedemail.com (Postfix) with ESMTP id 79F101A0619
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:13 +0000 (UTC)
X-FDA: 80111270946.02.ECDFE83
Received: from mail-pf1-f177.google.com (mail-pf1-f177.google.com
 [209.85.210.177])
	by imf26.hostedemail.com (Postfix) with ESMTP id 0ACEA14000C
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:12 +0000 (UTC)
Received: by mail-pf1-f177.google.com with SMTP id b29so14651496pfp.13
        for <linux-mm@kvack.org>; Tue, 08 Nov 2022 11:32:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:from:to:cc:subject:date
         :message-id:reply-to;
        bh=vtNgGl1ujIcnJmyweLV5m7PzX6xgoQT5EqlkUSI6PN4=;
        b=CyOeV3YR6WRXm9NAlBA+Dp/PY8YjpvxWUqOk9TAlz7g/mObVJI4JA3foNBQN+I+TtX
         VY45KurMLA224PLyhlSrDXngV7pOYZIVDg3ko+rjZXpM/uQuxVsi2UTTY0tU95FDQxvu
         GHIk0rncMmZdZIU6KFwq57ZIwxKCdqXbdbfhioKXu8JCjMlJvm3IGRNLxVkWgV/4MNwB
         9jrbhFl7AD0jGRjMyTuOi2bG7eSt+CoMUIJyIMbHxkVmBFdEdahXRkoC/sgSKNyADbJw
         vyUZiNTTalkkFX+AN4JNIcfz8987oPqJ1uGbWohr3toeoKyna7g7tEYukGg7sATmYTuO
         11+Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=vtNgGl1ujIcnJmyweLV5m7PzX6xgoQT5EqlkUSI6PN4=;
        b=5kIy68Q7U15DDmNY9g6oiEYKSv7UTIgeKGzwWfB7TKH8qiNc1FLwgbFhEoJ5iYOIIW
         T8D5onUv/lfS8SUJcLIRR+OFnloB0S3C61ExyjccDa2wZ8Eh4Xkt8Na1BK8tXIGidZ6W
         WKw4AL5TvTRGUXRJqXtfz1Cp5yEzBO0zc9uN8T1jrnFkfEuZq5CvIim1nDHy96ipO3y2
         qQ5/+L2FApp0fz0+GXKqtSlWh3tsNopw5Nyw+HH2qy4oqn3lzdD+hpIQ8XCp9F5ptiff
         eBITK8hmS5+QlTEObmRN60yoeyf+ZmetbMrqAuEvNquNVj4eWdvfhD0lqmXEbxgPjoy5
         YqKw==
X-Gm-Message-State: ACrzQf2FUwL8NbUiVoehG7gJj4Y8YbNbDPVpKgcTm2r2oQ1awBXmyVNX
	DF9U+sYu3zaM4iFZGE5FNbc=
X-Google-Smtp-Source: 
 AMsMyM7u99nOEGKZb++STMfVqtJU+InEbT6k2C/csUbE3dDzxLEQ/ipP74pO80At2w+h9mXfY8tPjw==
X-Received: by 2002:a63:f50f:0:b0:470:2eca:d84b with SMTP id
 w15-20020a63f50f000000b004702ecad84bmr24556115pgh.55.1667935932076;
        Tue, 08 Nov 2022 11:32:12 -0800 (PST)
Received: from localhost (fwdproxy-prn-016.fbsv.net.
 [2a03:2880:ff:10::face:b00c])
        by smtp.gmail.com with ESMTPSA id
 rj6-20020a17090b3e8600b0020a28156e11sm8354536pjb.26.2022.11.08.11.32.11
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 08 Nov 2022 11:32:11 -0800 (PST)
From: Nhat Pham <nphamcs@gmail.com>
To: akpm@linux-foundation.org
Cc: hannes@cmpxchg.org,
	linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	minchan@kernel.org,
	ngupta@vflare.org,
	senozhatsky@chromium.org,
	sjenning@redhat.com,
	ddstreet@ieee.org,
	vitaly.wool@konsulko.com
Subject: [PATCH v3 3/5] zsmalloc: Add a LRU to zs_pool to keep track of
 zspages in LRU order
Date: Tue,  8 Nov 2022 11:32:05 -0800
Message-Id: <20221108193207.3297327-4-nphamcs@gmail.com>
X-Mailer: git-send-email 2.30.2
In-Reply-To: <20221108193207.3297327-1-nphamcs@gmail.com>
References: <20221108193207.3297327-1-nphamcs@gmail.com>
MIME-Version: 1.0
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1667935933;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references:dkim-signature;
	bh=vtNgGl1ujIcnJmyweLV5m7PzX6xgoQT5EqlkUSI6PN4=;
	b=A0i5k5W5aK7QvRHW1a0g/n5HXDi6AGwJmsaAtu6mnzI616MIPE7vwU9joFWgKvl8A5ks1U
	GJFCCmCG2dg/xDVaKOAR+Y5gszgLYXTKNqYYddi1P/z4CZI0bkYKVOAiOzsvxhLKFJQbul
	NK4bwKjm7YDcl7AsDmpAfssMXg+VPJo=
ARC-Authentication-Results: i=1;
	imf26.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=CyOeV3YR;
	spf=pass (imf26.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.210.177 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1667935933; a=rsa-sha256;
	cv=none;
	b=MFLOz8IbcOK2A29CwYp2CSFutbzPzfjw6ll4X2FxIzgNwWEKiVZsC76rZUG4tWUyklZdXj
	6eKfuWXZup8g5sCtKWVrdlSgvjHj9mQd+8O9Jduvtuq8h7X/xus4xE+iISkunN5HTOKba7
	drWvODEp8UXOAwPVUuq2K2KSRo/CLxw=
X-Rspamd-Queue-Id: 0ACEA14000C
X-Rspam-User: 
X-Rspamd-Server: rspam08
Authentication-Results: imf26.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=CyOeV3YR;
	spf=pass (imf26.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.210.177 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
X-Stat-Signature: 8xdiroxor9gpzup9o978y5cdniyyd51b
X-HE-Tag: 1667935932-425883
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000005, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

This helps determines the coldest zspages as candidates for writeback.

Signed-off-by: Nhat Pham <nphamcs@gmail.com>
---
 mm/zsmalloc.c | 27 +++++++++++++++++++++++++++
 1 file changed, 27 insertions(+)

--
2.30.2

diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index 326faa751f0a..600c40121544 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -239,6 +239,9 @@ struct zs_pool {
 	/* Compact classes */
 	struct shrinker shrinker;

+	/* List tracking the zspages in LRU order by most recently added object */
+	struct list_head lru;
+
 #ifdef CONFIG_ZSMALLOC_STAT
 	struct dentry *stat_dentry;
 #endif
@@ -260,6 +263,10 @@ struct zspage {
 	unsigned int freeobj;
 	struct page *first_page;
 	struct list_head list; /* fullness list */
+
+	/* links the zspage to the lru list in the pool */
+	struct list_head lru;
+
 	struct zs_pool *pool;
 #ifdef CONFIG_COMPACTION
 	rwlock_t lock;
@@ -352,6 +359,16 @@ static void cache_free_zspage(struct zs_pool *pool, struct zspage *zspage)
 	kmem_cache_free(pool->zspage_cachep, zspage);
 }

+/* Moves the zspage to the front of the zspool's LRU */
+static void move_to_front(struct zs_pool *pool, struct zspage *zspage)
+{
+	assert_spin_locked(&pool->lock);
+
+	if (!list_empty(&zspage->lru))
+		list_del(&zspage->lru);
+	list_add(&zspage->lru, &pool->lru);
+}
+
 /* pool->lock(which owns the handle) synchronizes races */
 static void record_obj(unsigned long handle, unsigned long obj)
 {
@@ -953,6 +970,7 @@ static void free_zspage(struct zs_pool *pool, struct size_class *class,
 	}

 	remove_zspage(class, zspage, ZS_EMPTY);
+	list_del(&zspage->lru);
 	__free_zspage(pool, class, zspage);
 }

@@ -998,6 +1016,8 @@ static void init_zspage(struct size_class *class, struct zspage *zspage)
 		off %= PAGE_SIZE;
 	}

+	INIT_LIST_HEAD(&zspage->lru);
+
 	set_freeobj(zspage, 0);
 }

@@ -1418,6 +1438,8 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
 		fix_fullness_group(class, zspage);
 		record_obj(handle, obj);
 		class_stat_inc(class, OBJ_USED, 1);
+		/* Move the zspage to front of pool's LRU */
+		move_to_front(pool, zspage);
 		spin_unlock(&pool->lock);

 		return handle;
@@ -1444,6 +1466,8 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)

 	/* We completely set up zspage so mark them as movable */
 	SetZsPageMovable(pool, zspage);
+	/* Move the zspage to front of pool's LRU */
+	move_to_front(pool, zspage);
 	spin_unlock(&pool->lock);

 	return handle;
@@ -1967,6 +1991,7 @@ static void async_free_zspage(struct work_struct *work)
 		VM_BUG_ON(fullness != ZS_EMPTY);
 		class = pool->size_class[class_idx];
 		spin_lock(&pool->lock);
+		list_del(&zspage->lru);
 		__free_zspage(pool, class, zspage);
 		spin_unlock(&pool->lock);
 	}
@@ -2278,6 +2303,8 @@ struct zs_pool *zs_create_pool(const char *name)
 	 */
 	zs_register_shrinker(pool);

+	INIT_LIST_HEAD(&pool->lru);
+
 	return pool;

 err:

From patchwork Tue Nov  8 19:32:06 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Nhat Pham <nphamcs@gmail.com>
X-Patchwork-Id: 13036776
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id 6BEFAC43217
	for <linux-mm@archiver.kernel.org>; Tue,  8 Nov 2022 19:32:17 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id 4DC508E0006; Tue,  8 Nov 2022 14:32:15 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 415F68E0001; Tue,  8 Nov 2022 14:32:15 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 2B57D8E0006; Tue,  8 Nov 2022 14:32:15 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0010.hostedemail.com
 [216.40.44.10])
	by kanga.kvack.org (Postfix) with ESMTP id 146EE8E0001
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 14:32:15 -0500 (EST)
Received: from smtpin16.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay10.hostedemail.com (Postfix) with ESMTP id A3D85C0938
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:14 +0000 (UTC)
X-FDA: 80111270988.16.E604F25
Received: from mail-pg1-f177.google.com (mail-pg1-f177.google.com
 [209.85.215.177])
	by imf25.hostedemail.com (Postfix) with ESMTP id 48318A0004
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:14 +0000 (UTC)
Received: by mail-pg1-f177.google.com with SMTP id b62so14279304pgc.0
        for <linux-mm@kvack.org>; Tue, 08 Nov 2022 11:32:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:from:to:cc:subject:date
         :message-id:reply-to;
        bh=NcRGiok93yrAI7cvzhwIp5D29F3AFI0/ULm3Nt1q2VU=;
        b=fqWVAbjdw/fNyqUE/etLKdBoXVwrp6pKlcFnwPGKLcOfZBud1HZCf6BK53gHrYdV4g
         IGtamwQbgzqDFuKM78ggN/XUx84npKmZFw7VBbsZ9GFU4jvRffJ7QeuBgCY26lk/XZfJ
         Fo0/BYnSR0oQQHgW7jb+qeC2N6tS06hg+ewVnR12TCYRT2I2Eon1Q03flOwqrDoZx7C1
         0KDKAtbrovbnna4a5n5LUYeCM48kjQQceWfwPa/IuPCChs6JWTSuWBLITjyRGa5wBu0V
         Q0VJr/FlETrNwTiE0In3eUtKGNy2RX9SPuh38w4ZsTDemlolYpXZVDVjeZbQw4BLUMp1
         f8rA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=NcRGiok93yrAI7cvzhwIp5D29F3AFI0/ULm3Nt1q2VU=;
        b=liIrHCxtqMdUV1toUy9VLhjHQ7jTONIRel55uxyRz9zWQ7gniMDmLOv/sNBvtwEib4
         LirasY2sg9EVeyMBHLAYOotyloDHHP0Zrqe117t6xwvX0OedFRCbmSFNxVTasymvgqEs
         GyfMXc+nQVC2EW2NhA4BrNjZF114qY9owOXSxms1OdRzZ7BAV1Yq+o4Y5fhCLGhAaPBE
         nxuaenJgssL0rCRg2ONoYj5CTBGJBKzEwUfU6TqnEKMTVcbXCgOoU5k2AtfFK4LlCjhU
         QMa5fIE7PGVIcq6NKohSiykNqc0A16mrzheGrlb17kg76hQXn0rsgyJF92NCJTR9nRKp
         KVEg==
X-Gm-Message-State: ANoB5pmpqznEQLEzO5K/KKKRWXRINZwkBwnTcUzV+18TB0lTuEO7tINC
	fp1Kumx4vIB9GCilcFheYd239oRCz2Q1aw==
X-Google-Smtp-Source: 
 AA0mqf6iiFu5zDZOHd3n6s6iM4xfMmMFKoWMGjMEb6APqnthOimT7tHF15pQHafzLwYbBJRleC1A4w==
X-Received: by 2002:a05:6a00:1689:b0:56e:d7f4:3aca with SMTP id
 k9-20020a056a00168900b0056ed7f43acamr18175224pfc.55.1667935933322;
        Tue, 08 Nov 2022 11:32:13 -0800 (PST)
Received: from localhost (fwdproxy-prn-002.fbsv.net.
 [2a03:2880:ff:2::face:b00c])
        by smtp.gmail.com with ESMTPSA id
 j21-20020a170902c3d500b00186c9d17af2sm7336166plj.17.2022.11.08.11.32.12
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 08 Nov 2022 11:32:12 -0800 (PST)
From: Nhat Pham <nphamcs@gmail.com>
To: akpm@linux-foundation.org
Cc: hannes@cmpxchg.org,
	linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	minchan@kernel.org,
	ngupta@vflare.org,
	senozhatsky@chromium.org,
	sjenning@redhat.com,
	ddstreet@ieee.org,
	vitaly.wool@konsulko.com
Subject: [PATCH v3 4/5] zsmalloc: Add ops fields to zs_pool to store evict
 handlers
Date: Tue,  8 Nov 2022 11:32:06 -0800
Message-Id: <20221108193207.3297327-5-nphamcs@gmail.com>
X-Mailer: git-send-email 2.30.2
In-Reply-To: <20221108193207.3297327-1-nphamcs@gmail.com>
References: <20221108193207.3297327-1-nphamcs@gmail.com>
MIME-Version: 1.0
ARC-Authentication-Results: i=1;
	imf25.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=fqWVAbjd;
	spf=pass (imf25.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.215.177 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1667935934; a=rsa-sha256;
	cv=none;
	b=4WF5OPYmPszuNM6d0VtYHXzJbYfRJYAAdo0C5kHqjGdY53/vLO157oFroqWL/Tm28CpVCn
	eZ7pGG1euzi9NENmcjy8V5V99YNe5tD1BpGW4+KeUMlCafBvT+W+A608I29MKN+tvYt5U3
	aV8VdOHLDVH3rBeR33OsToe80zyhbqU=
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1667935934;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references:dkim-signature;
	bh=NcRGiok93yrAI7cvzhwIp5D29F3AFI0/ULm3Nt1q2VU=;
	b=L3U9SQ8pzCUJsv9pD1NrNj/Kbu/OdIp0qbIrtngUx0RB2ZHGvee/XkawynvbJ26vR2vwDF
	rXOR4CFsiyby4HfMZiuVCwuo/1b2EQx8dJpYTdTcccd5yb8tuEki8ckv2iVX6aHWA8Dcr1
	GBu8m7ihQVXS7krMg4MndkuAoyulItU=
X-Stat-Signature: 7u3nyfjpuopkttfaxgqm89wcr8mrwgky
X-Rspamd-Queue-Id: 48318A0004
Authentication-Results: imf25.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=fqWVAbjd;
	spf=pass (imf25.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.215.177 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
X-Rspam-User: 
X-Rspamd-Server: rspam06
X-HE-Tag: 1667935934-893768
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

This adds fields to zs_pool to store evict handlers for writeback,
analogous to the zbud allocator.

Signed-off-by: Nhat Pham <nphamcs@gmail.com>
---
 mm/zsmalloc.c | 38 +++++++++++++++++++++++++++++++++++++-
 1 file changed, 37 insertions(+), 1 deletion(-)

--
2.30.2

diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index 600c40121544..ac86cffa62cd 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -225,6 +225,12 @@ struct link_free {
 	};
 };

+struct zs_pool;
+
+struct zs_ops {
+	int (*evict)(struct zs_pool *pool, unsigned long handle);
+};
+
 struct zs_pool {
 	const char *name;

@@ -242,6 +248,12 @@ struct zs_pool {
 	/* List tracking the zspages in LRU order by most recently added object */
 	struct list_head lru;

+#ifdef CONFIG_ZPOOL
+	const struct zs_ops *ops;
+	struct zpool *zpool;
+	const struct zpool_ops *zpool_ops;
+#endif
+
 #ifdef CONFIG_ZSMALLOC_STAT
 	struct dentry *stat_dentry;
 #endif
@@ -379,6 +391,18 @@ static void record_obj(unsigned long handle, unsigned long obj)

 #ifdef CONFIG_ZPOOL

+static int zs_zpool_evict(struct zs_pool *pool, unsigned long handle)
+{
+	if (pool->zpool && pool->zpool_ops && pool->zpool_ops->evict)
+		return pool->zpool_ops->evict(pool->zpool, handle);
+	else
+		return -ENOENT;
+}
+
+static const struct zs_ops zs_zpool_ops = {
+	.evict =	zs_zpool_evict
+};
+
 static void *zs_zpool_create(const char *name, gfp_t gfp,
 			     const struct zpool_ops *zpool_ops,
 			     struct zpool *zpool)
@@ -388,7 +412,19 @@ static void *zs_zpool_create(const char *name, gfp_t gfp,
 	 * different contexts and its caller must provide a valid
 	 * gfp mask.
 	 */
-	return zs_create_pool(name);
+	struct zs_pool *pool = zs_create_pool(name);
+
+	if (pool) {
+		pool->zpool = zpool;
+		pool->zpool_ops = zpool_ops;
+
+		if (zpool_ops)
+			pool->ops = &zs_zpool_ops;
+		else
+			pool->ops = NULL;
+	}
+
+	return pool;
 }

 static void zs_zpool_destroy(void *pool)

From patchwork Tue Nov  8 19:32:07 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Nhat Pham <nphamcs@gmail.com>
X-Patchwork-Id: 13036777
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id ACDD8C4332F
	for <linux-mm@archiver.kernel.org>; Tue,  8 Nov 2022 19:32:18 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id 8F9318E0007; Tue,  8 Nov 2022 14:32:16 -0500 (EST)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 85AB78E0001; Tue,  8 Nov 2022 14:32:16 -0500 (EST)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 660738E0007; Tue,  8 Nov 2022 14:32:16 -0500 (EST)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0013.hostedemail.com
 [216.40.44.13])
	by kanga.kvack.org (Postfix) with ESMTP id 573DF8E0001
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 14:32:16 -0500 (EST)
Received: from smtpin25.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay02.hostedemail.com (Postfix) with ESMTP id 305F3121034
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:16 +0000 (UTC)
X-FDA: 80111271072.25.566040C
Received: from mail-pl1-f176.google.com (mail-pl1-f176.google.com
 [209.85.214.176])
	by imf16.hostedemail.com (Postfix) with ESMTP id 9B0A1180008
	for <linux-mm@kvack.org>; Tue,  8 Nov 2022 19:32:15 +0000 (UTC)
Received: by mail-pl1-f176.google.com with SMTP id l2so15029054pld.13
        for <linux-mm@kvack.org>; Tue, 08 Nov 2022 11:32:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:from:to:cc:subject:date
         :message-id:reply-to;
        bh=d9GdtafRH6I7WJFplWbRa+/LvLJ2SebdW8/3k5nqMJE=;
        b=HeLL9pUugyXL5l0vJmPh14x5FwOgG3uFcYxeXa4LAo6tlK5H6qwemLrVzMepGwwSbL
         1XSM5LB89hqDUXKTn7nCxIW9mS9P+Fe2hhT5km3fqjRlfguiMwcKbiHynvfkmJ9x9eCp
         2UW/Hmjpptv33NN6V41v1KgtmlBiMMniXhTarWPs9j0tpMUKnDeCo6gMrz1XoGZZEINO
         wOOf4F60w7GPAOUa25LvijmcrYvjUa4HjQ8F1QCneyl7ZxCvqTIRTpSrXvWldTTcF0F1
         dZ1PG/HKAd3fjeRPnS9ryrPYz86a9yMqpQJ/JN/wm6FSSywU/aHvaIwS+kHU46rflNAm
         mZRQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20210112;
        h=content-transfer-encoding:mime-version:references:in-reply-to
         :message-id:date:subject:cc:to:from:x-gm-message-state:from:to:cc
         :subject:date:message-id:reply-to;
        bh=d9GdtafRH6I7WJFplWbRa+/LvLJ2SebdW8/3k5nqMJE=;
        b=0litqAWYYxrAB3Vu4M6XBqmQbmKHNpreTCpf58TwWSG4nAD8ErZmZDM5sFypXABnuZ
         TQWnc1lOUi2/NDDdl0A2a/qAJi55mMqHy0AzYMwuaKuD+RMhDgQTGZ9CMWyiWfK3GUNf
         3OTedmsy+kMqHazlJvDZPFJnhRe0AZ8gMyCMZjTXi2cZ+RlsIx5qyNU+WB0bVbDjf5uV
         M5Ephbjqq0Y2L6Ab8rKMXf0bGwlaxU7L6+/k/fMs5xTn7Od8nykeAWezk2tPk7wAgE54
         9RM6xaNvFzICCiyn7sziv6mXvZquawLGIgBN0LKAoJ65CazVrv2anBYGgX4G3Wzo+Qb9
         bmrQ==
X-Gm-Message-State: ACrzQf330KaJuzZLegu31eMZTnhxRGBzAuedyMS1DUbMMAkbnlw4dgSO
	vFq2To8h3IQ5rnJsNaNVWXQ=
X-Google-Smtp-Source: 
 AMsMyM6K4/C2Ik5EpREYYXKM/B0EMSr9bK+hu2eaeAuEVB8LWrEeBb5yowdbVV2ZpD+JBf667t4c4Q==
X-Received: by 2002:a17:902:ced1:b0:186:b18a:d0d5 with SMTP id
 d17-20020a170902ced100b00186b18ad0d5mr58143806plg.60.1667935934513;
        Tue, 08 Nov 2022 11:32:14 -0800 (PST)
Received: from localhost (fwdproxy-prn-023.fbsv.net.
 [2a03:2880:ff:17::face:b00c])
        by smtp.gmail.com with ESMTPSA id
 d16-20020a170902ced000b0017f49b41c12sm7337665plg.173.2022.11.08.11.32.13
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 08 Nov 2022 11:32:14 -0800 (PST)
From: Nhat Pham <nphamcs@gmail.com>
To: akpm@linux-foundation.org
Cc: hannes@cmpxchg.org,
	linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	minchan@kernel.org,
	ngupta@vflare.org,
	senozhatsky@chromium.org,
	sjenning@redhat.com,
	ddstreet@ieee.org,
	vitaly.wool@konsulko.com
Subject: [PATCH v3 5/5] zsmalloc: Implement writeback mechanism for zsmalloc
Date: Tue,  8 Nov 2022 11:32:07 -0800
Message-Id: <20221108193207.3297327-6-nphamcs@gmail.com>
X-Mailer: git-send-email 2.30.2
In-Reply-To: <20221108193207.3297327-1-nphamcs@gmail.com>
References: <20221108193207.3297327-1-nphamcs@gmail.com>
MIME-Version: 1.0
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1667935935;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references:dkim-signature;
	bh=d9GdtafRH6I7WJFplWbRa+/LvLJ2SebdW8/3k5nqMJE=;
	b=I3hlRUENx48xKWuBICEeuakJ6HG1J42fhWVdTOBk3SDm7o5/z57G5U6P1fN4dPGhhkGBdG
	QSfCFO4RAsxPa7kjrbyOmEAa71UAVh0/W/8gLetScCljhJpSl6/z2vgTtAohWY161j6EPV
	l1eZnlV5gSJnuTfFU8jaCte6rlkMGRk=
ARC-Authentication-Results: i=1;
	imf16.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=HeLL9pUu;
	spf=pass (imf16.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.214.176 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1667935935; a=rsa-sha256;
	cv=none;
	b=rYnJuuFuyGQej+tvfTEghiRYXYn7dZHBPme/v8cDAUrSGRCVGFT+1EetkcECp4dJ71wOWN
	5mi/oNnnnK5JG5mSzXGMrVLhfJ8EZF0I5rPtVpA6yz1LvG9rhKbieu7cRROeGzznqo0SMc
	cQOlnNRWyLm4D880JZNxWt70rGT5CqQ=
X-Rspamd-Queue-Id: 9B0A1180008
Authentication-Results: imf16.hostedemail.com;
	dkim=pass header.d=gmail.com header.s=20210112 header.b=HeLL9pUu;
	spf=pass (imf16.hostedemail.com: domain of nphamcs@gmail.com designates
 209.85.214.176 as permitted sender) smtp.mailfrom=nphamcs@gmail.com;
	dmarc=pass (policy=none) header.from=gmail.com
X-Rspamd-Server: rspam10
X-Rspam-User: 
X-Stat-Signature: 6m3zb6jpdpw4appa4ff8wwch496wfzc6
X-HE-Tag: 1667935935-880712
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

This commit adds the writeback mechanism for zsmalloc, analogous to the
zbud allocator. Zsmalloc will attempt to determine the coldest zspage
(i.e least recently used) in the pool, and attempt to write back all the
stored compressed objects via the pool's evict handler.

Signed-off-by: Nhat Pham <nphamcs@gmail.com>
---
 mm/zsmalloc.c | 200 ++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 185 insertions(+), 15 deletions(-)

--
2.30.2

diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index ac86cffa62cd..3868ad3cd038 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -279,10 +279,13 @@ struct zspage {
 	/* links the zspage to the lru list in the pool */
 	struct list_head lru;

+	bool under_reclaim;
+
+	/* list of unfreed handles whose objects have been reclaimed */
+	unsigned long *deferred_handles;
+
 	struct zs_pool *pool;
-#ifdef CONFIG_COMPACTION
 	rwlock_t lock;
-#endif
 };

 struct mapping_area {
@@ -303,10 +306,11 @@ static bool ZsHugePage(struct zspage *zspage)
 	return zspage->huge;
 }

-#ifdef CONFIG_COMPACTION
 static void migrate_lock_init(struct zspage *zspage);
 static void migrate_read_lock(struct zspage *zspage);
 static void migrate_read_unlock(struct zspage *zspage);
+
+#ifdef CONFIG_COMPACTION
 static void migrate_write_lock(struct zspage *zspage);
 static void migrate_write_lock_nested(struct zspage *zspage);
 static void migrate_write_unlock(struct zspage *zspage);
@@ -314,9 +318,6 @@ static void kick_deferred_free(struct zs_pool *pool);
 static void init_deferred_free(struct zs_pool *pool);
 static void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage);
 #else
-static void migrate_lock_init(struct zspage *zspage) {}
-static void migrate_read_lock(struct zspage *zspage) {}
-static void migrate_read_unlock(struct zspage *zspage) {}
 static void migrate_write_lock(struct zspage *zspage) {}
 static void migrate_write_lock_nested(struct zspage *zspage) {}
 static void migrate_write_unlock(struct zspage *zspage) {}
@@ -446,6 +447,27 @@ static void zs_zpool_free(void *pool, unsigned long handle)
 	zs_free(pool, handle);
 }

+static int zs_reclaim_page(struct zs_pool *pool, unsigned int retries);
+
+static int zs_zpool_shrink(void *pool, unsigned int pages,
+			unsigned int *reclaimed)
+{
+	unsigned int total = 0;
+	int ret = -EINVAL;
+
+	while (total < pages) {
+		ret = zs_reclaim_page(pool, 8);
+		if (ret < 0)
+			break;
+		total++;
+	}
+
+	if (reclaimed)
+		*reclaimed = total;
+
+	return ret;
+}
+
 static void *zs_zpool_map(void *pool, unsigned long handle,
 			enum zpool_mapmode mm)
 {
@@ -484,6 +506,7 @@ static struct zpool_driver zs_zpool_driver = {
 	.malloc_support_movable = true,
 	.malloc =		  zs_zpool_malloc,
 	.free =			  zs_zpool_free,
+	.shrink =		  zs_zpool_shrink,
 	.map =			  zs_zpool_map,
 	.unmap =		  zs_zpool_unmap,
 	.total_size =		  zs_zpool_total_size,
@@ -957,6 +980,21 @@ static int trylock_zspage(struct zspage *zspage)
 	return 0;
 }

+/*
+ * Free all the deferred handles whose objects are freed in zs_free.
+ */
+static void free_handles(struct zs_pool *pool, struct zspage *zspage)
+{
+	unsigned long handle = (unsigned long)zspage->deferred_handles;
+
+	while (handle) {
+		unsigned long nxt_handle = handle_to_obj(handle);
+
+		cache_free_handle(pool, handle);
+		handle = nxt_handle;
+	}
+}
+
 static void __free_zspage(struct zs_pool *pool, struct size_class *class,
 				struct zspage *zspage)
 {
@@ -971,6 +1009,9 @@ static void __free_zspage(struct zs_pool *pool, struct size_class *class,
 	VM_BUG_ON(get_zspage_inuse(zspage));
 	VM_BUG_ON(fg != ZS_EMPTY);

+	/* Free all deferred handles from zs_free */
+	free_handles(pool, zspage);
+
 	next = page = get_first_page(zspage);
 	do {
 		VM_BUG_ON_PAGE(!PageLocked(page), page);
@@ -1053,6 +1094,8 @@ static void init_zspage(struct size_class *class, struct zspage *zspage)
 	}

 	INIT_LIST_HEAD(&zspage->lru);
+	zspage->under_reclaim = false;
+	zspage->deferred_handles = NULL;

 	set_freeobj(zspage, 0);
 }
@@ -1474,11 +1517,8 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
 		fix_fullness_group(class, zspage);
 		record_obj(handle, obj);
 		class_stat_inc(class, OBJ_USED, 1);
-		/* Move the zspage to front of pool's LRU */
-		move_to_front(pool, zspage);
-		spin_unlock(&pool->lock);

-		return handle;
+		goto out;
 	}

 	spin_unlock(&pool->lock);
@@ -1502,6 +1542,8 @@ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)

 	/* We completely set up zspage so mark them as movable */
 	SetZsPageMovable(pool, zspage);
+
+out:
 	/* Move the zspage to front of pool's LRU */
 	move_to_front(pool, zspage);
 	spin_unlock(&pool->lock);
@@ -1559,12 +1601,24 @@ void zs_free(struct zs_pool *pool, unsigned long handle)

 	obj_free(class->size, obj);
 	class_stat_dec(class, OBJ_USED, 1);
+
+	if (zspage->under_reclaim) {
+		/*
+		 * Reclaim needs the handles during writeback. It'll free
+		 * them along with the zspage when it's done with them.
+		 *
+		 * Record current deferred handle at the memory location
+		 * whose address is given by handle.
+		 */
+		record_obj(handle, (unsigned long)zspage->deferred_handles);
+		zspage->deferred_handles = (unsigned long *)handle;
+		spin_unlock(&pool->lock);
+		return;
+	}
 	fullness = fix_fullness_group(class, zspage);
-	if (fullness != ZS_EMPTY)
-		goto out;
+	if (fullness == ZS_EMPTY)
+		free_zspage(pool, class, zspage);

-	free_zspage(pool, class, zspage);
-out:
 	spin_unlock(&pool->lock);
 	cache_free_handle(pool, handle);
 }
@@ -1764,7 +1818,7 @@ static enum fullness_group putback_zspage(struct size_class *class,
 	return fullness;
 }

-#ifdef CONFIG_COMPACTION
+#if defined(CONFIG_ZPOOL) || defined(CONFIG_COMPACTION)
 /*
  * To prevent zspage destroy during migration, zspage freeing should
  * hold locks of all pages in the zspage.
@@ -1806,6 +1860,24 @@ static void lock_zspage(struct zspage *zspage)
 	}
 	migrate_read_unlock(zspage);
 }
+#endif /* defined(CONFIG_ZPOOL) || defined(CONFIG_COMPACTION) */
+
+#ifdef CONFIG_ZPOOL
+/*
+ * Unlocks all the pages of the zspage.
+ *
+ * pool->lock must be held before this function is called
+ * to prevent the underlying pages from migrating.
+ */
+static void unlock_zspage(struct zspage *zspage)
+{
+	struct page *page = get_first_page(zspage);
+
+	do {
+		unlock_page(page);
+	} while ((page = get_next_page(page)) != NULL);
+}
+#endif /* CONFIG_ZPOOL */

 static void migrate_lock_init(struct zspage *zspage)
 {
@@ -1822,6 +1894,7 @@ static void migrate_read_unlock(struct zspage *zspage) __releases(&zspage->lock)
 	read_unlock(&zspage->lock);
 }

+#ifdef CONFIG_COMPACTION
 static void migrate_write_lock(struct zspage *zspage)
 {
 	write_lock(&zspage->lock);
@@ -2382,6 +2455,103 @@ void zs_destroy_pool(struct zs_pool *pool)
 }
 EXPORT_SYMBOL_GPL(zs_destroy_pool);

+#ifdef CONFIG_ZPOOL
+static int zs_reclaim_page(struct zs_pool *pool, unsigned int retries)
+{
+	int i, obj_idx, ret = 0;
+	unsigned long handle;
+	struct zspage *zspage;
+	struct page *page;
+	enum fullness_group fullness;
+
+	if (retries == 0 || !pool->ops || !pool->ops->evict)
+		return -EINVAL;
+
+	/* Lock LRU and fullness list */
+	spin_lock(&pool->lock);
+	if (list_empty(&pool->lru)) {
+		spin_unlock(&pool->lock);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < retries; i++) {
+		struct size_class *class;
+
+		zspage = list_last_entry(&pool->lru, struct zspage, lru);
+		list_del(&zspage->lru);
+
+		/* zs_free may free objects, but not the zspage and handles */
+		zspage->under_reclaim = true;
+
+		class = zspage_class(pool, zspage);
+		fullness = get_fullness_group(class, zspage);
+
+		/* Lock out object allocations and object compaction */
+		remove_zspage(class, zspage, fullness);
+
+		spin_unlock(&pool->lock);
+
+		/* Lock backing pages into place */
+		lock_zspage(zspage);
+
+		obj_idx = 0;
+		page = zspage->first_page;
+		while (1) {
+			handle = find_alloced_obj(class, page, &obj_idx);
+			if (!handle) {
+				page = get_next_page(page);
+				if (!page)
+					break;
+				obj_idx = 0;
+				continue;
+			}
+
+			/*
+			 * This will write the object and call
+			 * zs_free.
+			 *
+			 * zs_free will free the object, but the
+			 * under_reclaim flag prevents it from freeing
+			 * the zspage altogether. This is necessary so
+			 * that we can continue working with the
+			 * zspage potentially after the last object
+			 * has been freed.
+			 */
+			ret = pool->ops->evict(pool, handle);
+			if (ret)
+				goto next;
+
+			obj_idx++;
+		}
+
+next:
+		/* For freeing the zspage, or putting it back in the pool and LRU list. */
+		spin_lock(&pool->lock);
+		zspage->under_reclaim = false;
+
+		if (!get_zspage_inuse(zspage)) {
+			/*
+			 * Fullness went stale as zs_free() won't touch it
+			 * while the page is removed from the pool. Fix it
+			 * up for the check in __free_zspage().
+			 */
+			zspage->fullness = ZS_EMPTY;
+
+			__free_zspage(pool, class, zspage);
+			spin_unlock(&pool->lock);
+			return 0;
+		}
+
+		putback_zspage(class, zspage);
+		list_add(&zspage->lru, &pool->lru);
+		unlock_zspage(zspage);
+	}
+
+	spin_unlock(&pool->lock);
+	return -EAGAIN;
+}
+#endif /* CONFIG_ZPOOL */
+
 static int __init zs_init(void)
 {
 	int ret;
