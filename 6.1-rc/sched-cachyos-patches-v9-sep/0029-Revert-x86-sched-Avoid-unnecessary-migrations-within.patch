From 839b04f7f7cf058dcee925610c7afccde185d2e5 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Tue, 6 Dec 2022 18:42:47 +0100
Subject: [PATCH 29/29] Revert "x86/sched: Avoid unnecessary migrations within
 SMT domains"

This reverts commit 9a1a6b6ae2b769a7dcc37094ad37e5e52ff9c864.
---
 arch/x86/kernel/itmt.c         | 30 ++++++++----
 arch/x86/kernel/smpboot.c      |  2 +-
 include/linux/sched.h          |  2 -
 include/linux/sched/topology.h |  2 +-
 kernel/sched/fair.c            | 90 +++++++++++++++++-----------------
 kernel/sched/sched.h           | 11 ++---
 kernel/sched/topology.c        |  6 +--
 7 files changed, 71 insertions(+), 72 deletions(-)

diff --git a/arch/x86/kernel/itmt.c b/arch/x86/kernel/itmt.c
index cb2294496..9ff480e94 100644
--- a/arch/x86/kernel/itmt.c
+++ b/arch/x86/kernel/itmt.c
@@ -167,29 +167,39 @@ void sched_clear_itmt_support(void)
 	mutex_unlock(&itmt_update_mutex);
 }
 
-int arch_asym_cpu_priority(int cpu, bool check_smt)
+int arch_asym_cpu_priority(int cpu)
 {
-	if (!check_smt || sched_smt_siblings_idle(cpu))
-		return per_cpu(sched_core_priority, cpu);
-
-	return 0;
+	return per_cpu(sched_core_priority, cpu);
 }
 
 /**
  * sched_set_itmt_core_prio() - Set CPU priority based on ITMT
- * @prio:	Priority of @cpu
- * @cpu:	The CPU number
+ * @prio:	Priority of cpu core
+ * @core_cpu:	The cpu number associated with the core
  *
  * The pstate driver will find out the max boost frequency
  * and call this function to set a priority proportional
- * to the max boost frequency. CPUs with higher boost
+ * to the max boost frequency. CPU with higher boost
  * frequency will receive higher priority.
  *
  * No need to rebuild sched domain after updating
  * the CPU priorities. The sched domains have no
  * dependency on CPU priorities.
  */
-void sched_set_itmt_core_prio(int prio, int cpu)
+void sched_set_itmt_core_prio(int prio, int core_cpu)
 {
-	per_cpu(sched_core_priority, cpu) = prio;
+	int cpu, i = 1;
+
+	for_each_cpu(cpu, topology_sibling_cpumask(core_cpu)) {
+		int smt_prio;
+
+		/*
+		 * Ensure that the siblings are moved to the end
+		 * of the priority chain and only used when
+		 * all other high priority cpus are out of capacity.
+		 */
+		smt_prio = prio * smp_num_siblings / (i * i);
+		per_cpu(sched_core_priority, cpu) = smt_prio;
+		i++;
+	}
 }
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index c3de98224..3f3ea0287 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -545,7 +545,7 @@ static int x86_core_flags(void)
 #ifdef CONFIG_SCHED_SMT
 static int x86_smt_flags(void)
 {
-	return cpu_smt_flags();
+	return cpu_smt_flags() | x86_sched_itmt_flags();
 }
 #endif
 #ifdef CONFIG_SCHED_CLUSTER
diff --git a/include/linux/sched.h b/include/linux/sched.h
index df5fdb095..23de7fe86 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2423,6 +2423,4 @@ static inline void sched_core_fork(struct task_struct *p) { }
 
 extern void sched_set_stop_task(int cpu, struct task_struct *stop);
 
-extern bool sched_smt_siblings_idle(int cpu);
-
 #endif
diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 87b64b977..816df6cc4 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -63,7 +63,7 @@ static inline int cpu_numa_flags(void)
 }
 #endif
 
-extern int arch_asym_cpu_priority(int cpu, bool check_smt);
+extern int arch_asym_cpu_priority(int cpu);
 
 struct sched_domain_attr {
 	int relax_domain_level;
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 1489a6aeb..3b95a93e1 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -142,11 +142,8 @@ __setup("sched_thermal_decay_shift=", setup_sched_thermal_decay_shift);
 #ifdef CONFIG_SMP
 /*
  * For asym packing, by default the lower numbered CPU has higher priority.
- *
- * When doing ASYM_PACKING at the "MC" or higher domains, architectures may
- * want to check the idle state of the SMT siblngs of @cpu.
  */
-int __weak arch_asym_cpu_priority(int cpu, bool check_smt)
+int __weak arch_asym_cpu_priority(int cpu)
 {
 	return -cpu;
 }
@@ -1052,28 +1049,6 @@ update_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)
  * Scheduling class queueing methods:
  */
 
-static inline bool is_core_idle(int cpu)
-{
-#ifdef CONFIG_SCHED_SMT
-	int sibling;
-
-	for_each_cpu(sibling, cpu_smt_mask(cpu)) {
-		if (cpu == sibling)
-			continue;
-
-		if (!idle_cpu(sibling))
-			return false;
-	}
-#endif
-
-	return true;
-}
-
-bool sched_smt_siblings_idle(int cpu)
-{
-	return is_core_idle(cpu);
-}
-
 #ifdef CONFIG_NUMA
 #define NUMA_IMBALANCE_MIN 2
 
@@ -1717,6 +1692,23 @@ struct numa_stats {
 	int idle_cpu;
 };
 
+static inline bool is_core_idle(int cpu)
+{
+#ifdef CONFIG_SCHED_SMT
+	int sibling;
+
+	for_each_cpu(sibling, cpu_smt_mask(cpu)) {
+		if (cpu == sibling)
+			continue;
+
+		if (!idle_cpu(sibling))
+			return false;
+	}
+#endif
+
+	return true;
+}
+
 struct task_numa_env {
 	struct task_struct *p;
 
@@ -9471,10 +9463,12 @@ static bool asym_smt_can_pull_tasks(int dst_cpu, struct sd_lb_stats *sds,
 				    struct sched_group *sg)
 {
 #ifdef CONFIG_SCHED_SMT
-	bool local_is_smt;
+	bool local_is_smt, sg_is_smt;
 	int sg_busy_cpus;
 
 	local_is_smt = sds->local->flags & SD_SHARE_CPUCAPACITY;
+	sg_is_smt = sg->flags & SD_SHARE_CPUCAPACITY;
+
 	sg_busy_cpus = sgs->group_weight - sgs->idle_cpus;
 
 	if (!local_is_smt) {
@@ -9492,20 +9486,29 @@ static bool asym_smt_can_pull_tasks(int dst_cpu, struct sd_lb_stats *sds,
 		 * can help if it has higher priority and is idle (i.e.,
 		 * it has no running tasks).
 		 */
-		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu, false);
+		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
+	}
+
+	/* @dst_cpu has SMT siblings. */
+
+	if (sg_is_smt) {
+		int local_busy_cpus = sds->local->group_weight -
+				      sds->local_stat.idle_cpus;
+		int busy_cpus_delta = sg_busy_cpus - local_busy_cpus;
+
+		if (busy_cpus_delta == 1)
+			return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
+
+		return false;
 	}
 
 	/*
-	 * @dst_cpu has SMT siblings. Do asym_packing load balancing only if
-	 * all its siblings are idle (moving tasks between physical cores in
-	 * which some SMT siblings are busy results in the same throughput).
-	 *
-	 * If the difference in the number of busy CPUs is two or more, let
-	 * find_busiest_group() take care of it. We only care if @sg has
-	 * exactly one busy CPU. This covers SMT and non-SMT sched groups.
+	 * @sg does not have SMT siblings. Ensure that @sds::local does not end
+	 * up with more than one busy SMT sibling and only pull tasks if there
+	 * are not busy CPUs (i.e., no CPU has running tasks).
 	 */
-	if (sg_busy_cpus == 1 && !sds->local_stat.sum_nr_running)
-		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu, false);
+	if (!sds->local_stat.sum_nr_running)
+		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
 
 	return false;
 #else
@@ -9523,8 +9526,7 @@ sched_asym(struct lb_env *env, struct sd_lb_stats *sds,  struct sg_lb_stats *sgs
 	    (group->flags & SD_SHARE_CPUCAPACITY))
 		return asym_smt_can_pull_tasks(env->dst_cpu, sds, sgs, group);
 
-	/* Neither env::dst_cpu nor group::asym_prefer_cpu have SMT siblings. */
-	return sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu, false);
+	return sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu);
 }
 
 static inline bool
@@ -9690,9 +9692,7 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 
 	case group_asym_packing:
 		/* Prefer to move from lowest priority CPU's work */
-		if (sched_asym_prefer(sg->asym_prefer_cpu,
-				      sds->busiest->asym_prefer_cpu,
-				      false))
+		if (sched_asym_prefer(sg->asym_prefer_cpu, sds->busiest->asym_prefer_cpu))
 			return false;
 		break;
 
@@ -10638,7 +10638,7 @@ static struct rq *find_busiest_queue(struct lb_env *env,
 
 		/* Make sure we only pull tasks from a CPU of lower priority */
 		if ((env->sd->flags & SD_ASYM_PACKING) &&
-		    sched_asym_prefer(i, env->dst_cpu, true) &&
+		    sched_asym_prefer(i, env->dst_cpu) &&
 		    nr_running == 1)
 			continue;
 
@@ -10731,7 +10731,7 @@ asym_active_balance(struct lb_env *env)
 	 * highest priority CPUs.
 	 */
 	return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
-	       sched_asym_prefer(env->dst_cpu, env->src_cpu, true);
+	       sched_asym_prefer(env->dst_cpu, env->src_cpu);
 }
 
 static inline bool
@@ -11467,7 +11467,7 @@ static void nohz_balancer_kick(struct rq *rq)
 		 * around.
 		 */
 		for_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {
-			if (sched_asym_prefer(i, cpu, true)) {
+			if (sched_asym_prefer(i, cpu)) {
 				flags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;
 				goto unlock;
 			}
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2c070ab07..2c89aaa92 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -795,15 +795,10 @@ static inline long se_weight(struct sched_entity *se)
 	return scale_load_down(se->load.weight);
 }
 
-/*
- * Used to compare specific CPUs. Also when comparing the preferred CPU of a
- * sched group or building the sched domains; in such cases checking the state
- * of SMT siblings, if any, is not needed.
- */
-static inline bool sched_asym_prefer(int a, int b, bool check_smt)
+
+static inline bool sched_asym_prefer(int a, int b)
 {
-	return arch_asym_cpu_priority(a, check_smt) >
-	       arch_asym_cpu_priority(b, check_smt);
+	return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
 }
 
 struct perf_domain {
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 33b0ca208..dea9fa39e 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -1282,11 +1282,7 @@ static void init_sched_groups_capacity(int cpu, struct sched_domain *sd)
 		for_each_cpu(cpu, sched_group_span(sg)) {
 			if (max_cpu < 0)
 				max_cpu = cpu;
-			/*
-			 * We want the CPU priorities unaffected by the idle
-			 * state of its SMT siblings, if any.
-			 */
-			else if (sched_asym_prefer(cpu, max_cpu, false))
+			else if (sched_asym_prefer(cpu, max_cpu))
 				max_cpu = cpu;
 		}
 		sg->asym_prefer_cpu = max_cpu;
-- 
2.38.1.473.ga0789512c5

