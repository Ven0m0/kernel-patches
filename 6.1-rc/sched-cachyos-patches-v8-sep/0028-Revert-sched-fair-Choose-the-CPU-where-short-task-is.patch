From affda475b77e1c3c65eee70415a4e4abaf921df9 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Mon, 5 Dec 2022 12:26:08 +0100
Subject: [PATCH 28/28] Revert "sched/fair: Choose the CPU where short task is
 running during wake up"

This reverts commit 48715e923adf25231c3bb37e54f67feed37ae248.
---
 include/linux/sched.h   |  4 ----
 kernel/sched/core.c     |  2 --
 kernel/sched/fair.c     | 27 ---------------------------
 kernel/sched/features.h |  1 -
 4 files changed, 34 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7e2d3d808..df5fdb095 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -558,10 +558,6 @@ struct sched_entity {
 
 	u64				nr_migrations;
 
-	u64				prev_sum_exec_runtime_vol;
-	/* average duration of a task */
-	u64				dur_avg;
-
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	int				depth;
 	struct sched_entity		*parent;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f5b6066ce..a7831ce37 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4372,8 +4372,6 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->se.prev_sum_exec_runtime	= 0;
 	p->se.nr_migrations		= 0;
 	p->se.vruntime			= 0;
-	p->se.dur_avg			= 0;
-	p->se.prev_sum_exec_runtime_vol	= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 039b9518f..1489a6aeb 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6684,16 +6684,6 @@ static int wake_wide(struct task_struct *p)
 	return 1;
 }
 
-/*
- * If a task switches in and then voluntarily relinquishes the
- * CPU quickly, it is regarded as a short duration task.
- */
-static inline int is_short_task(struct task_struct *p)
-{
-	return sched_feat(SIS_SHORT) &&
-		(p->se.dur_avg <= sysctl_sched_min_granularity);
-}
-
 /*
  * The purpose of wake_affine() is to quickly determine on which CPU we can run
  * soonest. For the purpose of speed we only consider the waking and previous
@@ -6730,11 +6720,6 @@ wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 	if (available_idle_cpu(prev_cpu))
 		return prev_cpu;
 
-	/* The only running task is a short duration one. */
-	if (cpu_rq(this_cpu)->nr_running == 1 &&
-	    is_short_task((struct task_struct *)cpu_curr(this_cpu)))
-		return this_cpu;
-
 	return nr_cpumask_bits;
 }
 
@@ -7101,11 +7086,6 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		time = cpu_clock(this);
 	}
 
-	if (!has_idle_core && cpu_rq(target)->nr_running == 1 &&
-	    is_short_task((struct task_struct *)cpu_curr(target)) &&
-	    is_short_task(p))
-		return target;
-
 	if (sched_feat(SIS_UTIL)) {
 		sd_share = rcu_dereference(per_cpu(sd_llc_shared, target));
 		if (sd_share) {
@@ -8214,13 +8194,6 @@ static void put_prev_task_fair(struct rq *rq, struct task_struct *prev)
 	struct sched_entity *se = &prev->se;
 	struct cfs_rq *cfs_rq;
 
-	if (sched_feat(SIS_SHORT) && !prev->on_rq) {
-		u64 this_dur = se->sum_exec_runtime - se->prev_sum_exec_runtime_vol;
-
-		se->prev_sum_exec_runtime_vol = se->sum_exec_runtime;
-		update_avg(&se->dur_avg, this_dur);
-	}
-
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
 		put_prev_entity(cfs_rq, se);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index efdc29c42..ee7f23c76 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -62,7 +62,6 @@ SCHED_FEAT(TTWU_QUEUE, true)
  */
 SCHED_FEAT(SIS_PROP, false)
 SCHED_FEAT(SIS_UTIL, true)
-SCHED_FEAT(SIS_SHORT, true)
 
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
-- 
2.38.1.473.ga0789512c5

