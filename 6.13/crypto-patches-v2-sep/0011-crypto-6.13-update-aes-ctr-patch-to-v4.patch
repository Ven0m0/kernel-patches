From 152ce69e882bdef94eb387bebf8bfbbe9fbc40ad Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 20 Feb 2025 14:52:29 +0100
Subject: [PATCH 11/11] crypto-6.13: update aes-ctr patch to v4

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 arch/x86/crypto/aes-ctr-avx-x86_64.S | 32 ++++++++++++++--------------
 1 file changed, 16 insertions(+), 16 deletions(-)

diff --git a/arch/x86/crypto/aes-ctr-avx-x86_64.S b/arch/x86/crypto/aes-ctr-avx-x86_64.S
index 25cab1d8e..1685d8b24 100644
--- a/arch/x86/crypto/aes-ctr-avx-x86_64.S
+++ b/arch/x86/crypto/aes-ctr-avx-x86_64.S
@@ -228,7 +228,7 @@
 
 // Do all AES rounds on the data in the given AESDATA vectors, excluding the
 // zero-th and last rounds.
-.macro	_aesenc_loop	vecs
+.macro	_aesenc_loop	vecs:vararg
 	mov		KEY, %rax
 1:
 	_vbroadcast128	(%rax), RNDKEY
@@ -244,7 +244,7 @@
 // AES round, then XOR those keystream blocks with the corresponding data.
 // Reduce latency by doing the XOR before the vaesenclast, utilizing the
 // property vaesenclast(key, a) ^ b == vaesenclast(key ^ b, a).
-.macro	_aesenclast_and_xor	vecs
+.macro	_aesenclast_and_xor	vecs:vararg
 .irp i, \vecs
 	_vpxor		\i*VL(SRC), RNDKEYLAST, RNDKEY
 	vaesenclast	RNDKEY, AESDATA\i, AESDATA\i
@@ -256,7 +256,7 @@
 
 // XOR the keystream blocks in the specified AESDATA vectors with the
 // corresponding data.
-.macro	_xor_data	vecs
+.macro	_xor_data	vecs:vararg
 .irp i, \vecs
 	_vpxor		\i*VL(SRC), AESDATA\i, AESDATA\i
 .endr
@@ -379,8 +379,8 @@
 	_prepare_2_ctr_vecs	\is_xctr, 2, 3
 	_prepare_2_ctr_vecs	\is_xctr, 4, 5
 	_prepare_2_ctr_vecs	\is_xctr, 6, 7
-	_aesenc_loop	"0,1,2,3,4,5,6,7"
-	_aesenclast_and_xor "0,1,2,3,4,5,6,7"
+	_aesenc_loop	0,1,2,3,4,5,6,7
+	_aesenclast_and_xor 0,1,2,3,4,5,6,7
 	sub		$-8*VL, SRC
 	sub		$-8*VL, DST
 	add		$-8*VL, LEN
@@ -401,8 +401,8 @@
 	// first 4 to XOR 4 full vectors of data.  Then XOR the remaining data.
 	_prepare_2_ctr_vecs	\is_xctr, 4, 5
 	_prepare_2_ctr_vecs	\is_xctr, 6, 7, final=1
-	_aesenc_loop	"0,1,2,3,4,5,6,7"
-	_aesenclast_and_xor "0,1,2,3"
+	_aesenc_loop	0,1,2,3,4,5,6,7
+	_aesenclast_and_xor 0,1,2,3
 	vaesenclast	RNDKEYLAST, AESDATA4, AESDATA0
 	vaesenclast	RNDKEYLAST, AESDATA5, AESDATA1
 	vaesenclast	RNDKEYLAST, AESDATA6, AESDATA2
@@ -412,16 +412,16 @@
 	add		$-4*VL, LEN
 	cmp		$1*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_0\@
-	_xor_data	"0"
+	_xor_data	0
 	cmp		$2*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_1\@
-	_xor_data	"1"
+	_xor_data	1
 	cmp		$3*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_2\@
-	_xor_data	"2"
+	_xor_data	2
 	cmp		$4*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_3\@
-	_xor_data	"3"
+	_xor_data	3
 	jmp		.Ldone\@
 
 .Lenc_tail_atmost4vecs\@:
@@ -430,8 +430,8 @@
 
 	// 2*VL < LEN <= 4*VL.  Generate 4 vectors of keystream blocks.  Use the
 	// first 2 to XOR 2 full vectors of data.  Then XOR the remaining data.
-	_aesenc_loop	"0,1,2,3"
-	_aesenclast_and_xor "0,1"
+	_aesenc_loop	0,1,2,3
+	_aesenclast_and_xor 0,1
 	vaesenclast	RNDKEYLAST, AESDATA2, AESDATA0
 	vaesenclast	RNDKEYLAST, AESDATA3, AESDATA1
 	sub		$-2*VL, SRC
@@ -442,17 +442,17 @@
 .Lenc_tail_atmost2vecs\@:
 	// 1 <= LEN <= 2*VL.  Generate 2 vectors of keystream blocks.  Then XOR
 	// the remaining data.
-	_aesenc_loop	"0,1"
+	_aesenc_loop	0,1
 	vaesenclast	RNDKEYLAST, AESDATA0, AESDATA0
 	vaesenclast	RNDKEYLAST, AESDATA1, AESDATA1
 
 .Lxor_tail_upto2vecs\@:
 	cmp		$1*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_0\@
-	_xor_data	"0"
+	_xor_data	0
 	cmp		$2*VL-1, LEN
 	jle		.Lxor_tail_partial_vec_1\@
-	_xor_data	"1"
+	_xor_data	1
 	jmp		.Ldone\@
 
 .Lxor_tail_partial_vec_1\@:
-- 
2.48.1

