From b65e709adc2c958eb7c968bdcdb66ef9309b2327 Mon Sep 17 00:00:00 2001
From: Peter Zijlstra <peterz@infradead.org>
Date: Tue, 26 Sep 2023 21:58:41 +0200
Subject: [PATCH 8/9] sched/eevdf: Bias the Eligibility test

In order to make things a little easier to schedule, while avoiding
degenerate cases like (which happens the eligibility test is entirely
disabled):

t=92 V=16
 A                   |----<
 B                    |<
>C   |----------------<
 D                    |<
 E                   |<
 F                    |<
 G                   |<
   |---------|-----*---|---------|---------|----

Add one weighted average slice to V when determining eligibility.

XXX needs more testing...

  slice 30000000
  # Min Latencies: 00068
  # Avg Latencies: 255743
  # Max Latencies: 955057
  slice 3000000
  # Min Latencies: 00044
  # Avg Latencies: 00691
  # Max Latencies: 13417
  slice 300000
  # Min Latencies: 00011
  # Avg Latencies: 00060
  # Max Latencies: 02257

Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
---
 kernel/sched/fair.c     | 10 ++++++++++
 kernel/sched/features.h |  1 +
 kernel/sched/sched.h    |  1 +
 3 files changed, 12 insertions(+)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index ff8f91dd9..e68c30e6e 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -642,6 +642,7 @@ avg_vruntime_add(struct cfs_rq *cfs_rq, struct sched_entity *se)
 	s64 key = entity_key(cfs_rq, se);
 
 	cfs_rq->avg_vruntime += key * weight;
+	cfs_rq->avg_vslice += scale_load(se->slice); /* vslice = slice/weight */
 	cfs_rq->avg_load += weight;
 }
 
@@ -652,6 +653,7 @@ avg_vruntime_sub(struct cfs_rq *cfs_rq, struct sched_entity *se)
 	s64 key = entity_key(cfs_rq, se);
 
 	cfs_rq->avg_vruntime -= key * weight;
+	cfs_rq->avg_vslice -= scale_load(se->slice);
 	cfs_rq->avg_load -= weight;
 }
 
@@ -739,15 +741,23 @@ int entity_eligible(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
 	struct sched_entity *curr = cfs_rq->curr;
 	s64 avg = cfs_rq->avg_vruntime;
+	u64 vslice = cfs_rq->avg_vslice;
 	long load = cfs_rq->avg_load;
 
 	if (curr && curr->on_rq) {
 		unsigned long weight = scale_load_down(curr->load.weight);
 
 		avg += entity_key(cfs_rq, curr) * weight;
+		vslice += scale_load(curr->slice);
 		load += weight;
 	}
 
+	/*
+	 * Push V right by one weighted average vslice.
+	 */
+	if (sched_feat(BIAS_ELIGIBLE))
+		avg += vslice;
+
 	return avg >= entity_key(cfs_rq, se) * load;
 }
 
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index d275cd351..99c291a10 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -10,6 +10,7 @@ SCHED_FEAT(RUN_TO_PARITY, true)
 SCHED_FEAT(PREEMPT_SHORT, true)
 SCHED_FEAT(PLACE_SLEEPER, false)
 SCHED_FEAT(GENTLE_SLEEPER, true)
+SCHED_FEAT(BIAS_ELIGIBLE, true)
 
 /*
  * Prefer to schedule the task we woke last (assuming it failed
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 94f0cd560..bd1f63d2c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -551,6 +551,7 @@ struct cfs_rq {
 	unsigned int		idle_h_nr_running; /* SCHED_IDLE */
 
 	s64			avg_vruntime;
+	u64			avg_vslice;
 	u64			avg_load;
 
 	u64			exec_clock;
-- 
2.42.0

