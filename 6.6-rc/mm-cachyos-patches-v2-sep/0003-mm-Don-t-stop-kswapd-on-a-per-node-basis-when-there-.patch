From 5f90f39a9cba4566f1c2c795b09732fb1eabc33f Mon Sep 17 00:00:00 2001
From: Sultan Alsawaf <sultan@kerneltoast.com>
Date: Wed, 20 May 2020 09:55:17 -0700
Subject: [PATCH 3/9] mm: Don't stop kswapd on a per-node basis when there are
 no waiters

The page allocator wakes all kswapds in an allocation context's allowed
nodemask in the slow path, so it doesn't make sense to have the kswapd-
waiter count per each NUMA node. Instead, it should be a global counter
to stop all kswapds when there are no failed allocation requests.

Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
---
 include/linux/mmzone.h | 1 -
 mm/internal.h          | 1 +
 mm/mm_init.c           | 1 -
 mm/page_alloc.c        | 7 ++++---
 mm/vmscan.c            | 2 +-
 5 files changed, 6 insertions(+), 6 deletions(-)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7c01b84ba..4106fbc5b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1300,7 +1300,6 @@ typedef struct pglist_data {
 	unsigned long node_spanned_pages; /* total size of physical page
 					     range, including holes */
 	int node_id;
-	atomic_t kswapd_waiters;
 	wait_queue_head_t kswapd_wait;
 	wait_queue_head_t pfmemalloc_wait;
 
diff --git a/mm/internal.h b/mm/internal.h
index 30cf724dd..d7a7cd532 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -439,6 +439,7 @@ extern void prep_compound_page(struct page *page, unsigned int order);
 extern void post_alloc_hook(struct page *page, unsigned int order,
 					gfp_t gfp_flags);
 extern int user_min_free_kbytes;
+extern atomic_long_t kswapd_waiters;
 
 extern void free_unref_page(struct page *page, unsigned int order);
 extern void free_unref_page_list(struct list_head *list);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index fe1969465..50f2f3474 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1361,7 +1361,6 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
 
 	pgdat_page_ext_init(pgdat);
 	lruvec_init(&pgdat->__lruvec);
-	pgdat->kswapd_waiters = (atomic_t)ATOMIC_INIT(0);
 }
 
 static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0988bdebf..5fe9577ec 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -204,6 +204,8 @@ EXPORT_SYMBOL(node_states);
 
 gfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;
 
+atomic_long_t kswapd_waiters = ATOMIC_LONG_INIT(0);
+
 /*
  * A cached value of the page's pageblock's migratetype, used when the page is
  * put on a pcplist. Used to avoid the pageblock migratetype lookup when
@@ -3911,7 +3913,6 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	unsigned int cpuset_mems_cookie;
 	unsigned int zonelist_iter_cookie;
 	int reserve_flags;
-	pg_data_t *pgdat = ac->preferred_zoneref->zone->zone_pgdat;
 	bool woke_kswapd = false;
 
 restart:
@@ -3954,7 +3955,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 
 	if (alloc_flags & ALLOC_KSWAPD) {
 		if (!woke_kswapd) {
-			atomic_inc(&pgdat->kswapd_waiters);
+			atomic_long_inc(&kswapd_waiters);
 			woke_kswapd = true;
 		}
 		wake_all_kswapds(order, gfp_mask, ac);
@@ -4175,7 +4176,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 fail:
 got_pg:
 	if (woke_kswapd)
-		atomic_dec(&pgdat->kswapd_waiters);
+		atomic_long_dec(&kswapd_waiters);
 	if (!page)
 		warn_alloc(gfp_mask, ac->nodemask,
 				"page allocation failure: order:%u", order);
diff --git a/mm/vmscan.c b/mm/vmscan.c
index df370823a..efe596ece 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -7535,7 +7535,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)
 		ret = try_to_freeze();
 		__fs_reclaim_acquire(_THIS_IP_);
 		if (ret || kthread_should_stop() ||
-		    !atomic_read(&pgdat->kswapd_waiters))
+		    !atomic_long_read(&kswapd_waiters))
 			break;
 
 		/*
-- 
2.42.0

