From c2fc779f7ad8ed7bc6d017533589fde1a3be4885 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 1 May 2023 21:54:10 +0200
Subject: [PATCH 18/21] Revert "sched/fair: Introduce SIS_CURRENT to wake up
 short task on current CPU"

This reverts commit 4a6734d7e1ad35aa535c06c36f518055c98287ec.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/sched.h   |  3 ---
 kernel/sched/core.c     |  2 --
 kernel/sched/debug.c    |  1 -
 kernel/sched/fair.c     | 59 -----------------------------------------
 kernel/sched/features.h |  1 -
 5 files changed, 66 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d398b337..63d242164 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -557,9 +557,6 @@ struct sched_entity {
 	u64				prev_sum_exec_runtime;
 
 	u64				nr_migrations;
-	u64				prev_sleep_sum_runtime;
-	/* average duration of a task */
-	u64				dur_avg;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	int				depth;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f338bb158..0d18c3969 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4434,8 +4434,6 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->se.prev_sum_exec_runtime	= 0;
 	p->se.nr_migrations		= 0;
 	p->se.vruntime			= 0;
-	p->se.dur_avg			= 0;
-	p->se.prev_sleep_sum_runtime	= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 8d64fba16..1637b65ba 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -1024,7 +1024,6 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	__PS("nr_involuntary_switches", p->nivcsw);
 
 	P(se.load.weight);
-	P(se.dur_avg);
 #ifdef CONFIG_SMP
 	P(se.avg.load_sum);
 	P(se.avg.runnable_sum);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6e857de3f..574f65398 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6315,18 +6315,6 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 static void set_next_buddy(struct sched_entity *se);
 
-static inline void dur_avg_update(struct task_struct *p, bool task_sleep)
-{
-	u64 dur;
-
-	if (!task_sleep)
-		return;
-
-	dur = p->se.sum_exec_runtime - p->se.prev_sleep_sum_runtime;
-	p->se.prev_sleep_sum_runtime = p->se.sum_exec_runtime;
-	update_avg(&p->se.dur_avg, dur);
-}
-
 /*
  * The dequeue_task method is called before nr_running is
  * decreased. We remove the task from the rbtree and
@@ -6399,7 +6387,6 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 dequeue_throttle:
 	util_est_update(&rq->cfs, p, task_sleep);
-	dur_avg_update(p, task_sleep);
 	hrtick_update(rq);
 }
 
@@ -6533,46 +6520,6 @@ static int wake_wide(struct task_struct *p)
 	return 1;
 }
 
-/*
- * Wake up the task on current CPU, if the following conditions are met:
- *
- * 1. waker A is the only running task on this_cpu
- * 3. A is a short duration task (waker will fall asleep soon)
- * 4. wakee B is a short duration task (impact of B on A is minor)
- * 5. A and B wake up each other alternately
- */
-static bool
-wake_on_current(int this_cpu, struct task_struct *p)
-{
-	if (!sched_feat(SIS_CURRENT))
-		return false;
-
-	if (cpu_rq(this_cpu)->nr_running > 1)
-		return false;
-
-	/*
-	 * If a task switches in and then voluntarily relinquishes the
-	 * CPU quickly, it is regarded as a short duration task. In that
-	 * way, the short waker is likely to relinquish the CPU soon, which
-	 * provides room for the wakee. Meanwhile, a short wakee would bring
-	 * minor impact to the target rq. Put the short waker and wakee together
-	 * bring benefit to cache-share task pairs and avoid migration overhead.
-	 */
-	if (!current->se.dur_avg || ((current->se.dur_avg * 8) >= sysctl_sched_min_granularity))
-		return false;
-
-	if (!p->se.dur_avg || ((p->se.dur_avg * 8) >= sysctl_sched_min_granularity))
-		return false;
-
-	if (current->wakee_flips || p->wakee_flips)
-		return false;
-
-	if (current->last_wakee != p || p->last_wakee != current)
-		return false;
-
-	return true;
-}
-
 /*
  * The purpose of wake_affine() is to quickly determine on which CPU we can run
  * soonest. For the purpose of speed we only consider the waking and previous
@@ -6666,9 +6613,6 @@ static int wake_affine(struct sched_domain *sd, struct task_struct *p,
 	if (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)
 		target = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);
 
-	if (target == nr_cpumask_bits && wake_on_current(this_cpu, p))
-		target = this_cpu;
-
 	schedstat_inc(p->stats.nr_wakeups_affine_attempts);
 	if (target == nr_cpumask_bits)
 		return prev_cpu;
@@ -7190,9 +7134,6 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 		}
 	}
 
-	if (smp_processor_id() == target && wake_on_current(target, p))
-		return target;
-
 	i = select_idle_cpu(p, sd, has_idle_core, target);
 	if ((unsigned)i < nr_cpumask_bits)
 		return i;
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index a3e05827f..ee7f23c76 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -62,7 +62,6 @@ SCHED_FEAT(TTWU_QUEUE, true)
  */
 SCHED_FEAT(SIS_PROP, false)
 SCHED_FEAT(SIS_UTIL, true)
-SCHED_FEAT(SIS_CURRENT, true)
 
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
-- 
2.40.1.445.gf85cd430b1

