From 30929623deb5e96ca3403a5c8f761588ce5ff838 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Thu, 6 Jan 2022 16:59:01 +0000
Subject: [PATCH 01/10] XANMOD: block/mq-deadline: Disable front_merges by
 default

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 block/mq-deadline.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index f10c2a0d1..d59cbab24 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -693,7 +693,7 @@ static int dd_init_sched(struct request_queue *q, struct elevator_type *e)
 	dd->fifo_expire[DD_READ] = read_expire;
 	dd->fifo_expire[DD_WRITE] = write_expire;
 	dd->writes_starved = writes_starved;
-	dd->front_merges = 1;
+	dd->front_merges = 0;
 	dd->last_dir = DD_WRITE;
 	dd->fifo_batch = fifo_batch;
 	dd->prio_aging_expire = prio_aging_expire;
-- 
2.40.0.71.g950264636c


From 98bc0246b237cd607722ee066f8bbeadd51caf10 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Wed, 11 May 2022 18:56:51 +0000
Subject: [PATCH 02/10] XANMOD: block/mq-deadline: Increase write priority to
 improve responsiveness

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 block/mq-deadline.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index d59cbab24..f94a4ee66 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -4,6 +4,9 @@
  *  for the blk-mq scheduling framework
  *
  *  Copyright (C) 2016 Jens Axboe <axboe@kernel.dk>
+ *
+ *  Tunes for responsiveness by Alexandre Frade
+ *  (C) 2022 Alexandre Frade <kernel@xanmod.org>
  */
 #include <linux/kernel.h>
 #include <linux/fs.h>
@@ -30,13 +33,13 @@
  * See Documentation/block/deadline-iosched.rst
  */
 static const int read_expire = HZ / 2;  /* max time before a read is submitted. */
-static const int write_expire = 5 * HZ; /* ditto for writes, these limits are SOFT! */
+static const int write_expire = HZ;     /* ditto for writes, these limits are SOFT! */
 /*
  * Time after which to dispatch lower priority requests even if higher
  * priority requests are pending.
  */
 static const int prio_aging_expire = 10 * HZ;
-static const int writes_starved = 2;    /* max times reads can starve a write */
+static const int writes_starved = 1;    /* max times reads can starve a write */
 static const int fifo_batch = 16;       /* # of sequential requests treated as one
 				     by the above parameters. For throughput. */
 
-- 
2.40.0.71.g950264636c


From 8c1f9b03e89ec6b02500069bf4f34568ccefa26d Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 7 Nov 2022 12:21:55 +0100
Subject: [PATCH 03/10] XANMOD: block: set rq_affinity to force full
 multithreading I/O requests

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/blkdev.h | 1 +
 1 file changed, 1 insertion(+)

diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 941304f17..8b99ecc9e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -577,6 +577,7 @@ struct request_queue {
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1UL << QUEUE_FLAG_IO_STAT) |		\
 				 (1UL << QUEUE_FLAG_SAME_COMP) |	\
+				 (1UL << QUEUE_FLAG_SAME_FORCE) |	\
 				 (1UL << QUEUE_FLAG_NOWAIT))
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
-- 
2.40.0.71.g950264636c


From b96c65aaec4cef5a386bbd6cd4d59c0ce0467d30 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <admfrade@gmail.com>
Date: Mon, 29 Jan 2018 16:59:22 +0000
Subject: [PATCH 04/10] XANMOD: dcache: cache_pressure = 50 decreases the rate
 at which VFS caches are reclaimed

Signed-off-by: Alexandre Frade <admfrade@gmail.com>
Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 fs/dcache.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/fs/dcache.c b/fs/dcache.c
index 52e6d5fda..39ff71350 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -71,7 +71,7 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
-int sysctl_vfs_cache_pressure __read_mostly = 100;
+int sysctl_vfs_cache_pressure __read_mostly = 50;
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
-- 
2.40.0.71.g950264636c


From 32fb94c784d870bccbbcd6e326f1a48562cead4f Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Wed, 15 Jun 2022 17:07:29 +0000
Subject: [PATCH 05/10] XANMOD: sched/autogroup: Add kernel parameter and
 config option to enable/disable autogroup feature by default

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 Documentation/admin-guide/kernel-parameters.txt |  6 ++++--
 init/Kconfig                                    | 12 ++++++++++++
 kernel/sched/autogroup.c                        |  9 ++++++---
 3 files changed, 22 insertions(+), 5 deletions(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 7016cb12d..98f736111 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -378,6 +378,10 @@
 	autoconf=	[IPV6]
 			See Documentation/networking/ipv6.rst.
 
+	autogroup=	[KNL] Enable or disable scheduler automatic task group
+			creation.
+			Format: <bool>
+
 	apm=		[APM] Advanced Power Management
 			See header of arch/x86/kernel/apm_32.c.
 
@@ -3619,8 +3623,6 @@
 	noapic		[SMP,APIC] Tells the kernel to not make use of any
 			IOAPICs that may be present in the system.
 
-	noautogroup	Disable scheduler automatic task group creation.
-
 	nocache		[ARM]
 
 	nodsp		[SH] Disable hardware DSP at boot time.
diff --git a/init/Kconfig b/init/Kconfig
index c88bb30a8..ef637a909 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1293,6 +1293,18 @@ config SCHED_AUTOGROUP
 	  desktop applications.  Task group autogeneration is currently based
 	  upon task session.
 
+config SCHED_AUTOGROUP_DEFAULT_ENABLED
+	bool "Enable automatic process group scheduling feature"
+	default y
+	depends on SCHED_AUTOGROUP
+	help
+	  If set, automatic process group scheduling will be enabled per
+	  default but can be disabled through passing autogroup=0 on the
+	  kernel commandline during boot or a value of 0 via the file
+	  proc/sys/kernel/sched_autogroup_enabled.
+
+	  If unsure say Y.
+
 config SYSFS_DEPRECATED
 	bool "Enable deprecated sysfs features to support old userspace tools"
 	depends on SYSFS
diff --git a/kernel/sched/autogroup.c b/kernel/sched/autogroup.c
index 991fc9002..132c3e2e6 100644
--- a/kernel/sched/autogroup.c
+++ b/kernel/sched/autogroup.c
@@ -4,7 +4,8 @@
  * Auto-group scheduling implementation:
  */
 
-unsigned int __read_mostly sysctl_sched_autogroup_enabled = 1;
+unsigned int __read_mostly sysctl_sched_autogroup_enabled =
+		IS_ENABLED(CONFIG_SCHED_AUTOGROUP_DEFAULT_ENABLED) ? 1 : 0;
 static struct autogroup autogroup_default;
 static atomic_t autogroup_seq_nr;
 
@@ -220,11 +221,13 @@ void sched_autogroup_exit(struct signal_struct *sig)
 
 static int __init setup_autogroup(char *str)
 {
-	sysctl_sched_autogroup_enabled = 0;
+	unsigned long enabled;
+	if (!kstrtoul(str, 0, &enabled))
+		sysctl_sched_autogroup_enabled = enabled ? 1 : 0;
 
 	return 1;
 }
-__setup("noautogroup", setup_autogroup);
+__setup("autogroup=", setup_autogroup);
 
 #ifdef CONFIG_PROC_FS
 
-- 
2.40.0.71.g950264636c


From 901f4b686952c0bbca673da65dd50681a7a4ce38 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <admfrade@gmail.com>
Date: Mon, 29 Jan 2018 17:31:25 +0000
Subject: [PATCH 06/10] XANMOD: mm/vmscan: vm_swappiness = 30 decreases the
 amount of swapping

Signed-off-by: Alexandre Frade <admfrade@gmail.com>
Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 mm/vmscan.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 9c1c5e8b2..0f8522678 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -186,7 +186,7 @@ struct scan_control {
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
-int vm_swappiness = 60;
+int vm_swappiness = 30;
 
 static void set_task_reclaim_state(struct task_struct *task,
 				   struct reclaim_state *rs)
-- 
2.40.0.71.g950264636c


From 0eeb96a5c48609622fd245ddcb0bbc3a8ce6a01b Mon Sep 17 00:00:00 2001
From: Alexandre Frade <admfrade@gmail.com>
Date: Tue, 31 Mar 2020 13:32:08 -0300
Subject: [PATCH 07/10] XANMOD: cpufreq: tunes ondemand and conservative
 governor for performance

Signed-off-by: Alexandre Frade <admfrade@gmail.com>
Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 drivers/cpufreq/cpufreq_conservative.c | 8 ++++----
 drivers/cpufreq/cpufreq_ondemand.c     | 8 ++++----
 2 files changed, 8 insertions(+), 8 deletions(-)

diff --git a/drivers/cpufreq/cpufreq_conservative.c b/drivers/cpufreq/cpufreq_conservative.c
index b6bd0ff35..94f028d9e 100644
--- a/drivers/cpufreq/cpufreq_conservative.c
+++ b/drivers/cpufreq/cpufreq_conservative.c
@@ -28,8 +28,8 @@ struct cs_dbs_tuners {
 };
 
 /* Conservative governor macros */
-#define DEF_FREQUENCY_UP_THRESHOLD		(80)
-#define DEF_FREQUENCY_DOWN_THRESHOLD		(20)
+#define DEF_FREQUENCY_UP_THRESHOLD		(63)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(26)
 #define DEF_FREQUENCY_STEP			(5)
 #define DEF_SAMPLING_DOWN_FACTOR		(1)
 #define MAX_SAMPLING_DOWN_FACTOR		(10)
@@ -47,9 +47,9 @@ static inline unsigned int get_freq_step(struct cs_dbs_tuners *cs_tuners,
 }
 
 /*
- * Every sampling_rate, we check, if current idle time is less than 20%
+ * Every sampling_rate, we check, if current idle time is less than 37%
  * (default), then we try to increase frequency. Every sampling_rate *
- * sampling_down_factor, we check, if current idle time is more than 80%
+ * sampling_down_factor, we check, if current idle time is more than 74%
  * (default), then we try to decrease frequency
  *
  * Frequency updates happen at minimum steps of 5% (default) of maximum
diff --git a/drivers/cpufreq/cpufreq_ondemand.c b/drivers/cpufreq/cpufreq_ondemand.c
index c52d19d67..ba5ea0267 100644
--- a/drivers/cpufreq/cpufreq_ondemand.c
+++ b/drivers/cpufreq/cpufreq_ondemand.c
@@ -18,10 +18,10 @@
 #include "cpufreq_ondemand.h"
 
 /* On-demand governor macros */
-#define DEF_FREQUENCY_UP_THRESHOLD		(80)
-#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define DEF_FREQUENCY_UP_THRESHOLD		(63)
+#define DEF_SAMPLING_DOWN_FACTOR		(100)
 #define MAX_SAMPLING_DOWN_FACTOR		(100000)
-#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(70)
 #define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
 #define MIN_FREQUENCY_UP_THRESHOLD		(1)
 #define MAX_FREQUENCY_UP_THRESHOLD		(100)
@@ -129,7 +129,7 @@ static void dbs_freq_increase(struct cpufreq_policy *policy, unsigned int freq)
 }
 
 /*
- * Every sampling_rate, we check, if current idle time is less than 20%
+ * Every sampling_rate, we check, if current idle time is less than 37%
  * (default), then we try to increase frequency. Else, we adjust the frequency
  * proportional to load.
  */
-- 
2.40.0.71.g950264636c


From 7d281d94b9e368967edfae058a6b9ce047b4b719 Mon Sep 17 00:00:00 2001
From: Alexandre Frade <admfrade@gmail.com>
Date: Thu, 25 Jun 2020 16:40:43 -0300
Subject: [PATCH 08/10] XANMOD: lib/kconfig.debug: disable default
 CONFIG_SYMBOLIC_ERRNAME and CONFIG_DEBUG_BUGVERBOSE

Signed-off-by: Alexandre Frade <admfrade@gmail.com>
Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 lib/Kconfig.debug | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 39d1d9316..490ccf47c 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -190,7 +190,7 @@ config DYNAMIC_DEBUG_CORE
 
 config SYMBOLIC_ERRNAME
 	bool "Support symbolic error names in printf"
-	default y if PRINTK
+	default n
 	help
 	  If you say Y here, the kernel's printf implementation will
 	  be able to print symbolic error names such as ENOSPC instead
@@ -200,7 +200,7 @@ config SYMBOLIC_ERRNAME
 config DEBUG_BUGVERBOSE
 	bool "Verbose BUG() reporting (adds 70K)" if DEBUG_KERNEL && EXPERT
 	depends on BUG && (GENERIC_BUG || HAVE_DEBUG_BUGVERBOSE)
-	default y
+	default n
 	help
 	  Say Y here to make BUG() panics output the file name and line number
 	  of the BUG call as well as the EIP and oops trace.  This aids
-- 
2.40.0.71.g950264636c


From 503bb0d2652e24887e28bffab0aa335de60148cb Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Thu, 15 Sep 2022 02:45:40 +0000
Subject: [PATCH 09/10] XANMOD: sched/core: Add yield_type sysctl to reduce or
 disable sched_yield

[1] https://github.com/hamadmarri/cacule-cpu-scheduler/issues/35

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 kernel/sched/core.c | 15 ++++++++++++++-
 kernel/sysctl.c     | 10 ++++++++++
 2 files changed, 24 insertions(+), 1 deletion(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 0d18c3969..c0e14d09c 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -139,6 +139,14 @@ __read_mostly int sysctl_resched_latency_warn_ms = 100;
 __read_mostly int sysctl_resched_latency_warn_once = 1;
 #endif /* CONFIG_SCHED_DEBUG */
 
+/*
+ * Choose the yield level that will perform.
+ * 0: No yield.
+ * 1: Yield only to better priority/deadline tasks.
+ * 2: Re-queue current tasks. (default CFS)
+ */
+__read_mostly int sysctl_sched_yield_type = 0;
+
 /*
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
@@ -8439,10 +8447,15 @@ static void do_sched_yield(void)
 	struct rq_flags rf;
 	struct rq *rq;
 
+	if (!sysctl_sched_yield_type)
+		return;
+
 	rq = this_rq_lock_irq(&rf);
 
 	schedstat_inc(rq->yld_count);
-	current->sched_class->yield_task(rq);
+
+	if (sysctl_sched_yield_type > 1)
+		current->sched_class->yield_task(rq);
 
 	preempt_disable();
 	rq_unlock_irq(rq, &rf);
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 1c240d2c9..aa34ca553 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -98,6 +98,7 @@ static const int six_hundred_forty_kb = 640 * 1024;
 #endif
 
 
+extern int sysctl_sched_yield_type;
 static const int ngroups_max = NGROUPS_MAX;
 static const int cap_last_cap = CAP_LAST_CAP;
 
@@ -1645,6 +1646,15 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
+	{
+		.procname	= "yield_type",
+		.data		= &sysctl_sched_yield_type,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_TWO,
+	},
 #ifdef CONFIG_PROC_SYSCTL
 	{
 		.procname	= "tainted",
-- 
2.40.0.71.g950264636c


From 67a3e784f5eeccbe710efe3cede1df7e9045abcd Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Wed, 5 Oct 2022 03:46:34 +0000
Subject: [PATCH 10/10] XANMOD: rcu: Change sched_setscheduler_nocheck() calls
 to SCHED_RR policy

Signed-off-by: Alexandre Frade <kernel@xanmod.org>
---
 Documentation/admin-guide/kernel-parameters.txt | 2 +-
 kernel/rcu/Kconfig                              | 4 ++--
 kernel/rcu/rcutorture.c                         | 2 +-
 kernel/rcu/tree.c                               | 6 +++---
 kernel/rcu/tree_nocb.h                          | 4 ++--
 kernel/rcu/tree_plugin.h                        | 4 ++--
 6 files changed, 11 insertions(+), 11 deletions(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 98f736111..1efb706c9 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -4759,7 +4759,7 @@
 			overwritten.
 
 	rcutree.kthread_prio= 	 [KNL,BOOT]
-			Set the SCHED_FIFO priority of the RCU per-CPU
+			Set the SCHED_RR priority of the RCU per-CPU
 			kthreads (rcuc/N). This value is also used for
 			the priority of the RCU boost threads (rcub/N)
 			and for the RCU grace-period kthreads (rcu_bh,
diff --git a/kernel/rcu/Kconfig b/kernel/rcu/Kconfig
index ab6207417..f1f909bdc 100644
--- a/kernel/rcu/Kconfig
+++ b/kernel/rcu/Kconfig
@@ -280,9 +280,9 @@ config RCU_NOCB_CPU_CB_BOOST
 	depends on RCU_NOCB_CPU && RCU_BOOST
 	default y if PREEMPT_RT
 	help
-	  Use this option to invoke offloaded callbacks as SCHED_FIFO
+	  Use this option to invoke offloaded callbacks as SCHED_RR
 	  to avoid starvation by heavy SCHED_OTHER background load.
-	  Of course, running as SCHED_FIFO during callback floods will
+	  Of course, running as SCHED_RR during callback floods will
 	  cause the rcuo[ps] kthreads to monopolize the CPU for hundreds
 	  of milliseconds or more.  Therefore, when enabling this option,
 	  it is your responsibility to ensure that latency-sensitive
diff --git a/kernel/rcu/rcutorture.c b/kernel/rcu/rcutorture.c
index 8e6c02321..d5c227c58 100644
--- a/kernel/rcu/rcutorture.c
+++ b/kernel/rcu/rcutorture.c
@@ -2406,7 +2406,7 @@ static int rcutorture_booster_init(unsigned int cpu)
 		t = per_cpu(ksoftirqd, cpu);
 		WARN_ON_ONCE(!t);
 		sp.sched_priority = 2;
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	}
 
 	/* Don't allow time recalculation while creating a new task. */
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 7b95ee98a..fff35d804 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -4553,8 +4553,8 @@ static void __init rcu_start_exp_gp_kworkers(void)
 		return;
 	}
 
-	sched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_FIFO, &param);
-	sched_setscheduler_nocheck(rcu_exp_par_gp_kworker->task, SCHED_FIFO,
+	sched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_RR, &param);
+	sched_setscheduler_nocheck(rcu_exp_par_gp_kworker->task, SCHED_RR,
 				   &param);
 }
 
@@ -4592,7 +4592,7 @@ static int __init rcu_spawn_gp_kthread(void)
 		return 0;
 	if (kthread_prio) {
 		sp.sched_priority = kthread_prio;
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	}
 	rnp = rcu_get_root();
 	raw_spin_lock_irqsave_rcu_node(rnp, flags);
diff --git a/kernel/rcu/tree_nocb.h b/kernel/rcu/tree_nocb.h
index 9e1c8caec..dd39c50ae 100644
--- a/kernel/rcu/tree_nocb.h
+++ b/kernel/rcu/tree_nocb.h
@@ -1465,7 +1465,7 @@ static void rcu_spawn_cpu_nocb_kthread(int cpu)
 		}
 		WRITE_ONCE(rdp_gp->nocb_gp_kthread, t);
 		if (kthread_prio)
-			sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+			sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	}
 	mutex_unlock(&rdp_gp->nocb_gp_kthread_mutex);
 
@@ -1476,7 +1476,7 @@ static void rcu_spawn_cpu_nocb_kthread(int cpu)
 		goto end;
 
 	if (IS_ENABLED(CONFIG_RCU_NOCB_CPU_CB_BOOST) && kthread_prio)
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+		sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 
 	WRITE_ONCE(rdp->nocb_cb_kthread, t);
 	WRITE_ONCE(rdp->nocb_gp_kthread, rdp_gp->nocb_gp_kthread);
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 7b0fe741a..77ad9e033 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -1007,7 +1007,7 @@ static void rcu_cpu_kthread_setup(unsigned int cpu)
 	struct sched_param sp;
 
 	sp.sched_priority = kthread_prio;
-	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+	sched_setscheduler_nocheck(current, SCHED_RR, &sp);
 #endif /* #ifdef CONFIG_RCU_BOOST */
 
 	WRITE_ONCE(rdp->rcuc_activity, jiffies);
@@ -1206,7 +1206,7 @@ static void rcu_spawn_one_boost_kthread(struct rcu_node *rnp)
 	rnp->boost_kthread_task = t;
 	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
 	sp.sched_priority = kthread_prio;
-	sched_setscheduler_nocheck(t, SCHED_FIFO, &sp);
+	sched_setscheduler_nocheck(t, SCHED_RR, &sp);
 	wake_up_process(t); /* get to TASK_INTERRUPTIBLE quickly. */
 
  out:
-- 
2.40.0.71.g950264636c

