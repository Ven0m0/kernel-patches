From e29d3261949aa94ee43222804f53ad8366695e43 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Wed, 21 Jun 2023 23:04:04 +0200
Subject: [PATCH 001/147] Revert "blk-cgroup: Flush stats before releasing
 blkcg_gq"

This reverts commit 0f6090d90f627d8c58f939067d6c6821ce1b3c68.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/blk-cgroup.c | 40 +++++++++-------------------------------
 1 file changed, 9 insertions(+), 31 deletions(-)

diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index dd6d1c011..75bad5d60 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -35,8 +35,6 @@
 #include "blk-throttle.h"
 #include "blk-rq-qos.h"
 
-static void __blkcg_rstat_flush(struct blkcg *blkcg, int cpu);
-
 /*
  * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.
  * blkcg_pol_register_mutex nests outside of it and synchronizes entire
@@ -60,8 +58,6 @@ static LIST_HEAD(all_blkcgs);		/* protected by blkcg_pol_mutex */
 bool blkcg_debug_stats = false;
 static struct workqueue_struct *blkcg_punt_bio_wq;
 
-static DEFINE_RAW_SPINLOCK(blkg_stat_lock);
-
 #define BLKG_DESTROY_BATCH_SIZE  64
 
 /*
@@ -169,18 +165,8 @@ static void blkg_free(struct blkcg_gq *blkg)
 static void __blkg_release(struct rcu_head *rcu)
 {
 	struct blkcg_gq *blkg = container_of(rcu, struct blkcg_gq, rcu_head);
-	struct blkcg *blkcg = blkg->blkcg;
-	int cpu;
 
 	WARN_ON(!bio_list_empty(&blkg->async_bios));
-	/*
-	 * Flush all the non-empty percpu lockless lists before releasing
-	 * us, given these stat belongs to us.
-	 *
-	 * blkg_stat_lock is for serializing blkg stat update
-	 */
-	for_each_possible_cpu(cpu)
-		__blkcg_rstat_flush(blkcg, cpu);
 
 	/* release the blkcg and parent blkg refs this blkg has been holding */
 	css_put(&blkg->blkcg->css);
@@ -902,26 +888,23 @@ static void blkcg_iostat_update(struct blkcg_gq *blkg, struct blkg_iostat *cur,
 	u64_stats_update_end_irqrestore(&blkg->iostat.sync, flags);
 }
 
-static void __blkcg_rstat_flush(struct blkcg *blkcg, int cpu)
+static void blkcg_rstat_flush(struct cgroup_subsys_state *css, int cpu)
 {
+	struct blkcg *blkcg = css_to_blkcg(css);
 	struct llist_head *lhead = per_cpu_ptr(blkcg->lhead, cpu);
 	struct llist_node *lnode;
 	struct blkg_iostat_set *bisc, *next_bisc;
 
+	/* Root-level stats are sourced from system-wide IO stats */
+	if (!cgroup_parent(css->cgroup))
+		return;
+
 	rcu_read_lock();
 
 	lnode = llist_del_all(lhead);
 	if (!lnode)
 		goto out;
 
-	/*
-	 * For covering concurrent parent blkg update from blkg_release().
-	 *
-	 * When flushing from cgroup, cgroup_rstat_lock is always held, so
-	 * this lock won't cause contention most of time.
-	 */
-	raw_spin_lock(&blkg_stat_lock);
-
 	/*
 	 * Iterate only the iostat_cpu's queued in the lockless list.
 	 */
@@ -945,19 +928,13 @@ static void __blkcg_rstat_flush(struct blkcg *blkcg, int cpu)
 		if (parent && parent->parent)
 			blkcg_iostat_update(parent, &blkg->iostat.cur,
 					    &blkg->iostat.last);
+		percpu_ref_put(&blkg->refcnt);
 	}
-	raw_spin_unlock(&blkg_stat_lock);
+
 out:
 	rcu_read_unlock();
 }
 
-static void blkcg_rstat_flush(struct cgroup_subsys_state *css, int cpu)
-{
-	/* Root-level stats are sourced from system-wide IO stats */
-	if (cgroup_parent(css->cgroup))
-		__blkcg_rstat_flush(css_to_blkcg(css), cpu);
-}
-
 /*
  * We source root cgroup stats from the system-wide stats to avoid
  * tracking the same information twice and incurring overhead when no
@@ -2086,6 +2063,7 @@ void blk_cgroup_bio_start(struct bio *bio)
 
 		llist_add(&bis->lnode, lhead);
 		WRITE_ONCE(bis->lqueued, true);
+		percpu_ref_get(&bis->blkg->refcnt);
 	}
 
 	u64_stats_update_end_irqrestore(&bis->sync, flags);
-- 
2.41.0

