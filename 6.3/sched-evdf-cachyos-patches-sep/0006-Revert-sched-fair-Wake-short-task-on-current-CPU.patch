From f4a6dfcb701bbc8a48865dc3a64dac9c683a4ccc Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Mon, 17 Apr 2023 18:39:42 +0200
Subject: [PATCH 06/22] Revert "sched/fair: Wake short task on current CPU"

This reverts commit 0ea7cba0094c59f0b834d5395c424b0b348f899e.
---
 include/linux/sched.h   |  3 ---
 kernel/sched/core.c     |  2 --
 kernel/sched/debug.c    |  1 -
 kernel/sched/fair.c     | 49 -----------------------------------------
 kernel/sched/features.h |  1 -
 5 files changed, 56 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d398b337..63d242164 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -557,9 +557,6 @@ struct sched_entity {
 	u64				prev_sum_exec_runtime;
 
 	u64				nr_migrations;
-	u64				prev_sleep_sum_runtime;
-	/* average duration of a task */
-	u64				dur_avg;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	int				depth;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 17bb9637f..ae4a9be11 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4434,8 +4434,6 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->se.prev_sum_exec_runtime	= 0;
 	p->se.nr_migrations		= 0;
 	p->se.vruntime			= 0;
-	p->se.dur_avg			= 0;
-	p->se.prev_sleep_sum_runtime	= 0;
 	INIT_LIST_HEAD(&p->se.group_node);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 8d64fba16..1637b65ba 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -1024,7 +1024,6 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	__PS("nr_involuntary_switches", p->nivcsw);
 
 	P(se.load.weight);
-	P(se.dur_avg);
 #ifdef CONFIG_SMP
 	P(se.avg.load_sum);
 	P(se.avg.runnable_sum);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6c3f71c35..574f65398 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6315,18 +6315,6 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 static void set_next_buddy(struct sched_entity *se);
 
-static inline void dur_avg_update(struct task_struct *p, bool task_sleep)
-{
-	u64 dur;
-
-	if (!task_sleep)
-		return;
-
-	dur = p->se.sum_exec_runtime - p->se.prev_sleep_sum_runtime;
-	p->se.prev_sleep_sum_runtime = p->se.sum_exec_runtime;
-	update_avg(&p->se.dur_avg, dur);
-}
-
 /*
  * The dequeue_task method is called before nr_running is
  * decreased. We remove the task from the rbtree and
@@ -6399,7 +6387,6 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 dequeue_throttle:
 	util_est_update(&rq->cfs, p, task_sleep);
-	dur_avg_update(p, task_sleep);
 	hrtick_update(rq);
 }
 
@@ -6533,23 +6520,6 @@ static int wake_wide(struct task_struct *p)
 	return 1;
 }
 
-/*
- * If a task switches in and then voluntarily relinquishes the
- * CPU quickly, it is regarded as a short duration task.
- *
- * SIS_SHORT tries to wake up the short wakee on current CPU. This
- * aims to avoid race condition among CPUs due to frequent context
- * switch. Besides, the candidate short task should not be the one
- * that wakes up more than one tasks, otherwise SIS_SHORT might
- * stack too many tasks on current CPU.
- */
-static inline int is_short_task(struct task_struct *p)
-{
-	return sched_feat(SIS_SHORT) && !p->wakee_flips &&
-	       p->se.dur_avg &&
-	       ((p->se.dur_avg * 8) < sysctl_sched_min_granularity);
-}
-
 /*
  * The purpose of wake_affine() is to quickly determine on which CPU we can run
  * soonest. For the purpose of speed we only consider the waking and previous
@@ -6586,11 +6556,6 @@ wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 	if (available_idle_cpu(prev_cpu))
 		return prev_cpu;
 
-	/* The only running task is a short duration one. */
-	if (cpu_rq(this_cpu)->nr_running == 1 &&
-	    is_short_task(rcu_dereference(cpu_curr(this_cpu))))
-		return this_cpu;
-
 	return nr_cpumask_bits;
 }
 
@@ -6965,20 +6930,6 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 			/* overloaded LLC is unlikely to have idle cpu/core */
 			if (nr == 1)
 				return -1;
-
-			/*
-			 * If the scan number suggested by SIS_UTIL is smaller
-			 * than 60% of llc_weight, it indicates a util_avg% higher
-			 * than 50%. System busier than this could lower its bar to
-			 * choose a compromised "idle" CPU. This co-exists with
-			 * !has_idle_core to not stack too many tasks on one CPU.
-			 */
-			if (!has_idle_core && this == target &&
-			    (5 * nr < 3 * sd->span_weight) &&
-			    cpu_rq(target)->nr_running <= 1 &&
-			    is_short_task(p) &&
-			    is_short_task(rcu_dereference(cpu_curr(target))))
-				return target;
 		}
 	}
 
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index efdc29c42..ee7f23c76 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -62,7 +62,6 @@ SCHED_FEAT(TTWU_QUEUE, true)
  */
 SCHED_FEAT(SIS_PROP, false)
 SCHED_FEAT(SIS_UTIL, true)
-SCHED_FEAT(SIS_SHORT, true)
 
 /*
  * Issue a WARN when we do multiple update_rq_clock() calls
-- 
2.40.0.71.g950264636c

