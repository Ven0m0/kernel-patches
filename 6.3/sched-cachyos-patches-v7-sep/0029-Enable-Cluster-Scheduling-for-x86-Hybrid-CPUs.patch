From 3728416a8d7285f6c554ea9ff752ef4dab874c9d Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Thu, 4 May 2023 19:51:10 +0200
Subject: [PATCH 29/29] Enable Cluster Scheduling for x86 Hybrid CPUs

Cluster scheduling domain is not enabled on x86 hybrid CPUs as the logic
is missing to do proper load balancing between a cluster
with SMT CPUs in single core and a cluster with multiple Atom CPUs.

When cluster scheduling was first introduced to x86, it was noticed
that with cluster scheduling on hybrid CPU, single threaded task often
ended up on Atom core (or E-core) instead on idle Big core (or P-core),
resulting in lower performance.  Hence cluster scheduling was disabled
on x86 hybrid CPU.  (See: https://www.phoronix.com/review/linux-516-regress)

Ricardo recently introduced a patch series that greatly improved
the load balancing logic between P-cores and E-cores on x86 hybrid
CPUs.
https://lore.kernel.org/lkml/20230429153219.GC1495785@hirez.programming.kicks-ass.net/T/#m16ebc8de64dbf4c54adebab701b42b47805105f4

However, that patch series is not enough to allow the enabling of cluster
scheduling on hybrid x86 CPUs.  This patch series provides some additional
fixes needed for load balancing between cluster sched group consisting
of SMT CPUs of Big cores and cluster sched group consisting of Atom CPUs.
With these patches applied on top of Ricardo's patch series, load is
properly balanced between the P-core and E-core clusters.  Idle CPUs
are used in the proper order:

1) SMT CPU on an idle P-core,
2) idle E-core,
3) unused SMT CPU with a busy sibling.

On x86, CPUs in a cluster share L2 cache. Load is now balanced
between the clusters with cluster enabled, for potentially less L2 cache
contention.

I did some experiments on an Alder Lake with 6 P-cores and 8 E-cores,
organized in two clusters of 4 E-core each.

I tested some single threaded benchmarks in Phoronix suite that previously
have shown regressions when cluster scheduling was first enabled. Cluster
scheduling using this patch series performs as well as vanilla kernel.

Single Threaded	6.3-rc5 		with cluster 	Improvement
Benchmark				scheduling	in Performance
		(run-run deviation)
-------------------------------------------------------------------------------------------
tjbench		(+/- 0.08%)		(+/- 0.23%)	-0.23%
PhPbench	(+/- 0.31%)		(+/- 0.89%)	-0.39%
flac		(+/- 0.58%)		(+/- 0.22%)	+0.17%
pybench		(+/- 3.16%)		(+/- 0.27%)	+2.55%

For multi-threaded benchmarks, I tried kernel build and tensor flow lite.
Cluster scheduling did best for the 10 thread case where 6 threads run on
the P-cores, 2 threads on one Atom cluster and 2 threads on the other Atom
cluster. Whereas the vanilla kernel will have 6 threads on the P-cores,
4 threads on one Atom cluster.  Though the differences are small and
fall within run variations.

Multi Threaded	6.3-rc5 		with cluster 	Improvement
Benchmark				scheduling	in Performance
(-#threads)	(run-run deviation)
-------------------------------------------------------------------------------------------
Kbuild-8	(+/- 2.90%)		(+/- 1.16%)	-0.76%
Kbuild-10	(+/- 3.08%)		(+/- 3.09%)	+0.64%
Kbuild-12	(+/- 3.28%)		(+/- 3.55%)	+0.91%
Tensor Lite-8	(+/- 4.84%)		(+/- 4.61%)	-0.23%
Tensor Lite-10	(+/- 0.87%)		(+/- 1.45%)	+0.47%
Tensor Lite-12	(+/- 1.37%)		(+/- 1.04%)	-0.12%

Thanks for reviewing these patches.

Tim Chen

Ricardo Neri (1):
  sched/fair: Consider the idle state of the whole core for load balance

Tim C Chen (5):
  sched/topology: Propagate SMT flags when removing degenerate domain
  sched/fair: Check whether active load balance is needed in busiest
    group
  sched/fair: Fix busiest group selection for asym groups
  sched/fair: Skip prefer sibling move between SMT group and non-SMT
    group
  sched/x86: Add cluster topology to hybrid CPU

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 arch/x86/kernel/smpboot.c |  3 ++
 kernel/sched/fair.c       | 78 ++++++++++++++++++++++++++++++++++++++-
 kernel/sched/topology.c   |  7 +++-
 3 files changed, 86 insertions(+), 2 deletions(-)

diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index cea297d97..2489d767c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -575,6 +575,9 @@ static struct sched_domain_topology_level x86_hybrid_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT) },
 #endif
+#ifdef CONFIG_SCHED_CLUSTER
+	{ cpu_clustergroup_mask, x86_cluster_flags, SD_INIT_NAME(CLS) },
+#endif
 #ifdef CONFIG_SCHED_MC
 	{ cpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC) },
 #endif
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index fdf07ca09..c26d0343b 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -9492,6 +9492,17 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 				sgs->group_capacity;
 }
 
+/* One group is SMT while the other group is not */
+static inline bool asymmetric_groups(struct sched_group *sg1,
+				    struct sched_group *sg2)
+{
+	if (!sg1 || !sg2)
+		return false;
+
+	return (sg1->flags & SD_SHARE_CPUCAPACITY) !=
+		(sg2->flags & SD_SHARE_CPUCAPACITY);
+}
+
 /**
  * update_sd_pick_busiest - return 1 on busiest group
  * @env: The load balancing environment.
@@ -9596,6 +9607,18 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 		break;
 
 	case group_has_spare:
+		/*
+		 * Do not pick sg with SMT CPUs over sg with pure CPUs,
+		 * as we do not want to pull task off half empty SMT core
+		 * and make the core idle.
+		 */
+		if (asymmetric_groups(sds->busiest, sg)) {
+			if (sds->busiest->flags & SD_SHARE_CPUCAPACITY)
+				return true;
+			else
+				return false;
+		}
+
 		/*
 		 * Select not overloaded group with lowest number of idle cpus
 		 * and highest number of running tasks. We could also compare
@@ -10138,6 +10161,31 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 	update_idle_cpu_scan(env, sum_util);
 }
 
+static inline bool asym_active_balance_busiest(struct lb_env *env, struct sd_lb_stats *sds)
+{
+	/*
+	 * Don't balance to a group without spare capacity.
+	 *
+	 * Skip non asymmetric sched group balancing. That check
+	 * is handled by code path handling imbalanced load between
+	 * similar groups.
+	 */
+	if (env->idle == CPU_NOT_IDLE ||
+	    sds->local_stat.group_type != group_has_spare ||
+	    !asymmetric_groups(sds->local, sds->busiest))
+		return false;
+
+	/*
+	 * For SMT source group, pull when there are two or more
+	 * tasks over-utilizing a core.
+	 */
+	if (sds->busiest->flags & SD_SHARE_CPUCAPACITY &&
+	    sds->busiest_stat.sum_h_nr_running > 1)
+		return true;
+
+	return false;
+}
+
 /**
  * calculate_imbalance - Calculate the amount of imbalance present within the
  *			 groups of a given sched_domain during load balance.
@@ -10223,6 +10271,12 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 			return;
 		}
 
+		if (asym_active_balance_busiest(env, sds)) {
+			env->migration_type = migrate_task;
+			env->imbalance = 1;
+			return;
+		}
+
 		if (busiest->group_weight == 1 || sds->prefer_sibling) {
 			unsigned int nr_diff = busiest->sum_nr_running;
 			/*
@@ -10426,8 +10480,12 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 	/*
 	 * Try to move all excess tasks to a sibling domain of the busiest
 	 * group's child domain.
+	 *
+	 * Do not try to move between non smt sched group and smt sched
+	 * group. Let asym active balance properly handle that case.
 	 */
 	if (sds.prefer_sibling && local->group_type == group_has_spare &&
+	    !asymmetric_groups(sds.busiest, sds.local) &&
 	    busiest->sum_nr_running > local->sum_nr_running + 1)
 		goto force_balance;
 
@@ -10440,6 +10498,9 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 			 */
 			goto out_balanced;
 
+		if (asym_active_balance_busiest(env, &sds))
+			goto force_balance;
+
 		if (busiest->group_weight > 1 &&
 		    local->idle_cpus <= (busiest->idle_cpus + 1))
 			/*
@@ -10691,7 +10752,7 @@ static int active_load_balance_cpu_stop(void *data);
 static int should_we_balance(struct lb_env *env)
 {
 	struct sched_group *sg = env->sd->groups;
-	int cpu;
+	int cpu, idle_smt = -1;
 
 	/*
 	 * Ensure the balancing environment is consistent; can happen
@@ -10717,11 +10778,26 @@ static int should_we_balance(struct lb_env *env)
 	for_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {
 		if (!idle_cpu(cpu))
 			continue;
+		else {
+			/*
+			 * Don't balance to idle SMT in busy core right away when
+			 * balancing cores, but remember the first idle SMT CPU for
+			 * later consideration.  Find CPU on an idle core first.
+			 */
+			if (!(env->sd->flags & SD_SHARE_CPUCAPACITY) && !is_core_idle(cpu)) {
+				if (idle_smt == -1)
+					idle_smt = cpu;
+				continue;
+			}
+		}
 
 		/* Are we the first idle CPU? */
 		return cpu == env->dst_cpu;
 	}
 
+	if (idle_smt == env->dst_cpu)
+		return true;
+
 	/* Are we the first CPU of this group ? */
 	return group_balance_cpu(sg) == env->dst_cpu;
 }
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 6682535e3..ca4472281 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -719,8 +719,13 @@ cpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)
 
 		if (sd_parent_degenerate(tmp, parent)) {
 			tmp->parent = parent->parent;
-			if (parent->parent)
+
+			if (parent->parent) {
 				parent->parent->child = tmp;
+				if (tmp->flags & SD_SHARE_CPUCAPACITY)
+					parent->parent->groups->flags |= SD_SHARE_CPUCAPACITY;
+			}
+
 			/*
 			 * Transfer SD_PREFER_SIBLING down in case of a
 			 * degenerate parent; the spans match for this
-- 
2.40.1.445.gf85cd430b1

