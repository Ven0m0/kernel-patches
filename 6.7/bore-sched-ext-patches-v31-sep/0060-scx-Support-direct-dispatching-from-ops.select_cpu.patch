From fcd76f013bcbb67dfe806ba14b606c3bcba809b7 Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Thu, 21 Dec 2023 17:47:21 -0600
Subject: [PATCH 060/121] scx: Support direct dispatching from ops.select_cpu()

A common pattern in schedulers is to find and reserve an idle core in
ops.select_cpu(), and to then use a task local storage map to specify
that the task should be enqueued in SCX_DSQ_LOCAL on the ops.enqueue()
path. At the same time, we also have a special SCX_TASK_ENQ_LOCAL
enqueue flag which is used by scx_select_cpu_dfl() to notify
ops.enqueue() that it may want to do a local enqueue.

Taking a step back, direct dispatch is something that should be
supported from the ops.select_cpu() path as well. The contract is that
doing a direct dispatch to SCX_DSQ_LOCAL will dispatch the task to the
local CPU of whatever is returned by ops.select_cpu(). With that in
mind, let's just extend the API a bit to support direct dispatch from
ops.select_cpu().

Signed-off-by: David Vernet <void@manifault.com>
---
 Documentation/scheduler/sched-ext.rst |  12 +++-
 include/linux/sched/ext.h             |  18 ++++-
 init/init_task.c                      |   1 +
 kernel/sched/core.c                   |   1 +
 kernel/sched/ext.c                    | 100 ++++++++++++++++----------
 5 files changed, 91 insertions(+), 41 deletions(-)

diff --git a/Documentation/scheduler/sched-ext.rst b/Documentation/scheduler/sched-ext.rst
index b67346cf5..fc45366b7 100644
--- a/Documentation/scheduler/sched-ext.rst
+++ b/Documentation/scheduler/sched-ext.rst
@@ -142,11 +142,19 @@ The following briefly shows how a waking task is scheduled and executed.
    scheduler can wake up any cpu using the ``scx_bpf_kick_cpu()`` helper,
    using ``ops.select_cpu()`` judiciously can be simpler and more efficient.
 
+   A task can be immediately dispatched to a DSQ from ``ops.select_cpu()`` by
+   calling ``scx_bpf_dispatch()``. If the task is dispatched to
+   ``SCX_DSQ_LOCAL`` from ``ops.select_cpu()``, it will be dispatched to the
+   local DSQ of whichever CPU is returned from ``ops.select_cpu()``.
+   Additionally, dispatching directly from ``ops.select_cpu()`` will cause the
+   ``ops.enqueue()`` callback to be skipped.
+
    Note that the scheduler core will ignore an invalid CPU selection, for
    example, if it's outside the allowed cpumask of the task.
 
-2. Once the target CPU is selected, ``ops.enqueue()`` is invoked. It can
-   make one of the following decisions:
+2. Once the target CPU is selected, ``ops.enqueue()`` is invoked (unless the
+   task was dispatched directly from ``ops.select_cpu()``). ``ops.enqueue()``
+   can make one of the following decisions:
 
    * Immediately dispatch the task to either the global or local DSQ by
      calling ``scx_bpf_dispatch()`` with ``SCX_DSQ_GLOBAL`` or
diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
index b20a7620b..5096389ed 100644
--- a/include/linux/sched/ext.h
+++ b/include/linux/sched/ext.h
@@ -184,6 +184,11 @@ struct sched_ext_ops {
 	 * If an idle CPU is returned, the CPU is kicked and will try to
 	 * dispatch. While an explicit custom mechanism can be added,
 	 * select_cpu() serves as the default way to wake up idle CPUs.
+	 *
+	 * @p may be dispatched directly by calling scx_bpf_dispatch(). If @p
+	 * is dispatched, the ops.enqueue() callback will be skipped. Finally,
+	 * if @p is dispatched to SCX_DSQ_LOCAL, it will be dispatched to the
+	 * local DSQ of whatever CPU is returned by this callback.
 	 */
 	s32 (*select_cpu)(struct task_struct *p, s32 prev_cpu, u64 wake_flags);
 
@@ -196,6 +201,9 @@ struct sched_ext_ops {
 	 * or enqueue on the BPF scheduler. If not directly dispatched, the bpf
 	 * scheduler owns @p and if it fails to dispatch @p, the task will
 	 * stall.
+	 *
+	 * If @p was dispatched from ops.select_cpu(), this callback is
+	 * skipped.
 	 */
 	void (*enqueue)(struct task_struct *p, u64 enq_flags);
 
@@ -597,7 +605,7 @@ struct scx_dispatch_q {
 enum scx_ent_flags {
 	SCX_TASK_QUEUED		= 1 << 0, /* on ext runqueue */
 	SCX_TASK_BAL_KEEP	= 1 << 1, /* balance decided to keep current */
-	SCX_TASK_ENQ_LOCAL	= 1 << 2, /* used by scx_select_cpu_dfl() to set SCX_ENQ_LOCAL */
+	SCX_TASK_DDSP_PRIQ	= 1 << 2, /* task should be enqueued on priq when directly dispatched */
 
 	SCX_TASK_OPS_PREPPED	= 1 << 8, /* prepared for BPF scheduler enable */
 	SCX_TASK_OPS_ENABLED	= 1 << 9, /* task has BPF scheduler enabled */
@@ -630,7 +638,7 @@ enum scx_kf_mask {
 	SCX_KF_CPU_RELEASE	= 1 << 2, /* ops.cpu_release() */
 	/* ops.dequeue (in REST) may be nested inside DISPATCH */
 	SCX_KF_DISPATCH		= 1 << 3, /* ops.dispatch() */
-	SCX_KF_ENQUEUE		= 1 << 4, /* ops.enqueue() */
+	SCX_KF_ENQUEUE		= 1 << 4, /* ops.enqueue() and ops.select_cpu() */
 	SCX_KF_REST		= 1 << 5, /* other rq-locked operations */
 
 	__SCX_KF_RQ_LOCKED	= SCX_KF_CPU_RELEASE | SCX_KF_DISPATCH |
@@ -685,6 +693,12 @@ struct sched_ext_entity {
 	 */
 	u64			dsq_vtime;
 
+	/*
+	 * Used to track when a task has requested a direct dispatch from the
+	 * ops.select_cpu() path.
+	 */
+	u64			ddsq_id;
+
 	/*
 	 * If set, reject future sched_setscheduler(2) calls updating the policy
 	 * to %SCHED_EXT with -%EACCES.
diff --git a/init/init_task.c b/init/init_task.c
index a7a89af17..1d47e0224 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -113,6 +113,7 @@ struct task_struct init_task
 		.ops_state	= ATOMIC_INIT(0),
 		.runnable_at	= INITIAL_JIFFIES,
 		.slice		= SCX_SLICE_DFL,
+		.ddsq_id	= SCX_DSQ_INVALID,
 	},
 #endif
 	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index dc042671f..15af55372 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4564,6 +4564,7 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	atomic_long_set(&p->scx.ops_state, 0);
 	p->scx.runnable_at	= INITIAL_JIFFIES;
 	p->scx.slice		= SCX_SLICE_DFL;
+	p->scx.ddsq_id		= SCX_DSQ_INVALID;
 #endif
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index 634fcb7cb..1882d1ccc 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -786,18 +786,24 @@ static struct scx_dispatch_q *find_dsq_for_dispatch(struct rq *rq, u64 dsq_id,
 	return dsq;
 }
 
-static void direct_dispatch(struct task_struct *ddsp_task, struct task_struct *p,
-			    u64 dsq_id, u64 enq_flags)
+static void mark_direct_dispatch(struct task_struct *ddsp_task,
+				 struct task_struct *p, u64 dsq_id,
+				 u64 enq_flags)
 {
-	struct scx_dispatch_q *dsq;
+	/*
+	 * Mark that dispatch already happened from ops.select_cpu() or
+	 * ops.enqueue() by spoiling direct_dispatch_task with a non-NULL value
+	 * which can never match a valid task pointer.
+	 */
+	__this_cpu_write(direct_dispatch_task, ERR_PTR(-ESRCH));
 
-	/* @p must match the task which is being enqueued */
+	/* @p must match the task on the enqueue path */
 	if (unlikely(p != ddsp_task)) {
 		if (IS_ERR(ddsp_task))
 			scx_ops_error("%s[%d] already direct-dispatched",
 				      p->comm, p->pid);
 		else
-			scx_ops_error("enqueueing %s[%d] but trying to direct-dispatch %s[%d]",
+			scx_ops_error("scheduling for %s[%d] but trying to direct-dispatch %s[%d]",
 				      ddsp_task->comm, ddsp_task->pid,
 				      p->comm, p->pid);
 		return;
@@ -814,16 +820,28 @@ static void direct_dispatch(struct task_struct *ddsp_task, struct task_struct *p
 		return;
 	}
 
+	WARN_ON_ONCE(p->scx.ddsq_id != SCX_DSQ_INVALID);
+	WARN_ON_ONCE(p->scx.flags & SCX_TASK_DDSP_PRIQ);
+
+	p->scx.ddsq_id = dsq_id;
+	if (enq_flags & SCX_ENQ_DSQ_PRIQ)
+		p->scx.flags |= SCX_TASK_DDSP_PRIQ;
+}
+
+static void direct_dispatch(struct task_struct *p, u64 enq_flags)
+{
+	struct scx_dispatch_q *dsq;
+
 	touch_core_sched_dispatch(task_rq(p), p);
 
-	dsq = find_dsq_for_dispatch(task_rq(p), dsq_id, p);
-	dispatch_enqueue(dsq, p, enq_flags | SCX_ENQ_CLEAR_OPSS);
+	if (p->scx.flags & SCX_TASK_DDSP_PRIQ) {
+		enq_flags |= SCX_ENQ_DSQ_PRIQ;
+		p->scx.flags &= ~SCX_TASK_DDSP_PRIQ;
+	}
 
-	/*
-	 * Mark that dispatch already happened by spoiling direct_dispatch_task
-	 * with a non-NULL value which can never match a valid task pointer.
-	 */
-	__this_cpu_write(direct_dispatch_task, ERR_PTR(-ESRCH));
+	dsq = find_dsq_for_dispatch(task_rq(p), p->scx.ddsq_id, p);
+	dispatch_enqueue(dsq, p, enq_flags | SCX_ENQ_CLEAR_OPSS);
+	p->scx.ddsq_id = SCX_DSQ_INVALID;
 }
 
 static bool test_rq_online(struct rq *rq)
@@ -843,10 +861,8 @@ static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,
 
 	WARN_ON_ONCE(!(p->scx.flags & SCX_TASK_QUEUED));
 
-	if (p->scx.flags & SCX_TASK_ENQ_LOCAL) {
-		enq_flags |= SCX_ENQ_LOCAL;
-		p->scx.flags &= ~SCX_TASK_ENQ_LOCAL;
-	}
+	if (p->scx.ddsq_id != SCX_DSQ_INVALID)
+		goto direct;
 
 	/* rq migration */
 	if (sticky_cpu == cpu_of(rq))
@@ -889,13 +905,19 @@ static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,
 
 	SCX_CALL_OP_TASK(SCX_KF_ENQUEUE, enqueue, p, enq_flags);
 
+	*ddsp_taskp = NULL;
+	if (p->scx.ddsq_id != SCX_DSQ_INVALID)
+		goto direct;
+
 	/*
 	 * If not directly dispatched, QUEUEING isn't clear yet and dispatch or
 	 * dequeue may be waiting. The store_release matches their load_acquire.
 	 */
-	if (*ddsp_taskp == p)
-		atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_QUEUED | qseq);
-	*ddsp_taskp = NULL;
+	atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_QUEUED | qseq);
+	return;
+
+direct:
+	direct_dispatch(p, enq_flags);
 	return;
 
 local:
@@ -2011,10 +2033,8 @@ static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flag
 	if ((wake_flags & SCX_WAKE_SYNC) && p->nr_cpus_allowed > 1 &&
 	    !cpumask_empty(idle_masks.cpu) && !(current->flags & PF_EXITING)) {
 		cpu = smp_processor_id();
-		if (cpumask_test_cpu(cpu, p->cpus_ptr)) {
-			p->scx.flags |= SCX_TASK_ENQ_LOCAL;
-			return cpu;
-		}
+		if (cpumask_test_cpu(cpu, p->cpus_ptr))
+			goto dispatch_local;
 	}
 
 	if (p->nr_cpus_allowed == 1)
@@ -2027,38 +2047,44 @@ static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flag
 	if (sched_smt_active()) {
 		if (cpumask_test_cpu(prev_cpu, idle_masks.smt) &&
 		    test_and_clear_cpu_idle(prev_cpu)) {
-			p->scx.flags |= SCX_TASK_ENQ_LOCAL;
-			return prev_cpu;
+			cpu = prev_cpu;
+			goto dispatch_local;
 		}
 
 		cpu = scx_pick_idle_cpu(p->cpus_ptr, SCX_PICK_IDLE_CORE);
-		if (cpu >= 0) {
-			p->scx.flags |= SCX_TASK_ENQ_LOCAL;
-			return cpu;
-		}
+		if (cpu >= 0)
+			goto dispatch_local;
 	}
 
 	if (test_and_clear_cpu_idle(prev_cpu)) {
-		p->scx.flags |= SCX_TASK_ENQ_LOCAL;
-		return prev_cpu;
+		cpu = prev_cpu;
+		goto dispatch_local;
 	}
 
 	cpu = scx_pick_idle_cpu(p->cpus_ptr, 0);
-	if (cpu >= 0) {
-		p->scx.flags |= SCX_TASK_ENQ_LOCAL;
-		return cpu;
-	}
+	if (cpu >= 0)
+		goto dispatch_local;
 
 	return prev_cpu;
+
+dispatch_local:
+	p->scx.ddsq_id = SCX_DSQ_LOCAL;
+	return cpu;
 }
 
 static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flags)
 {
 	if (SCX_HAS_OP(select_cpu)) {
 		s32 cpu;
+		struct task_struct **ddsp_taskp;
+
+		ddsp_taskp = this_cpu_ptr(&direct_dispatch_task);
+		WARN_ON_ONCE(*ddsp_taskp);
+		*ddsp_taskp = p;
 
-		cpu = SCX_CALL_OP_TASK_RET(SCX_KF_REST, select_cpu, p, prev_cpu,
+		cpu = SCX_CALL_OP_TASK_RET(SCX_KF_ENQUEUE, select_cpu, p, prev_cpu,
 					   wake_flags);
+		*ddsp_taskp = NULL;
 		if (ops_cpu_valid(cpu)) {
 			return cpu;
 		} else {
@@ -3870,7 +3896,7 @@ static void scx_dispatch_commit(struct task_struct *p, u64 dsq_id, u64 enq_flags
 
 	ddsp_task = __this_cpu_read(direct_dispatch_task);
 	if (ddsp_task) {
-		direct_dispatch(ddsp_task, p, dsq_id, enq_flags);
+		mark_direct_dispatch(ddsp_task, p, dsq_id, enq_flags);
 		return;
 	}
 
-- 
2.43.0.232.ge79552d197

