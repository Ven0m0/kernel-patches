From 9241e53295c44dd333235c35c83ff832318de3bb Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Thu, 14 Dec 2023 13:26:15 -0600
Subject: [PATCH 067/122] scx: Call enable / disable on entry / exit to scx

Currently, the ops.enable() and ops.disable() callbacks are invoked a
single time for every task on the system. ops.enable() is invoked
shortly after a task succeeds in ops.prep_enable(), and ops.disable() is
invoked when a task exits, or when the BPF scheduler is unloaded.

This API is a bit odd because ops.enable() can be invoked well before a
task actually starts running in the BPF scheduler, so it's not
necessarily useful as a way to bootstrap a process. For example,
scx_simple does the following:

void BPF_STRUCT_OPS(simple_enable, struct task_struct *p,
                    struct scx_enable_args *args)
{
        p->scx.dsq_vtime = vtime_now;
}

If the task later switches to sched_ext, the value will of course be
stale. While it ends up balancing out due to logic elsewhere in the
scheduler, it's indicative of a somewhat awkward component of the API
that can be improved.

Instead, this patch has ops.enable() be invoked when a task is entering
the scheduler for the first time, and and ops.disable() be invoked
whenever a task is leaving the scheduler; be it because of exiting, the
scheduler being unloaded, or the task manually switching sched policies.

Signed-off-by: David Vernet <void@manifault.com>
---
 include/linux/sched/ext.h |  36 ++++++---
 init/init_task.c          |   1 +
 kernel/sched/ext.c        | 162 +++++++++++++++++++++++++-------------
 3 files changed, 136 insertions(+), 63 deletions(-)

diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
index 5a0336349..732ce6801 100644
--- a/include/linux/sched/ext.h
+++ b/include/linux/sched/ext.h
@@ -431,11 +431,11 @@ struct sched_ext_ops {
 	 *
 	 * Either we're loading a BPF scheduler or a new task is being forked.
 	 * Prepare BPF scheduling for @p. This operation may block and can be
-	 * used for allocations.
+	 * used for allocations, and is called exactly once for a task.
 	 *
 	 * Return 0 for success, -errno for failure. An error return while
-	 * loading will abort loading of the BPF scheduler. During a fork, will
-	 * abort the specific fork.
+	 * loading will abort loading of the BPF scheduler. During a fork, it
+	 * will abort that specific fork.
 	 */
 	s32 (*prep_enable)(struct task_struct *p, struct scx_enable_args *args);
 
@@ -444,8 +444,9 @@ struct sched_ext_ops {
 	 * @p: task to enable BPF scheduling for
 	 * @args: enable arguments, see the struct definition
 	 *
-	 * Enable @p for BPF scheduling. @p is now in the cgroup specified for
-	 * the preceding prep_enable() and will start running soon.
+	 * Enable @p for BPF scheduling. @p is now in the cgroup specified in
+	 * @args. enable() is called on @p any time it enters SCX, and is
+	 * always paired with a matching disable().
 	 */
 	void (*enable)(struct task_struct *p, struct scx_enable_args *args);
 
@@ -465,7 +466,8 @@ struct sched_ext_ops {
 	 * @p: task to disable BPF scheduling for
 	 *
 	 * @p is exiting, leaving SCX or the BPF scheduler is being unloaded.
-	 * Disable BPF scheduling for @p.
+	 * Disable BPF scheduling for @p. A disable() call is always matched
+	 * with a prior enable() call.
 	 */
 	void (*disable)(struct task_struct *p);
 
@@ -606,14 +608,15 @@ enum scx_ent_flags {
 	SCX_TASK_QUEUED		= 1 << 0, /* on ext runqueue */
 	SCX_TASK_BAL_KEEP	= 1 << 1, /* balance decided to keep current */
 	SCX_TASK_DDSP_PRIQ	= 1 << 2, /* task should be enqueued on priq when directly dispatched */
-
-	SCX_TASK_OPS_PREPPED	= 1 << 8, /* prepared for BPF scheduler enable */
-	SCX_TASK_OPS_ENABLED	= 1 << 9, /* task has BPF scheduler enabled */
+	SCX_TASK_STATE_0	= 1 << 3, /* first bit encoding the task's current state */
+	SCX_TASK_STATE_1	= 1 << 4, /* second bit encoding the task's current state */
 
 	SCX_TASK_WATCHDOG_RESET = 1 << 16, /* task watchdog counter should be reset */
 	SCX_TASK_DEQD_FOR_SLEEP	= 1 << 17, /* last dequeue was for SLEEP */
 
 	SCX_TASK_CURSOR		= 1 << 31, /* iteration cursor, not a task */
+
+	SCX_TASK_STATE_MASK	= SCX_TASK_STATE_0 | SCX_TASK_STATE_1,
 };
 
 /* scx_entity.dsq_flags */
@@ -647,6 +650,21 @@ enum scx_kf_mask {
 	__SCX_KF_TERMINAL	= SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
 };
 
+/* scx_entity.task_state */
+enum scx_task_state {
+	/* ops.prep_enable() has not yet been called on task */
+	SCX_TASK_NONE,
+
+	/* ops.prep_enable() succeeded on task, but it still be cancelled */
+	SCX_TASK_INIT,
+
+	/* Task is fully initialized, but not being scheduled in sched_ext */
+	SCX_TASK_READY,
+
+	/* Task is fully initialized and is being scheduled in sched_ext */
+	SCX_TASK_ENABLED,
+};
+
 /*
  * The following is embedded in task_struct and contains all fields necessary
  * for a task to be scheduled by SCX.
diff --git a/init/init_task.c b/init/init_task.c
index 1d47e0224..c1a0776a8 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -108,6 +108,7 @@ struct task_struct init_task
 	.scx		= {
 		.dsq_node.fifo	= LIST_HEAD_INIT(init_task.scx.dsq_node.fifo),
 		.watchdog_node	= LIST_HEAD_INIT(init_task.scx.watchdog_node),
+		.flags		= 0,
 		.sticky_cpu	= -1,
 		.holding_cpu	= -1,
 		.ops_state	= ATOMIC_INIT(0),
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index dab874f0b..55cc5f5c9 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -2287,12 +2287,49 @@ static struct cgroup *tg_cgrp(struct task_group *tg)
 
 #endif	/* CONFIG_EXT_GROUP_SCHED */
 
+static enum scx_task_state scx_get_task_state(const struct task_struct *p)
+{
+	int state = p->scx.flags & SCX_TASK_STATE_MASK;
+
+	switch (state) {
+	case SCX_TASK_STATE_0 | SCX_TASK_STATE_1:
+		return SCX_TASK_ENABLED;
+	case SCX_TASK_STATE_1:
+		return SCX_TASK_READY;
+	case SCX_TASK_STATE_0:
+		return SCX_TASK_INIT;
+	default:
+		return SCX_TASK_NONE;
+	}
+}
+
+static void scx_set_task_state(struct task_struct *p, enum scx_task_state state)
+{
+	enum scx_task_state prev_state = scx_get_task_state(p);
+
+	p->scx.flags &= ~SCX_TASK_STATE_MASK;
+	switch (state) {
+	case SCX_TASK_NONE:
+		return;
+	case SCX_TASK_INIT:
+		WARN_ON_ONCE(prev_state != SCX_TASK_NONE);
+		p->scx.flags |= SCX_TASK_STATE_0;
+		return;
+	case SCX_TASK_READY:
+		WARN_ON_ONCE(prev_state == SCX_TASK_NONE);
+		p->scx.flags |= SCX_TASK_STATE_1;
+		return;
+	case SCX_TASK_ENABLED:
+		WARN_ON_ONCE(prev_state != SCX_TASK_READY);
+		p->scx.flags |= (SCX_TASK_STATE_0 | SCX_TASK_STATE_1);
+		return;
+	}
+}
+
 static int scx_ops_prepare_task(struct task_struct *p, struct task_group *tg)
 {
 	int ret;
 
-	WARN_ON_ONCE(p->scx.flags & SCX_TASK_OPS_PREPPED);
-
 	p->scx.disallow = false;
 
 	if (SCX_HAS_OP(prep_enable)) {
@@ -2307,6 +2344,8 @@ static int scx_ops_prepare_task(struct task_struct *p, struct task_group *tg)
 		}
 	}
 
+	scx_set_task_state(p, SCX_TASK_INIT);
+
 	if (p->scx.disallow) {
 		struct rq *rq;
 		struct rq_flags rf;
@@ -2328,7 +2367,7 @@ static int scx_ops_prepare_task(struct task_struct *p, struct task_group *tg)
 		task_rq_unlock(rq, p, &rf);
 	}
 
-	p->scx.flags |= (SCX_TASK_OPS_PREPPED | SCX_TASK_WATCHDOG_RESET);
+	p->scx.flags |= SCX_TASK_WATCHDOG_RESET;
 	return 0;
 }
 
@@ -2342,62 +2381,58 @@ static void set_task_scx_weight(struct task_struct *p)
 static void scx_ops_enable_task(struct task_struct *p)
 {
 	lockdep_assert_rq_held(task_rq(p));
-	WARN_ON_ONCE(!(p->scx.flags & SCX_TASK_OPS_PREPPED));
 
+	/*
+	 * Set the weight before calling ops.enable() so that the scheduler
+	 * doesn't see a stale value if they inspect the task struct.
+	 */
+	set_task_scx_weight(p);
 	if (SCX_HAS_OP(enable)) {
 		struct scx_enable_args args = {
 			SCX_ENABLE_ARGS_INIT_CGROUP(task_group(p))
 		};
 
-		/*
-		 * Set the weight manually before calling ops.enable() so that
-		 * the scheduler doesn't see a stale value if they inspect the
-		 * task struct. ops.set_weight() is invoked afterwards in the
-		 * caller, as it would be odd to receive a callback on the task
-		 * before we tell the scheduler that it's been fully enabled.
-		 */
-		set_task_scx_weight(p);
 		SCX_CALL_OP_TASK(SCX_KF_REST, enable, p, &args);
 	}
-	p->scx.flags &= ~SCX_TASK_OPS_PREPPED;
-	p->scx.flags |= SCX_TASK_OPS_ENABLED;
+	scx_set_task_state(p, SCX_TASK_ENABLED);
+
+	if (SCX_HAS_OP(set_weight))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);
 }
 
 static void scx_ops_disable_task(struct task_struct *p)
 {
 	lockdep_assert_rq_held(task_rq(p));
+	WARN_ON_ONCE(scx_get_task_state(p) != SCX_TASK_ENABLED);
 
-	if (p->scx.flags & SCX_TASK_OPS_PREPPED) {
+	if (SCX_HAS_OP(disable))
+		SCX_CALL_OP(SCX_KF_REST, disable, p);
+	scx_set_task_state(p, SCX_TASK_READY);
+}
+
+static void scx_ops_exit_task(struct task_struct *p)
+{
+	lockdep_assert_rq_held(task_rq(p));
+
+	switch (scx_get_task_state(p)) {
+	case SCX_TASK_NONE:
+		return;
+	case SCX_TASK_INIT:
 		if (SCX_HAS_OP(cancel_enable)) {
 			struct scx_enable_args args = {
 				SCX_ENABLE_ARGS_INIT_CGROUP(task_group(p))
 			};
 			SCX_CALL_OP(SCX_KF_REST, cancel_enable, p, &args);
 		}
-		p->scx.flags &= ~SCX_TASK_OPS_PREPPED;
-	} else if (p->scx.flags & SCX_TASK_OPS_ENABLED) {
-		if (SCX_HAS_OP(disable))
-			SCX_CALL_OP(SCX_KF_REST, disable, p);
-		p->scx.flags &= ~SCX_TASK_OPS_ENABLED;
+		break;
+	case SCX_TASK_READY:
+		break;
+	case SCX_TASK_ENABLED:
+		scx_ops_disable_task(p);
+		break;
 	}
-}
 
-/**
- * refresh_scx_weight - Refresh a task's ext weight
- * @p: task to refresh ext weight for
- *
- * @p->scx.weight carries the task's static priority in cgroup weight scale to
- * enable easy access from the BPF scheduler. To keep it synchronized with the
- * current task priority, this function should be called when a new task is
- * created, priority is changed for a task on sched_ext, and a task is switched
- * to sched_ext from other classes.
- */
-static void refresh_scx_weight(struct task_struct *p)
-{
-	lockdep_assert_rq_held(task_rq(p));
-	set_task_scx_weight(p);
-	if (SCX_HAS_OP(set_weight))
-		SCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);
+	scx_set_task_state(p, SCX_TASK_NONE);
 }
 
 void scx_pre_fork(struct task_struct *p)
@@ -2424,13 +2459,20 @@ int scx_fork(struct task_struct *p)
 void scx_post_fork(struct task_struct *p)
 {
 	if (scx_enabled()) {
-		struct rq_flags rf;
-		struct rq *rq;
+		scx_set_task_state(p, SCX_TASK_READY);
+		/*
+		 * Enable the task immediately if it's running on sched_ext.
+		 * Otherwise, it'll be enabled in switching_to_scx() if and
+		 * when it's ever configured to run with a SCHED_EXT policy.
+		 */
+		if (p->sched_class == &ext_sched_class) {
+			struct rq_flags rf;
+			struct rq *rq;
 
-		rq = task_rq_lock(p, &rf);
-		scx_ops_enable_task(p);
-		refresh_scx_weight(p);
-		task_rq_unlock(rq, p, &rf);
+			rq = task_rq_lock(p, &rf);
+			scx_ops_enable_task(p);
+			task_rq_unlock(rq, p, &rf);
+		}
 	}
 
 	spin_lock_irq(&scx_tasks_lock);
@@ -2442,8 +2484,10 @@ void scx_post_fork(struct task_struct *p)
 
 void scx_cancel_fork(struct task_struct *p)
 {
-	if (scx_enabled())
-		scx_ops_disable_task(p);
+	if (scx_enabled()) {
+		WARN_ON_ONCE(scx_get_task_state(p) >= SCX_TASK_READY);
+		scx_ops_exit_task(p);
+	}
 	percpu_up_read(&scx_fork_rwsem);
 }
 
@@ -2456,22 +2500,26 @@ void sched_ext_free(struct task_struct *p)
 	spin_unlock_irqrestore(&scx_tasks_lock, flags);
 
 	/*
-	 * @p is off scx_tasks and wholly ours. scx_ops_enable()'s PREPPED ->
+	 * @p is off scx_tasks and wholly ours. scx_ops_enable()'s READY ->
 	 * ENABLED transitions can't race us. Disable ops for @p.
 	 */
-	if (p->scx.flags & (SCX_TASK_OPS_PREPPED | SCX_TASK_OPS_ENABLED)) {
+	if (scx_get_task_state(p) != SCX_TASK_NONE) {
 		struct rq_flags rf;
 		struct rq *rq;
 
 		rq = task_rq_lock(p, &rf);
-		scx_ops_disable_task(p);
+		scx_ops_exit_task(p);
 		task_rq_unlock(rq, p, &rf);
 	}
 }
 
 static void reweight_task_scx(struct rq *rq, struct task_struct *p, int newprio)
 {
-	refresh_scx_weight(p);
+	lockdep_assert_rq_held(task_rq(p));
+
+	set_task_scx_weight(p);
+	if (SCX_HAS_OP(set_weight))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);
 }
 
 static void prio_changed_scx(struct rq *rq, struct task_struct *p, int oldprio)
@@ -2480,7 +2528,7 @@ static void prio_changed_scx(struct rq *rq, struct task_struct *p, int oldprio)
 
 static void switching_to_scx(struct rq *rq, struct task_struct *p)
 {
-	refresh_scx_weight(p);
+	scx_ops_enable_task(p);
 
 	/*
 	 * set_cpus_allowed_scx() is not called while @p is associated with a
@@ -2491,6 +2539,11 @@ static void switching_to_scx(struct rq *rq, struct task_struct *p)
 				 (struct cpumask *)p->cpus_ptr);
 }
 
+static void switched_from_scx(struct rq *rq, struct task_struct *p)
+{
+	scx_ops_disable_task(p);
+}
+
 static void wakeup_preempt_scx(struct rq *rq, struct task_struct *p,int wake_flags) {}
 static void switched_to_scx(struct rq *rq, struct task_struct *p) {}
 
@@ -2705,7 +2758,7 @@ static inline void scx_cgroup_unlock(void) {}
  * - task_fork/dead: We need fork/dead notifications for all tasks regardless of
  *   their current sched_class. Call them directly from sched core instead.
  *
- * - task_woken, switched_from: Unnecessary.
+ * - task_woken: Unnecessary.
  */
 DEFINE_SCHED_CLASS(ext) = {
 	.enqueue_task		= enqueue_task_scx,
@@ -2736,6 +2789,7 @@ DEFINE_SCHED_CLASS(ext) = {
 	.task_tick		= task_tick_scx,
 
 	.switching_to		= switching_to_scx,
+	.switched_from		= switched_from_scx,
 	.switched_to		= switched_to_scx,
 	.reweight_task		= reweight_task_scx,
 	.prio_changed		= prio_changed_scx,
@@ -3124,7 +3178,7 @@ static void scx_ops_disable_workfn(struct kthread_work *work)
 		if (alive)
 			check_class_changed(task_rq(p), p, old_class, p->prio);
 
-		scx_ops_disable_task(p);
+		scx_ops_exit_task(p);
 	}
 	scx_task_iter_exit(&sti);
 	spin_unlock_irq(&scx_tasks_lock);
@@ -3445,7 +3499,7 @@ static int scx_ops_enable(struct sched_ext_ops *ops)
 			sched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE,
 					       &ctx);
 
-			scx_ops_enable_task(p);
+			scx_set_task_state(p, SCX_TASK_READY);
 			__setscheduler_prio(p, p->prio);
 			check_class_changing(task_rq(p), p, old_class);
 
@@ -3453,7 +3507,7 @@ static int scx_ops_enable(struct sched_ext_ops *ops)
 
 			check_class_changed(task_rq(p), p, old_class, p->prio);
 		} else {
-			scx_ops_disable_task(p);
+			scx_ops_exit_task(p);
 		}
 	}
 	scx_task_iter_exit(&sti);
-- 
2.43.0.232.ge79552d197

