From e8f24f4f695e7c63d7c10db394a445355f9ff389 Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Mon, 8 Jan 2024 10:21:22 -1000
Subject: [PATCH 78/89] scx: Rename rq->scx.watchdog_list and friends to
 runnable_list and counterparts

The list will be used for another purpose too. Rename to indicate the
generic nature.

Signed-off-by: Tejun Heo <tj@kernel.org>
---
 include/linux/sched/ext.h |  6 ++++--
 init/init_task.c          |  2 +-
 kernel/sched/core.c       |  2 +-
 kernel/sched/ext.c        | 41 ++++++++++++++++++++-------------------
 kernel/sched/sched.h      |  2 +-
 5 files changed, 28 insertions(+), 25 deletions(-)

diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
index e5e69587b..90383453d 100644
--- a/include/linux/sched/ext.h
+++ b/include/linux/sched/ext.h
@@ -616,7 +616,7 @@ enum scx_ent_flags {
 	SCX_TASK_STATE_0	= 1 << 3, /* first bit encoding the task's current state */
 	SCX_TASK_STATE_1	= 1 << 4, /* second bit encoding the task's current state */
 
-	SCX_TASK_WATCHDOG_RESET = 1 << 16, /* task watchdog counter should be reset */
+	SCX_TASK_RESET_RUNNABLE_AT = 1 << 16, /* runnable_at should be reset */
 	SCX_TASK_DEQD_FOR_SLEEP	= 1 << 17, /* last dequeue was for SLEEP */
 
 	SCX_TASK_CURSOR		= 1 << 31, /* iteration cursor, not a task */
@@ -680,7 +680,6 @@ struct sched_ext_entity {
 		struct list_head	fifo;	/* dispatch order */
 		struct rb_node		priq;	/* p->scx.dsq_vtime order */
 	} dsq_node;
-	struct list_head	watchdog_node;
 	u32			flags;		/* protected by rq lock */
 	u32			dsq_flags;	/* protected by dsq lock */
 	u32			weight;
@@ -689,7 +688,10 @@ struct sched_ext_entity {
 	u32			kf_mask;	/* see scx_kf_mask above */
 	struct task_struct	*kf_tasks[2];	/* see SCX_CALL_OP_TASK() */
 	atomic_long_t		ops_state;
+
+	struct list_head	runnable_node;	/* rq->scx.runnable_list */
 	unsigned long		runnable_at;
+
 #ifdef CONFIG_SCHED_CORE
 	u64			core_sched_at;	/* see scx_prio_less() */
 #endif
diff --git a/init/init_task.c b/init/init_task.c
index c1a0776a8..1e035992a 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -107,7 +107,7 @@ struct task_struct init_task
 #ifdef CONFIG_SCHED_CLASS_EXT
 	.scx		= {
 		.dsq_node.fifo	= LIST_HEAD_INIT(init_task.scx.dsq_node.fifo),
-		.watchdog_node	= LIST_HEAD_INIT(init_task.scx.watchdog_node),
+		.runnable_node	= LIST_HEAD_INIT(init_task.scx.runnable_node),
 		.flags		= 0,
 		.sticky_cpu	= -1,
 		.holding_cpu	= -1,
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 15af55372..781e8a00b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4555,7 +4555,7 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->scx.dsq		= NULL;
 	INIT_LIST_HEAD(&p->scx.dsq_node.fifo);
 	RB_CLEAR_NODE(&p->scx.dsq_node.priq);
-	INIT_LIST_HEAD(&p->scx.watchdog_node);
+	INIT_LIST_HEAD(&p->scx.runnable_node);
 	p->scx.flags		= 0;
 	p->scx.weight		= 0;
 	p->scx.sticky_cpu	= -1;
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index 096a025da..21906020d 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -946,25 +946,25 @@ static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,
 	dispatch_enqueue(&scx_dsq_global, p, enq_flags);
 }
 
-static bool watchdog_task_watched(const struct task_struct *p)
+static bool task_runnable(const struct task_struct *p)
 {
-	return !list_empty(&p->scx.watchdog_node);
+	return !list_empty(&p->scx.runnable_node);
 }
 
-static void watchdog_watch_task(struct rq *rq, struct task_struct *p)
+static void set_task_runnable(struct rq *rq, struct task_struct *p)
 {
 	lockdep_assert_rq_held(rq);
-	if (p->scx.flags & SCX_TASK_WATCHDOG_RESET)
+	if (p->scx.flags & SCX_TASK_RESET_RUNNABLE_AT)
 		p->scx.runnable_at = jiffies;
-	p->scx.flags &= ~SCX_TASK_WATCHDOG_RESET;
-	list_add_tail(&p->scx.watchdog_node, &rq->scx.watchdog_list);
+	p->scx.flags &= ~SCX_TASK_RESET_RUNNABLE_AT;
+	list_add_tail(&p->scx.runnable_node, &rq->scx.runnable_list);
 }
 
-static void watchdog_unwatch_task(struct task_struct *p, bool reset_timeout)
+static void clr_task_runnable(struct task_struct *p, bool reset_runnable_at)
 {
-	list_del_init(&p->scx.watchdog_node);
-	if (reset_timeout)
-		p->scx.flags |= SCX_TASK_WATCHDOG_RESET;
+	list_del_init(&p->scx.runnable_node);
+	if (reset_runnable_at)
+		p->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;
 }
 
 static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags)
@@ -986,11 +986,11 @@ static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags
 		sticky_cpu = cpu_of(rq);
 
 	if (p->scx.flags & SCX_TASK_QUEUED) {
-		WARN_ON_ONCE(!watchdog_task_watched(p));
+		WARN_ON_ONCE(!task_runnable(p));
 		return;
 	}
 
-	watchdog_watch_task(rq, p);
+	set_task_runnable(rq, p);
 	p->scx.flags |= SCX_TASK_QUEUED;
 	rq->scx.nr_running++;
 	add_nr_running(rq, 1);
@@ -1008,7 +1008,8 @@ static void ops_dequeue(struct task_struct *p, u64 deq_flags)
 {
 	unsigned long opss;
 
-	watchdog_unwatch_task(p, false);
+	/* dequeue is always temporary, don't reset runnable_at */
+	clr_task_runnable(p, false);
 
 	/* acquire ensures that we see the preceding updates on QUEUED */
 	opss = atomic_long_read_acquire(&p->scx.ops_state);
@@ -1055,7 +1056,7 @@ static void dequeue_task_scx(struct rq *rq, struct task_struct *p, int deq_flags
 	struct scx_rq *scx_rq = &rq->scx;
 
 	if (!(p->scx.flags & SCX_TASK_QUEUED)) {
-		WARN_ON_ONCE(watchdog_task_watched(p));
+		WARN_ON_ONCE(task_runnable(p));
 		return;
 	}
 
@@ -1710,7 +1711,7 @@ static void set_next_task_scx(struct rq *rq, struct task_struct *p, bool first)
 	if (SCX_HAS_OP(running) && (p->scx.flags & SCX_TASK_QUEUED))
 		SCX_CALL_OP_TASK(SCX_KF_REST, running, p);
 
-	watchdog_unwatch_task(p, true);
+	clr_task_runnable(p, true);
 
 	/*
 	 * @p is getting newly scheduled or got kicked after someone updated its
@@ -1772,13 +1773,13 @@ static void put_prev_task_scx(struct rq *rq, struct task_struct *p)
 	 */
 	if (p->scx.flags & SCX_TASK_BAL_KEEP) {
 		p->scx.flags &= ~SCX_TASK_BAL_KEEP;
-		watchdog_watch_task(rq, p);
+		set_task_runnable(rq, p);
 		dispatch_enqueue(&rq->scx.local_dsq, p, SCX_ENQ_HEAD);
 		return;
 	}
 
 	if (p->scx.flags & SCX_TASK_QUEUED) {
-		watchdog_watch_task(rq, p);
+		set_task_runnable(rq, p);
 
 		/*
 		 * If @p has slice left and balance_scx() didn't tag it for
@@ -2220,7 +2221,7 @@ static bool check_rq_for_timeouts(struct rq *rq)
 	bool timed_out = false;
 
 	rq_lock_irqsave(rq, &rf);
-	list_for_each_entry(p, &rq->scx.watchdog_list, scx.watchdog_node) {
+	list_for_each_entry(p, &rq->scx.runnable_list, scx.runnable_node) {
 		unsigned long last_runnable = p->scx.runnable_at;
 
 		if (unlikely(time_after(jiffies,
@@ -2375,7 +2376,7 @@ static int scx_ops_init_task(struct task_struct *p, struct task_group *tg)
 		task_rq_unlock(rq, p, &rf);
 	}
 
-	p->scx.flags |= SCX_TASK_WATCHDOG_RESET;
+	p->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;
 	return 0;
 }
 
@@ -3902,7 +3903,7 @@ void __init init_sched_ext_class(void)
 		struct rq *rq = cpu_rq(cpu);
 
 		init_dsq(&rq->scx.local_dsq, SCX_DSQ_LOCAL);
-		INIT_LIST_HEAD(&rq->scx.watchdog_list);
+		INIT_LIST_HEAD(&rq->scx.runnable_list);
 
 		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_kick, GFP_KERNEL));
 		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_preempt, GFP_KERNEL));
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 32eddb62a..ec254f5e3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -690,7 +690,7 @@ enum scx_rq_flags {
 
 struct scx_rq {
 	struct scx_dispatch_q	local_dsq;
-	struct list_head	watchdog_list;
+	struct list_head	runnable_list;		/* runnable tasks on this rq */
 	unsigned long		ops_qseq;
 	u64			extra_enq_flags;	/* see move_task_to_local_dsq() */
 	u32			nr_running;
-- 
2.43.0.232.ge79552d197

