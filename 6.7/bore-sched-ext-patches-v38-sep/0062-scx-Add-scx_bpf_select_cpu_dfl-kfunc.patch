From b0f9c7874c3c4516d027037cecbc299c7cc09b20 Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Fri, 22 Dec 2023 00:30:53 -0600
Subject: [PATCH 062/129] scx: Add scx_bpf_select_cpu_dfl() kfunc

Some scheduler implementations may want to have ops.enqueue() invoked
even if scx_select_cpu_dfl() finds an idle core for the enqueuing task
to run on. In order to enable this, we can add a new
scx_bpf_select_cpu_dfl() kfunc which allows a BPF scheduler to get the
same behavior as the default ops.select_cpu() implementation, and then
decide whether they want to dispatch directly from ops.select_cpu().

Signed-off-by: David Vernet <void@manifault.com>
---
 Documentation/scheduler/sched-ext.rst | 50 ++++++++++++++++++++++++---
 include/linux/sched/ext.h             |  7 ++--
 kernel/sched/ext.c                    | 49 ++++++++++++++++++--------
 3 files changed, 85 insertions(+), 21 deletions(-)

diff --git a/Documentation/scheduler/sched-ext.rst b/Documentation/scheduler/sched-ext.rst
index c2b8dca57..3e1e0a4e9 100644
--- a/Documentation/scheduler/sched-ext.rst
+++ b/Documentation/scheduler/sched-ext.rst
@@ -77,18 +77,59 @@ optional. The following modified excerpt is from
 
 .. code-block:: c
 
-    s32 BPF_STRUCT_OPS(simple_init)
+    /*
+     * Decide which CPU a task should be migrated to before being
+     * enqueued (either at wakeup, fork time, or exec time). If an
+     * idle core is found by the default ops.select_cpu() implementation,
+     * then dispatch the task directly to SCX_DSQ_LOCAL and skip the
+     * ops.enqueue() callback.
+     *
+     * Note that this implemenation has exactly the same behavior as the
+     * default ops.select_cpu implementation. The behavior of the scheduler
+     * would be exactly same if the implementation just didn't define the
+     * simple_select_cpu() struct_ops prog.
+     */
+    s32 BPF_STRUCT_OPS(simple_select_cpu, struct task_struct *p,
+                       s32 prev_cpu, u64 wake_flags)
     {
-            if (!switch_partial)
-                    scx_bpf_switch_all();
-            return 0;
+            s32 cpu;
+            /* Need to initialize or the BPF verifier will reject the program */
+            bool direct = false;
+
+            cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &direct);
+
+            if (direct)
+                    scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, 0);
+
+            return cpu;
     }
 
+    /*
+     * Do a direct dispatch of a task to the global DSQ. This ops.enqueue()
+     * callback will only be invoked if we failed to find a core to dispatch
+     * to in ops.select_cpu() above.
+     *
+     * Note that this implemenation has exactly the same behavior as the
+     * default ops.enqueue implementation, which just dispatches the task
+     * to SCX_DSQ_GLOBAL. The behavior of the scheduler would be exactly same
+     * if the implementation just didn't define the simple_enqueue struct_ops
+     * prog.
+     */
     void BPF_STRUCT_OPS(simple_enqueue, struct task_struct *p, u64 enq_flags)
     {
             scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
     }
 
+    s32 BPF_STRUCT_OPS(simple_init)
+    {
+            /*
+             * All SCHED_OTHER, SCHED_IDLE, and SCHED_BATCH tasks should
+             * use sched_ext.
+             */
+            scx_bpf_switch_all();
+            return 0;
+    }
+
     void BPF_STRUCT_OPS(simple_exit, struct scx_exit_info *ei)
     {
             exit_type = ei->type;
@@ -96,6 +137,7 @@ optional. The following modified excerpt is from
 
     SEC(".struct_ops")
     struct sched_ext_ops simple_ops = {
+            .select_cpu             = (void *)simple_select_cpu,
             .enqueue                = (void *)simple_enqueue,
             .init                   = (void *)simple_init,
             .exit                   = (void *)simple_exit,
diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
index 5096389ed..5a0336349 100644
--- a/include/linux/sched/ext.h
+++ b/include/linux/sched/ext.h
@@ -639,11 +639,12 @@ enum scx_kf_mask {
 	/* ops.dequeue (in REST) may be nested inside DISPATCH */
 	SCX_KF_DISPATCH		= 1 << 3, /* ops.dispatch() */
 	SCX_KF_ENQUEUE		= 1 << 4, /* ops.enqueue() and ops.select_cpu() */
-	SCX_KF_REST		= 1 << 5, /* other rq-locked operations */
+	SCX_KF_SELECT_CPU	= 1 << 5, /* ops.select_cpu() */
+	SCX_KF_REST		= 1 << 6, /* other rq-locked operations */
 
 	__SCX_KF_RQ_LOCKED	= SCX_KF_CPU_RELEASE | SCX_KF_DISPATCH |
-				  SCX_KF_ENQUEUE | SCX_KF_REST,
-	__SCX_KF_TERMINAL	= SCX_KF_ENQUEUE | SCX_KF_REST,
+				  SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
+	__SCX_KF_TERMINAL	= SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
 };
 
 /*
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index 796713aaf..7b4825ec1 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -272,7 +272,7 @@ do {										\
  */
 #define SCX_CALL_OP_TASK(mask, op, task, args...)				\
 do {										\
-	BUILD_BUG_ON(mask & ~__SCX_KF_TERMINAL);				\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
 	current->scx.kf_tasks[0] = task;					\
 	SCX_CALL_OP(mask, op, task, ##args);					\
 	current->scx.kf_tasks[0] = NULL;					\
@@ -281,7 +281,7 @@ do {										\
 #define SCX_CALL_OP_TASK_RET(mask, op, task, args...)				\
 ({										\
 	__typeof__(scx_ops.op(task, ##args)) __ret;				\
-	BUILD_BUG_ON(mask & ~__SCX_KF_TERMINAL);				\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
 	current->scx.kf_tasks[0] = task;					\
 	__ret = SCX_CALL_OP_RET(mask, op, task, ##args);			\
 	current->scx.kf_tasks[0] = NULL;					\
@@ -291,7 +291,7 @@ do {										\
 #define SCX_CALL_OP_2TASKS_RET(mask, op, task0, task1, args...)			\
 ({										\
 	__typeof__(scx_ops.op(task0, task1, ##args)) __ret;			\
-	BUILD_BUG_ON(mask & ~__SCX_KF_TERMINAL);				\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
 	current->scx.kf_tasks[0] = task0;					\
 	current->scx.kf_tasks[1] = task1;					\
 	__ret = SCX_CALL_OP_RET(mask, op, task0, task1, ##args);		\
@@ -2013,10 +2013,13 @@ static s32 scx_pick_idle_cpu(const struct cpumask *cpus_allowed, u64 flags)
 		goto retry;
 }
 
-static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags)
+static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu,
+			      u64 wake_flags, bool *found)
 {
 	s32 cpu;
 
+	*found = false;
+
 	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
 		scx_ops_error("built-in idle tracking is disabled");
 		return prev_cpu;
@@ -2030,7 +2033,7 @@ static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flag
 	    !cpumask_empty(idle_masks.cpu) && !(current->flags & PF_EXITING)) {
 		cpu = smp_processor_id();
 		if (cpumask_test_cpu(cpu, p->cpus_ptr))
-			goto dispatch_local;
+			goto cpu_found;
 	}
 
 	if (p->nr_cpus_allowed == 1)
@@ -2044,30 +2047,41 @@ static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flag
 		if (cpumask_test_cpu(prev_cpu, idle_masks.smt) &&
 		    test_and_clear_cpu_idle(prev_cpu)) {
 			cpu = prev_cpu;
-			goto dispatch_local;
+			goto cpu_found;
 		}
 
 		cpu = scx_pick_idle_cpu(p->cpus_ptr, SCX_PICK_IDLE_CORE);
 		if (cpu >= 0)
-			goto dispatch_local;
+			goto cpu_found;
 	}
 
 	if (test_and_clear_cpu_idle(prev_cpu)) {
 		cpu = prev_cpu;
-		goto dispatch_local;
+		goto cpu_found;
 	}
 
 	cpu = scx_pick_idle_cpu(p->cpus_ptr, 0);
 	if (cpu >= 0)
-		goto dispatch_local;
+		goto cpu_found;
 
 	return prev_cpu;
 
-dispatch_local:
-	p->scx.ddsq_id = SCX_DSQ_LOCAL;
+cpu_found:
+	*found = true;
 	return cpu;
 }
 
+s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags,
+			   bool *found)
+{
+	if (!scx_kf_allowed(SCX_KF_SELECT_CPU)) {
+		*found = false;
+		return prev_cpu;
+	}
+
+	return scx_select_cpu_dfl(p, prev_cpu, wake_flags, found);
+}
+
 static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flags)
 {
 	if (SCX_HAS_OP(select_cpu)) {
@@ -2078,8 +2092,8 @@ static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flag
 		WARN_ON_ONCE(*ddsp_taskp);
 		*ddsp_taskp = p;
 
-		cpu = SCX_CALL_OP_TASK_RET(SCX_KF_ENQUEUE, select_cpu, p, prev_cpu,
-					   wake_flags);
+		cpu = SCX_CALL_OP_TASK_RET(SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU,
+					   select_cpu, p, prev_cpu, wake_flags);
 		*ddsp_taskp = NULL;
 		if (ops_cpu_valid(cpu)) {
 			return cpu;
@@ -2088,7 +2102,13 @@ static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flag
 			return prev_cpu;
 		}
 	} else {
-		return scx_select_cpu_dfl(p, prev_cpu, wake_flags);
+		bool found;
+		s32 cpu;
+
+		cpu = scx_select_cpu_dfl(p, prev_cpu, wake_flags, &found);
+		if (found)
+			p->scx.ddsq_id = SCX_DSQ_LOCAL;
+		return cpu;
 	}
 }
 
@@ -4487,6 +4507,7 @@ BTF_ID_FLAGS(func, scx_bpf_test_and_clear_cpu_idle)
 BTF_ID_FLAGS(func, scx_bpf_pick_idle_cpu, KF_RCU)
 BTF_ID_FLAGS(func, scx_bpf_pick_any_cpu, KF_RCU)
 BTF_ID_FLAGS(func, scx_bpf_destroy_dsq)
+BTF_ID_FLAGS(func, scx_bpf_select_cpu_dfl, KF_RCU)
 BTF_SET8_END(scx_kfunc_ids_ops_only)
 
 static const struct btf_kfunc_id_set scx_kfunc_set_ops_only = {
-- 
2.43.0.232.ge79552d197

