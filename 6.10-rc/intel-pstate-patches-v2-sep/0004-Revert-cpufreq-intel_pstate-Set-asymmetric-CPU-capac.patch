From c85c894863cbd416063b1f21c3d085d31b199d70 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Tue, 2 Jul 2024 20:24:02 +0200
Subject: [PATCH 4/9] Revert "cpufreq: intel_pstate: Set asymmetric CPU
 capacity on hybrid systems"

This reverts commit 9c3a95d698f33fd458287adbed58e56b09fd797f.
---
 drivers/cpufreq/intel_pstate.c | 199 +--------------------------------
 1 file changed, 4 insertions(+), 195 deletions(-)

diff --git a/drivers/cpufreq/intel_pstate.c b/drivers/cpufreq/intel_pstate.c
index c6388974a..c31914a98 100644
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@ -16,7 +16,6 @@
 #include <linux/tick.h>
 #include <linux/slab.h>
 #include <linux/sched/cpufreq.h>
-#include <linux/sched/smt.h>
 #include <linux/list.h>
 #include <linux/cpu.h>
 #include <linux/cpufreq.h>
@@ -216,8 +215,6 @@ struct global_params {
  * @hwp_req_cached:	Cached value of the last HWP Request MSR
  * @hwp_cap_cached:	Cached value of the last HWP Capabilities MSR
  * @last_io_update:	Last time when IO wake flag was set
- * @capacity_perf:	Perf from HWP_CAP used for capacity computations
- * @max_freq_ratio:	Max to base frequency ratio times SCHED_CAPACITY_SCALE
  * @sched_flags:	Store scheduler flags for possible cross CPU update
  * @hwp_boost_min:	Last HWP boosted min performance
  * @suspended:		Whether or not the driver has been suspended.
@@ -256,8 +253,6 @@ struct cpudata {
 	u64 hwp_req_cached;
 	u64 hwp_cap_cached;
 	u64 last_io_update;
-	unsigned int capacity_perf;
-	unsigned int max_freq_ratio;
 	unsigned int sched_flags;
 	u32 hwp_boost_min;
 	bool suspended;
@@ -300,7 +295,6 @@ static int hwp_mode_bdw __ro_after_init;
 static bool per_cpu_limits __ro_after_init;
 static bool hwp_forced __ro_after_init;
 static bool hwp_boost __read_mostly;
-static bool hwp_is_hybrid;
 
 static struct cpufreq_driver *intel_pstate_driver __read_mostly;
 
@@ -939,105 +933,6 @@ static struct freq_attr *hwp_cpufreq_attrs[] = {
 	NULL,
 };
 
-static void intel_pstate_set_cpu_capacity(struct cpudata *cpu, u64 cap)
-{
-	arch_set_cpu_capacity(cpu->cpu, cap, cpu->max_freq_ratio);
-}
-
-static void intel_pstate_clear_cpu_capacity(unsigned int cpunum)
-{
-	arch_set_cpu_capacity(cpunum, SCHED_CAPACITY_SCALE, 0);
-}
-
-static struct cpudata *hybrid_max_perf_cpu __read_mostly;
-/*
- * This protects hybrid_max_perf_cpu, the @capacity_perf fields in struct
- * cpudata, and the x86 arch capacity information from concurrent updates.
- */
-static DEFINE_MUTEX(hybrid_capacity_lock);
-
-static void hybrid_get_cap_perf(struct cpudata *cpu)
-{
-	u64 cap_perf;
-
-	if (READ_ONCE(global.no_turbo))
-		cap_perf = cpu->pstate.max_pstate_physical;
-	else
-		cap_perf = HWP_HIGHEST_PERF(READ_ONCE(cpu->hwp_cap_cached));
-
-	cpu->capacity_perf = cap_perf;
-	cpu->max_freq_ratio = div_u64(cap_perf << SCHED_CAPACITY_SHIFT,
-				      cpu->pstate.max_pstate_physical);
-}
-
-static void hybrid_set_cpu_capacity(struct cpudata *cpu)
-{
-	u64 cap = div_u64((u64)cpu->capacity_perf << SCHED_CAPACITY_SHIFT,
-			  hybrid_max_perf_cpu->capacity_perf);
-
-	intel_pstate_set_cpu_capacity(cpu, cap);
-}
-
-static void hybrid_set_capacity_of_cpus(void)
-{
-	int cpunum;
-
-	for_each_online_cpu(cpunum) {
-		struct cpudata *cpu = all_cpu_data[cpunum];
-
-		/*
-		 * Skip hybrid_max_perf_cpu because its capacity is the
-		 * maximum and need not be computed.
-		 */
-		if (cpu && cpu != hybrid_max_perf_cpu)
-			hybrid_set_cpu_capacity(cpu);
-	}
-}
-
-static void hybrid_update_cpu_scaling(void)
-{
-	struct cpudata *max_perf_cpu = NULL;
-	unsigned int max_cap_perf = 0;
-	int cpunum;
-
-	for_each_online_cpu(cpunum) {
-		struct cpudata *cpu = all_cpu_data[cpunum];
-
-		/*
-		 * If hybrid_max_perf_cpu is not NULL at this point, it is
-		 * being replaced, so skip it.
-		 */
-		if (!cpu || cpu == hybrid_max_perf_cpu)
-			continue;
-
-		hybrid_get_cap_perf(cpu);
-		if (cpu->capacity_perf > max_cap_perf) {
-			max_cap_perf = cpu->capacity_perf;
-			max_perf_cpu = cpu;
-		}
-	}
-
-	if (max_perf_cpu) {
-		intel_pstate_set_cpu_capacity(max_perf_cpu, SCHED_CAPACITY_SCALE);
-		hybrid_max_perf_cpu = max_perf_cpu;
-		hybrid_set_capacity_of_cpus();
-	} else {
-		/* Revert to the flat CPU capacity structure. */
-		for_each_online_cpu(cpunum)
-			intel_pstate_clear_cpu_capacity(cpunum);
-	}
-}
-
-static void hybrid_init_cpu_scaling(void)
-{
-	mutex_lock(&hybrid_capacity_lock);
-
-	hybrid_max_perf_cpu = NULL;
-	hybrid_update_cpu_scaling();
-
-	mutex_unlock(&hybrid_capacity_lock);
-}
-
 static void __intel_pstate_get_hwp_cap(struct cpudata *cpu)
 {
 	u64 cap;
@@ -1066,38 +961,6 @@ static void intel_pstate_get_hwp_cap(struct cpudata *cpu)
 	}
 }
 
-static void hybrid_update_capacity(struct cpudata *cpu)
-{
-	unsigned int max_cap_perf;
-
-	mutex_lock(&hybrid_capacity_lock);
-
-	if (!hybrid_max_perf_cpu)
-		goto unlock;
-
-	max_cap_perf = hybrid_max_perf_cpu->capacity_perf;
-
-	intel_pstate_get_hwp_cap(cpu);
-
-	hybrid_get_cap_perf(cpu);
-	if (cpu->capacity_perf > max_cap_perf) {
-		intel_pstate_set_cpu_capacity(cpu, SCHED_CAPACITY_SCALE);
-		hybrid_max_perf_cpu = cpu;
-		hybrid_set_capacity_of_cpus();
-		goto unlock;
-	}
-
-	if (cpu == hybrid_max_perf_cpu && cpu->capacity_perf < max_cap_perf) {
-		hybrid_update_cpu_scaling();
-		goto unlock;
-	}
-
-	hybrid_set_cpu_capacity(cpu);
-
-unlock:
-	mutex_unlock(&hybrid_capacity_lock);
-}
-
 static void intel_pstate_hwp_set(unsigned int cpu)
 {
 	struct cpudata *cpu_data = all_cpu_data[cpu];
@@ -1206,16 +1069,6 @@ static void intel_pstate_hwp_offline(struct cpudata *cpu)
 		value |= HWP_ENERGY_PERF_PREFERENCE(HWP_EPP_POWERSAVE);
 
 	wrmsrl_on_cpu(cpu->cpu, MSR_HWP_REQUEST, value);
-
-	mutex_lock(&hybrid_capacity_lock);
-
-	if (hybrid_max_perf_cpu == cpu)
-		hybrid_update_cpu_scaling();
-
-	mutex_unlock(&hybrid_capacity_lock);
-
-	/* Reset the capacity of the CPU going offline to the initial value. */
-	intel_pstate_clear_cpu_capacity(cpu->cpu);
 }
 
 #define POWER_CTL_EE_ENABLE	1
@@ -1311,41 +1164,21 @@ static void __intel_pstate_update_max_freq(struct cpudata *cpudata,
 static void intel_pstate_update_limits(unsigned int cpu)
 {
 	struct cpufreq_policy *policy = cpufreq_cpu_acquire(cpu);
-	struct cpudata *cpudata;
 
 	if (!policy)
 		return;
 
-	cpudata = all_cpu_data[cpu];
-
-	__intel_pstate_update_max_freq(cpudata, policy);
-
-	/* Prevent the driver from being unregistered now. */
-	mutex_lock(&intel_pstate_driver_lock);
+	__intel_pstate_update_max_freq(all_cpu_data[cpu], policy);
 
 	cpufreq_cpu_release(policy);
-
-	hybrid_update_capacity(cpudata);
-
-	mutex_unlock(&intel_pstate_driver_lock);
 }
 
 static void intel_pstate_update_limits_for_all(void)
 {
 	int cpu;
 
-	for_each_possible_cpu(cpu) {
-		struct cpufreq_policy *policy = cpufreq_cpu_acquire(cpu);
-
-		if (!policy)
-			continue;
-
-		__intel_pstate_update_max_freq(all_cpu_data[cpu], policy);
-
-		cpufreq_cpu_release(policy);
-	}
-
-	hybrid_init_cpu_scaling();
+	for_each_possible_cpu(cpu)
+		intel_pstate_update_limits(cpu);
 }
 
 /************************** sysfs begin ************************/
@@ -1784,13 +1617,6 @@ static void intel_pstate_notify_work(struct work_struct *work)
 		__intel_pstate_update_max_freq(cpudata, policy);
 
 		cpufreq_cpu_release(policy);
-
-		/*
-		 * The driver will not be unregistered while this function is
-		 * running, so update the capacity without acquiring the driver
-		 * lock.
-		 */
-		hybrid_update_capacity(cpudata);
 	}
 
 	wrmsrl_on_cpu(cpudata->cpu, MSR_HWP_STATUS, 0);
@@ -2192,10 +2018,8 @@ static void intel_pstate_get_cpu_pstates(struct cpudata *cpu)
 
 		if (pstate_funcs.get_cpu_scaling) {
 			cpu->pstate.scaling = pstate_funcs.get_cpu_scaling(cpu->cpu);
-			if (cpu->pstate.scaling != perf_ctl_scaling) {
+			if (cpu->pstate.scaling != perf_ctl_scaling)
 				intel_pstate_hybrid_hwp_adjust(cpu);
-				hwp_is_hybrid = true;
-			}
 		} else {
 			cpu->pstate.scaling = perf_ctl_scaling;
 		}
@@ -2863,8 +2687,6 @@ static int intel_pstate_cpu_online(struct cpufreq_policy *policy)
 		 */
 		intel_pstate_hwp_reenable(cpu);
 		cpu->suspended = false;
-
-		hybrid_update_capacity(cpu);
 	}
 
 	return 0;
@@ -3307,19 +3129,6 @@ static int intel_pstate_register_driver(struct cpufreq_driver *driver)
 
 	global.min_perf_pct = min_perf_pct_min();
 
-	/*
-	 * On hybrid systems, use asym capacity instead of ITMT, but because
-	 * the capacity of SMT threads is not deterministic even approximately,
-	 * do not do that when SMT is in use.
-	 */
-	if (hwp_is_hybrid && !sched_smt_active()) {
-		sched_clear_itmt_support();
-
-		hybrid_init_cpu_scaling();
-
-		arch_rebuild_sched_domains();
-	}
-
 	return 0;
 }
 
-- 
2.45.2.606.g9005149a4a

