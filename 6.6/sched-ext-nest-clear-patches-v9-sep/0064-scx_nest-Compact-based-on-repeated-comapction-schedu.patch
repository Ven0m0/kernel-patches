From 3608fb14771a0579a63f5ac4a6718bc2928d9d82 Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Thu, 7 Dec 2023 12:21:03 -0600
Subject: [PATCH 64/68] scx_nest: Compact based on repeated comapction
 schedulings

When deciding whether to compact a nest, we're currently looking at how
much work was scheduled on the core before it's scheduled for a
compaction. This is not a great solution though. The whole point of nest
is that we're trying to keep a core warm on the assumption of temporal
locality, and that it will be used again soon. What would be preferable
is to aggressively compact it if it's repeatedly being scheduled for
compaction, as it should only be repeatedly scheduled for compaction if
there aren't enough tasks to fully utilize the primary nest.

Signed-off-by: David Vernet <void@manifault.com>
---
 tools/sched_ext/scx_nest.bpf.c | 52 ++++++++++++++++++----------------
 tools/sched_ext/scx_nest.c     |  2 +-
 2 files changed, 29 insertions(+), 25 deletions(-)

diff --git a/tools/sched_ext/scx_nest.bpf.c b/tools/sched_ext/scx_nest.bpf.c
index d60636e8d..3ab6d52d0 100644
--- a/tools/sched_ext/scx_nest.bpf.c
+++ b/tools/sched_ext/scx_nest.bpf.c
@@ -50,7 +50,7 @@ const volatile u64 r_impatient = 2;
 const volatile u64 slice_ns = SCX_SLICE_DFL;
 const volatile bool find_fully_idle = false;
 const volatile u64 sampling_cadence_ns = 1 * NSEC_PER_SEC;
-const volatile u64 r_depth = 6;
+const volatile u64 r_depth = 5;
 
 // Used for stats tracking. May be stale at any given time.
 u64 stats_primary_mask, stats_reserved_mask, stats_other_mask, stats_idle_mask;
@@ -111,8 +111,8 @@ struct pcpu_ctx {
 	/* Whether the current core has been scheduled for compaction. */
 	bool scheduled_compaction;
 
-	/* Number of tasks active on the core. */
-	u32 num_active;
+	/* Number of times a primary core has been scheduled for compaction. */
+	u32 num_schedulings;
 };
 
 struct {
@@ -373,7 +373,6 @@ void BPF_STRUCT_OPS(nest_enqueue, struct task_struct *p, u64 enq_flags)
 	struct task_ctx *tctx;
 	u64 vtime = p->scx.dsq_vtime;
 	s32 cpu = bpf_get_smp_processor_id();
-	struct pcpu_ctx *pcpu_ctx;
 
 	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
 	if (!tctx) {
@@ -387,11 +386,6 @@ void BPF_STRUCT_OPS(nest_enqueue, struct task_struct *p, u64 enq_flags)
 			update_attached(tctx, tctx->prev_cpu, cpu);
 
 		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, enq_flags);
-		pcpu_ctx = bpf_map_lookup_elem(&pcpu_ctxs, &cpu);
-		if (!pcpu_ctx)
-			scx_bpf_error("Failed to lookup pcpu ctx");
-		else
-			pcpu_ctx->num_active++;
 		return;
 	}
 
@@ -420,30 +414,41 @@ void BPF_STRUCT_OPS(nest_dispatch, s32 cpu, struct task_struct *prev)
 		return;
 	}
 
-	pcpu_ctx = bpf_map_lookup_elem(&pcpu_ctxs, &key);
-	if (!pcpu_ctx) {
-		scx_bpf_error("Failed to lookup pcpu ctx");
-		return;
-	}
-	in_primary = bpf_cpumask_test_cpu(cpu, cast_mask(primary));
 	if (!scx_bpf_consume(FALLBACK_DSQ_ID)) {
+		in_primary = bpf_cpumask_test_cpu(cpu, cast_mask(primary));
+
 		if (prev && (prev->scx.flags & SCX_TASK_QUEUED) && in_primary) {
-			pcpu_ctx->num_active++;
 			scx_bpf_dispatch(prev, SCX_DSQ_LOCAL, slice_ns, 0);
 			return;
 		}
 
 		stat_inc(NEST_STAT(NOT_CONSUMED));
 		if (in_primary) {
+			pcpu_ctx = bpf_map_lookup_elem(&pcpu_ctxs, &key);
+			if (!pcpu_ctx) {
+				scx_bpf_error("Failed to lookup pcpu ctx");
+				return;
+			}
+
 			/*
-			 * Always immediately demote a primary core if we
-			 * couldn't consume, and the previous task is dying.
+			 * Immediately demote a primary core if:
+			 * - It's been scheduled for compaction at least
+			 *   r_depth times without actually being compacted.
+			 * - The previous task on it is dying
+			 *
+			 * Note that we elect to not compact the "first" CPU in
+			 * the mask so as to encourage at least one core to
+			 * remain in the nest. It would be better to check for
+			 * whether there is only one core remaining in the
+			 * nest, but BPF doesn't yet have a kfunc for querying
+			 * cpumask weight.
 			 */
-			if (bpf_cpumask_first(cast_mask(primary)) != cpu &&
-			    (pcpu_ctx->num_active < r_depth || (prev && prev->__state == TASK_DEAD))) {
+			if ((prev && prev->__state == TASK_DEAD) ||
+			    (cpu != bpf_cpumask_first(cast_mask(primary)) && pcpu_ctx->num_schedulings >= r_depth)) {
 				stat_inc(NEST_STAT(COMPACTED));
 				bpf_cpumask_clear_cpu(cpu, primary);
 				try_make_core_reserved(cpu, reserve, false);
+				pcpu_ctx->num_schedulings = 0;
 			} else  {
 				pcpu_ctx->scheduled_compaction = true;
 				/*
@@ -453,15 +458,13 @@ void BPF_STRUCT_OPS(nest_dispatch, s32 cpu, struct task_struct *prev)
 				 */
 				bpf_timer_start(&pcpu_ctx->timer, p_remove_ns,
 						0 /*BPF_F_TIMER_CPU_PIN*/);
+				pcpu_ctx->num_schedulings++;
 				stat_inc(NEST_STAT(SCHEDULED_COMPACTION));
 			}
-			pcpu_ctx->num_active = 0;
 		}
 		return;
 	}
 	stat_inc(NEST_STAT(CONSUMED));
-	if (in_primary)
-		pcpu_ctx->num_active++;
 }
 
 void BPF_STRUCT_OPS(nest_running, struct task_struct *p)
@@ -546,6 +549,7 @@ static int compact_primary_core(void *map, int *key, struct bpf_timer *timer)
 	bpf_cpumask_clear_cpu(cpu, primary);
 	try_make_core_reserved(cpu, reserve, false);
 	bpf_rcu_read_unlock();
+	pcpu_ctx->num_schedulings = 0;
 	pcpu_ctx->scheduled_compaction = false;
 	return 0;
 }
@@ -638,7 +642,7 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(nest_init)
 			scx_bpf_error("Failed to initialize pcpu timer");
 			return -EINVAL;
 		}
-		ctx->num_active  = 0;
+		ctx->num_schedulings  = 0;
 		bpf_timer_set_callback(&ctx->timer, compact_primary_core);
 	}
 
diff --git a/tools/sched_ext/scx_nest.c b/tools/sched_ext/scx_nest.c
index 5d59e4824..90f5a8bd2 100644
--- a/tools/sched_ext/scx_nest.c
+++ b/tools/sched_ext/scx_nest.c
@@ -27,7 +27,7 @@ const char help_fmt[] =
 "  -m R_MAX      Maximum number of cores in the reserve nest (default 5)\n"
 "  -i ITERS      Number of successive placement failures tolerated before trying to aggressively expand primary nest (default 2), or 0 to disable\n"
 "  -s SLICE_US   Override slice duration in us (default 20000us / 20ms)\n"
-"  -D R_DECAY    Override the depth of work before which a core will be decayed\n"
+"  -D R_SCHED    Override the number of times that a core may be scheduled for compaction before having compaction happen immediately (default 5), or -1 to disable\n"
 "  -I            First try to find a fully idle core, and then any idle core, when searching nests. Default behavior is to ignore hypertwins and check for any idle core.\n"
 "  -h            Display this help and exit\n";
 
-- 
2.43.0.rc2

