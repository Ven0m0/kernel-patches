From 1d9a3d0073cf5bbba3c582af11c40c150ff924ce Mon Sep 17 00:00:00 2001
From: David Vernet <void@manifault.com>
Date: Fri, 1 Dec 2023 16:16:43 -0600
Subject: [PATCH 63/69] nest: Implement more aggressive compaction decay

Because scx_nest spreads tasks aggressively across idle cores, it seems
that existing mechanisms for nest compaction aren't sufficiently
aggressive so as to prevent tasks from splaying out to different cores
rather than taking cores in a specific nest. We therefore implement a
strategy where we compact a core from a nest when it would otherwise go
idle if it hasn't been scheduled on at least r_depth times.

Signed-off-by: David Vernet <void@manifault.com>
---
 tools/sched_ext/scx_nest.bpf.c | 63 +++++++++++++++++++++++++++-------
 tools/sched_ext/scx_nest.c     |  6 +++-
 2 files changed, 56 insertions(+), 13 deletions(-)

diff --git a/tools/sched_ext/scx_nest.bpf.c b/tools/sched_ext/scx_nest.bpf.c
index 3eed2a4e0..d60636e8d 100644
--- a/tools/sched_ext/scx_nest.bpf.c
+++ b/tools/sched_ext/scx_nest.bpf.c
@@ -27,6 +27,8 @@
 #include "vmlinux.h"
 #include "scx_nest.h"
 
+#define TASK_DEAD                       0x00000080
+
 char _license[] SEC("license") = "GPL";
 
 enum {
@@ -48,6 +50,7 @@ const volatile u64 r_impatient = 2;
 const volatile u64 slice_ns = SCX_SLICE_DFL;
 const volatile bool find_fully_idle = false;
 const volatile u64 sampling_cadence_ns = 1 * NSEC_PER_SEC;
+const volatile u64 r_depth = 6;
 
 // Used for stats tracking. May be stale at any given time.
 u64 stats_primary_mask, stats_reserved_mask, stats_other_mask, stats_idle_mask;
@@ -107,6 +110,9 @@ struct pcpu_ctx {
 
 	/* Whether the current core has been scheduled for compaction. */
 	bool scheduled_compaction;
+
+	/* Number of tasks active on the core. */
+	u32 num_active;
 };
 
 struct {
@@ -218,6 +224,7 @@ s32 BPF_STRUCT_OPS(nest_select_cpu, struct task_struct *p, s32 prev_cpu,
 
 	// Unset below if we can't find a core to migrate to.
 	tctx->force_local = true;
+	tctx->prev_cpu = prev_cpu;
 
 	bpf_cpumask_and(p_mask, p->cpus_ptr, cast_mask(primary));
 
@@ -324,7 +331,6 @@ s32 BPF_STRUCT_OPS(nest_select_cpu, struct task_struct *p, s32 prev_cpu,
 
 	bpf_rcu_read_unlock();
 	tctx->force_local = false;
-	tctx->prev_cpu = prev_cpu;
 	return prev_cpu;
 
 promote_to_primary:
@@ -366,6 +372,8 @@ void BPF_STRUCT_OPS(nest_enqueue, struct task_struct *p, u64 enq_flags)
 {
 	struct task_ctx *tctx;
 	u64 vtime = p->scx.dsq_vtime;
+	s32 cpu = bpf_get_smp_processor_id();
+	struct pcpu_ctx *pcpu_ctx;
 
 	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
 	if (!tctx) {
@@ -373,9 +381,17 @@ void BPF_STRUCT_OPS(nest_enqueue, struct task_struct *p, u64 enq_flags)
 		return;
 	}
 
-	if (tctx->force_local) {
+	if (tctx->force_local || (enq_flags & SCX_ENQ_LOCAL)) {
 		tctx->force_local = false;
+		if (enq_flags & SCX_ENQ_LOCAL)
+			update_attached(tctx, tctx->prev_cpu, cpu);
+
 		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, enq_flags);
+		pcpu_ctx = bpf_map_lookup_elem(&pcpu_ctxs, &cpu);
+		if (!pcpu_ctx)
+			scx_bpf_error("Failed to lookup pcpu ctx");
+		else
+			pcpu_ctx->num_active++;
 		return;
 	}
 
@@ -393,13 +409,14 @@ void BPF_STRUCT_OPS(nest_enqueue, struct task_struct *p, u64 enq_flags)
 void BPF_STRUCT_OPS(nest_dispatch, s32 cpu, struct task_struct *prev)
 {
 	struct pcpu_ctx *pcpu_ctx;
-	struct bpf_cpumask *primary;
+	struct bpf_cpumask *primary, *reserve;
 	s32 key = cpu;
 	bool in_primary;
 
 	primary = primary_cpumask;
-	if (!primary) {
-		scx_bpf_error("No primary cpumask");
+	reserve = reserve_cpumask;
+	if (!primary || !reserve) {
+		scx_bpf_error("No primary or reserve cpumask");
 		return;
 	}
 
@@ -410,21 +427,41 @@ void BPF_STRUCT_OPS(nest_dispatch, s32 cpu, struct task_struct *prev)
 	}
 	in_primary = bpf_cpumask_test_cpu(cpu, cast_mask(primary));
 	if (!scx_bpf_consume(FALLBACK_DSQ_ID)) {
+		if (prev && (prev->scx.flags & SCX_TASK_QUEUED) && in_primary) {
+			pcpu_ctx->num_active++;
+			scx_bpf_dispatch(prev, SCX_DSQ_LOCAL, slice_ns, 0);
+			return;
+		}
+
 		stat_inc(NEST_STAT(NOT_CONSUMED));
 		if (in_primary) {
-			pcpu_ctx->scheduled_compaction = true;
 			/*
-			 * The core isn't being used anymore. Set a timer to
-			 * remove the core from the nest in p_remove if it's
-			 * still unused by that point.
+			 * Always immediately demote a primary core if we
+			 * couldn't consume, and the previous task is dying.
 			 */
-			bpf_timer_start(&pcpu_ctx->timer, p_remove_ns,
-					0 /*BPF_F_TIMER_CPU_PIN*/);
-			stat_inc(NEST_STAT(SCHEDULED_COMPACTION));
+			if (bpf_cpumask_first(cast_mask(primary)) != cpu &&
+			    (pcpu_ctx->num_active < r_depth || (prev && prev->__state == TASK_DEAD))) {
+				stat_inc(NEST_STAT(COMPACTED));
+				bpf_cpumask_clear_cpu(cpu, primary);
+				try_make_core_reserved(cpu, reserve, false);
+			} else  {
+				pcpu_ctx->scheduled_compaction = true;
+				/*
+				 * The core isn't being used anymore. Set a
+				 * timer to remove the core from the nest in
+				 * p_remove if it's still unused by that point.
+				 */
+				bpf_timer_start(&pcpu_ctx->timer, p_remove_ns,
+						0 /*BPF_F_TIMER_CPU_PIN*/);
+				stat_inc(NEST_STAT(SCHEDULED_COMPACTION));
+			}
+			pcpu_ctx->num_active = 0;
 		}
 		return;
 	}
 	stat_inc(NEST_STAT(CONSUMED));
+	if (in_primary)
+		pcpu_ctx->num_active++;
 }
 
 void BPF_STRUCT_OPS(nest_running, struct task_struct *p)
@@ -601,6 +638,7 @@ s32 BPF_STRUCT_OPS_SLEEPABLE(nest_init)
 			scx_bpf_error("Failed to initialize pcpu timer");
 			return -EINVAL;
 		}
+		ctx->num_active  = 0;
 		bpf_timer_set_callback(&ctx->timer, compact_primary_core);
 	}
 
@@ -634,5 +672,6 @@ struct sched_ext_ops nest_ops = {
 	.enable			= (void *)nest_enable,
 	.init			= (void *)nest_init,
 	.exit			= (void *)nest_exit,
+	.flags			= 0,
 	.name			= "nest",
 };
diff --git a/tools/sched_ext/scx_nest.c b/tools/sched_ext/scx_nest.c
index 18579ea0b..5d59e4824 100644
--- a/tools/sched_ext/scx_nest.c
+++ b/tools/sched_ext/scx_nest.c
@@ -27,6 +27,7 @@ const char help_fmt[] =
 "  -m R_MAX      Maximum number of cores in the reserve nest (default 5)\n"
 "  -i ITERS      Number of successive placement failures tolerated before trying to aggressively expand primary nest (default 2), or 0 to disable\n"
 "  -s SLICE_US   Override slice duration in us (default 20000us / 20ms)\n"
+"  -D R_DECAY    Override the depth of work before which a core will be decayed\n"
 "  -I            First try to find a fully idle core, and then any idle core, when searching nests. Default behavior is to ignore hypertwins and check for any idle core.\n"
 "  -h            Display this help and exit\n";
 
@@ -163,11 +164,14 @@ int main(int argc, char **argv)
 	skel->rodata->nr_cpus = libbpf_num_possible_cpus();
 	skel->rodata->sampling_cadence_ns = SAMPLING_CADENCE_S * 1000 * 1000 * 1000;
 
-	while ((opt = getopt(argc, argv, "hId:m:i:s:")) != -1) {
+	while ((opt = getopt(argc, argv, "hId:D:m:i:s:")) != -1) {
 		switch (opt) {
 		case 'd':
 			skel->rodata->p_remove_ns = strtoull(optarg, NULL, 0) * 1000;
 			break;
+		case 'D':
+			skel->rodata->r_depth = strtoull(optarg, NULL, 0);
+			break;
 		case 'm':
 			skel->rodata->r_max = strtoull(optarg, NULL, 0);
 			break;
-- 
2.43.0.232.ge79552d197

