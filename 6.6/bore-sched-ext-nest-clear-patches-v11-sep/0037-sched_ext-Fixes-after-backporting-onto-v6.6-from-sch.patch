From 0e8a328defe394a3399d2e4bf8072bc87c3db69a Mon Sep 17 00:00:00 2001
From: Tejun Heo <tj@kernel.org>
Date: Tue, 21 Nov 2023 13:26:45 -1000
Subject: [PATCH 37/68] sched_ext: Fixes after backporting onto v6.6 from
 sched_ext-v5

This series is backport of sched_ext-v5 (597552f5447a) which is the split
version of devel branch commit e69323c41c71 ("Merge pull request #85 from
sched-ext/misc-fixes ").

This commit makes the following fixes for backport:

- s/wakeup_preempt_scx/check_preempt_curr_scx/ to revert the rename which
  happened post v6.6.

- Remove the use of BPF_F_TIMER_CPU_PIN which was introduced after v6.6.
---
 kernel/sched/ext.c                | 11 ++++++-----
 tools/sched_ext/scx_central.bpf.c | 12 ++++++++----
 2 files changed, 14 insertions(+), 9 deletions(-)

diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
index ebe295eb7..a4d3d8397 100644
--- a/kernel/sched/ext.c
+++ b/kernel/sched/ext.c
@@ -2436,7 +2436,7 @@ static void switching_to_scx(struct rq *rq, struct task_struct *p)
 				 (struct cpumask *)p->cpus_ptr);
 }
 
-static void wakeup_preempt_scx(struct rq *rq, struct task_struct *p,int wake_flags) {}
+static void check_preempt_curr_scx(struct rq *rq, struct task_struct *p,int wake_flags) {}
 static void switched_to_scx(struct rq *rq, struct task_struct *p) {}
 
 int scx_check_setscheduler(struct task_struct *p, int policy)
@@ -2634,9 +2634,10 @@ static inline void scx_cgroup_unlock(void) {}
 /*
  * Omitted operations:
  *
- * - wakeup_preempt: NOOP as it isn't useful in the wakeup path because the task
- *   isn't tied to the CPU at that point. Preemption is implemented by resetting
- *   the victim task's slice to 0 and triggering reschedule on the target CPU.
+ * - check_preempt_curr: NOOP as it isn't useful in the wakeup path because the
+ *   task isn't tied to the CPU at that point. Preemption is implemented by
+ *   resetting the victim task's slice to 0 and triggering reschedule on the
+ *   target CPU.
  *
  * - migrate_task_rq: Unncessary as task to cpu mapping is transient.
  *
@@ -2651,7 +2652,7 @@ DEFINE_SCHED_CLASS(ext) = {
 	.yield_task		= yield_task_scx,
 	.yield_to_task		= yield_to_task_scx,
 
-	.wakeup_preempt		= wakeup_preempt_scx,
+	.check_preempt_curr	= check_preempt_curr_scx,
 
 	.pick_next_task		= pick_next_task_scx,
 
diff --git a/tools/sched_ext/scx_central.bpf.c b/tools/sched_ext/scx_central.bpf.c
index de0577961..890e97e22 100644
--- a/tools/sched_ext/scx_central.bpf.c
+++ b/tools/sched_ext/scx_central.bpf.c
@@ -255,11 +255,15 @@ static int central_timerfn(void *map, int *key, struct bpf_timer *timer)
 	s32 i, curr_cpu;
 
 	curr_cpu = bpf_get_smp_processor_id();
-	if (curr_cpu != central_cpu) {
+	/*
+	 * XXX BACKPORT NOTE - BPF_F_TIMER_CPU_PIN is not available in v6.6 and
+	 * we can't guarantee that the central timer runs on the central CPU.
+	 */
+	/*if (curr_cpu != central_cpu) {
 		scx_bpf_error("Central timer ran on CPU %d, not central CPU %d",
 			      curr_cpu, central_cpu);
 		return 0;
-	}
+	}*/
 
 	bpf_for(i, 0, nr_cpu_ids) {
 		s32 cpu = (nr_timers + i) % nr_cpu_ids;
@@ -286,7 +290,7 @@ static int central_timerfn(void *map, int *key, struct bpf_timer *timer)
 		scx_bpf_kick_cpu(cpu, SCX_KICK_PREEMPT);
 	}
 
-	bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
+	bpf_timer_start(timer, TIMER_INTERVAL_NS, 0 /*BPF_F_TIMER_CPU_PIN*/);
 	__sync_fetch_and_add(&nr_timers, 1);
 	return 0;
 }
@@ -313,7 +317,7 @@ int BPF_STRUCT_OPS_SLEEPABLE(central_init)
 
 	bpf_timer_init(timer, &central_timer, CLOCK_MONOTONIC);
 	bpf_timer_set_callback(timer, central_timerfn);
-	ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
+	ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, 0 /*BPF_F_TIMER_CPU_PIN*/);
 	return ret;
 }
 
-- 
2.43.0.rc2

