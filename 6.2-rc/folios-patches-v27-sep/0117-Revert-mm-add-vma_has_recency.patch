From b81e4cdc696912ff59c1f388a335d5fd8cb8bfbc Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Fri, 6 Jan 2023 15:18:37 +0100
Subject: [PATCH 117/219] Revert "mm: add vma_has_recency()"

This reverts commit 3b31e1d0e3833f228ce98792b758f14111dbb322.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/mm_inline.h |  8 --------
 mm/memory.c               |  7 ++++---
 mm/rmap.c                 | 42 ++++++++++++++++++++++-----------------
 mm/vmscan.c               |  5 +----
 4 files changed, 29 insertions(+), 33 deletions(-)

diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h
index edd5ef097..e8ed225d8 100644
--- a/include/linux/mm_inline.h
+++ b/include/linux/mm_inline.h
@@ -578,12 +578,4 @@ pte_install_uffd_wp_if_needed(struct vm_area_struct *vma, unsigned long addr,
 #endif
 }
 
-static inline bool vma_has_recency(struct vm_area_struct *vma)
-{
-	if (vma->vm_flags & (VM_SEQ_READ | VM_RAND_READ))
-		return false;
-
-	return true;
-}
-
 #endif
diff --git a/mm/memory.c b/mm/memory.c
index 940441801..d4953b10a 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1406,7 +1406,8 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 						force_flush = 1;
 					}
 				}
-				if (pte_young(ptent) && likely(vma_has_recency(vma)))
+				if (pte_young(ptent) &&
+				    likely(!(vma->vm_flags & VM_SEQ_READ)))
 					mark_page_accessed(page);
 			}
 			rss[mm_counter(page)]--;
@@ -5146,8 +5147,8 @@ static inline void mm_account_fault(struct pt_regs *regs,
 #ifdef CONFIG_LRU_GEN
 static void lru_gen_enter_fault(struct vm_area_struct *vma)
 {
-	/* the LRU algorithm only applies to accesses with recency */
-	current->in_lru_fault = vma_has_recency(vma);
+	/* the LRU algorithm doesn't apply to sequential or random reads */
+	current->in_lru_fault = !(vma->vm_flags & (VM_SEQ_READ | VM_RAND_READ));
 }
 
 static void lru_gen_exit_fault(void)
diff --git a/mm/rmap.c b/mm/rmap.c
index eea92a307..d658f4316 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -823,14 +823,25 @@ static bool folio_referenced_one(struct folio *folio,
 		}
 
 		if (pvmw.pte) {
-			if (lru_gen_enabled() && pte_young(*pvmw.pte)) {
+			if (lru_gen_enabled() && pte_young(*pvmw.pte) &&
+			    !(vma->vm_flags & (VM_SEQ_READ | VM_RAND_READ))) {
 				lru_gen_look_around(&pvmw);
 				referenced++;
 			}
 
 			if (ptep_clear_flush_young_notify(vma, address,
-						pvmw.pte))
-				referenced++;
+						pvmw.pte)) {
+				/*
+				 * Don't treat a reference through
+				 * a sequentially read mapping as such.
+				 * If the folio has been used in another mapping,
+				 * we will catch it; if this other mapping is
+				 * already gone, the unmap path will have set
+				 * the referenced flag or activated the folio.
+				 */
+				if (likely(!(vma->vm_flags & VM_SEQ_READ)))
+					referenced++;
+			}
 		} else if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {
 			if (pmdp_clear_flush_young_notify(vma, address,
 						pvmw.pmd))
@@ -864,20 +875,7 @@ static bool invalid_folio_referenced_vma(struct vm_area_struct *vma, void *arg)
 	struct folio_referenced_arg *pra = arg;
 	struct mem_cgroup *memcg = pra->memcg;
 
-	/*
-	 * Ignore references from this mapping if it has no recency. If the
-	 * folio has been used in another mapping, we will catch it; if this
-	 * other mapping is already gone, the unmap path will have set the
-	 * referenced flag or activated the folio in zap_pte_range().
-	 */
-	if (!vma_has_recency(vma))
-		return true;
-
-	/*
-	 * If we are reclaiming on behalf of a cgroup, skip counting on behalf
-	 * of references from different cgroups.
-	 */
-	if (memcg && !mm_match_cgroup(vma->vm_mm, memcg))
+	if (!mm_match_cgroup(vma->vm_mm, memcg))
 		return true;
 
 	return false;
@@ -908,7 +906,6 @@ int folio_referenced(struct folio *folio, int is_locked,
 		.arg = (void *)&pra,
 		.anon_lock = folio_lock_anon_vma_read,
 		.try_lock = true,
-		.invalid_vma = invalid_folio_referenced_vma,
 	};
 
 	*vm_flags = 0;
@@ -924,6 +921,15 @@ int folio_referenced(struct folio *folio, int is_locked,
 			return 1;
 	}
 
+	/*
+	 * If we are reclaiming on behalf of a cgroup, skip
+	 * counting on behalf of references from different
+	 * cgroups
+	 */
+	if (memcg) {
+		rwc.invalid_vma = invalid_folio_referenced_vma;
+	}
+
 	rmap_walk(folio, &rwc);
 	*vm_flags = pra.vm_flags;
 
diff --git a/mm/vmscan.c b/mm/vmscan.c
index dd3baab51..bd6637fcd 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3782,10 +3782,7 @@ static int should_skip_vma(unsigned long start, unsigned long end, struct mm_wal
 	if (is_vm_hugetlb_page(vma))
 		return true;
 
-	if (!vma_has_recency(vma))
-		return true;
-
-	if (vma->vm_flags & (VM_LOCKED | VM_SPECIAL))
+	if (vma->vm_flags & (VM_LOCKED | VM_SPECIAL | VM_SEQ_READ | VM_RAND_READ))
 		return true;
 
 	if (vma == get_gate_vma(vma->vm_mm))
-- 
2.39.0.rc2.1.gbd5df96b79

