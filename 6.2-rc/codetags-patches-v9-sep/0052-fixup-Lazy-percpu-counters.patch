From cb7cdbae30b58a713eb5511030469e74e2070c2e Mon Sep 17 00:00:00 2001
From: Kent Overstreet <kent.overstreet@linux.dev>
Date: Thu, 2 Feb 2023 12:37:34 -0500
Subject: [PATCH 52/55] fixup! Lazy percpu counters

---
 include/linux/lazy-percpu-counter.h | 62 +++++++++++------------------
 lib/lazy-percpu-counter.c           | 46 +++++++++++++++------
 2 files changed, 57 insertions(+), 51 deletions(-)

diff --git a/include/linux/lazy-percpu-counter.h b/include/linux/lazy-percpu-counter.h
index 2e02180f3..45ca9e2ce 100644
--- a/include/linux/lazy-percpu-counter.h
+++ b/include/linux/lazy-percpu-counter.h
@@ -14,11 +14,6 @@
  * percpu mode, and the high 8 bits are a secondary counter that's incremented
  * when the counter is modified - meaning 55 bits of precision are available for
  * the counter itself.
- *
- * lazy_percpu_counter is 16 bytes (on 64 bit machines), raw_lazy_percpu_counter
- * is 8 bytes but requires a separate unsigned long to record when the counter
- * wraps - because sometimes multiple counters are used together and can share
- * the same timestamp.
  */
 
 #ifndef _LINUX_LAZY_PERCPU_COUNTER_H
@@ -27,14 +22,15 @@
 #include <linux/atomic.h>
 #include <asm/percpu.h>
 
-struct raw_lazy_percpu_counter {
+struct lazy_percpu_counter {
 	atomic64_t			v;
+	unsigned long			last_wrap;
 };
 
-void __lazy_percpu_counter_exit(struct raw_lazy_percpu_counter *c);
-void lazy_percpu_counter_add_slowpath(struct raw_lazy_percpu_counter *c,
-			       unsigned long *last_wrap, s64 i);
-s64 __lazy_percpu_counter_read(struct raw_lazy_percpu_counter *c);
+void lazy_percpu_counter_exit(struct lazy_percpu_counter *c);
+void lazy_percpu_counter_add_slowpath(struct lazy_percpu_counter *c, s64 i);
+void lazy_percpu_counter_add_slowpath_noupgrade(struct lazy_percpu_counter *c, s64 i);
+s64 lazy_percpu_counter_read(struct lazy_percpu_counter *c);
 
 /*
  * We use the high bits of the atomic counter for a secondary counter, which is
@@ -64,14 +60,12 @@ static inline u64 __percpu *lazy_percpu_counter_is_pcpu(u64 v)
 }
 
 /**
- * __lazy_percpu_counter_add: Add a value to a lazy_percpu_counter
+ * lazy_percpu_counter_add: Add a value to a lazy_percpu_counter
  *
  * @c: counter to modify
- * @last_wrap: pointer to a timestamp, updated when mod counter wraps
  * @i: value to add
  */
-static inline void __lazy_percpu_counter_add(struct raw_lazy_percpu_counter *c,
-					     unsigned long *last_wrap, s64 i)
+static inline void lazy_percpu_counter_add(struct lazy_percpu_counter *c, s64 i)
 {
 	u64 v = atomic64_read(&c->v);
 	u64 __percpu *pcpu_v = lazy_percpu_counter_is_pcpu(v);
@@ -79,38 +73,30 @@ static inline void __lazy_percpu_counter_add(struct raw_lazy_percpu_counter *c,
 	if (likely(pcpu_v))
 		this_cpu_add(*pcpu_v, i);
 	else
-		lazy_percpu_counter_add_slowpath(c, last_wrap, i);
-}
-
-static inline void __lazy_percpu_counter_sub(struct raw_lazy_percpu_counter *c,
-					     unsigned long *last_wrap, s64 i)
-{
-	__lazy_percpu_counter_add(c, last_wrap, -i);
+		lazy_percpu_counter_add_slowpath(c, i);
 }
 
-struct lazy_percpu_counter {
-	struct raw_lazy_percpu_counter	v;
-	unsigned long			last_wrap;
-};
-
-static inline void lazy_percpu_counter_exit(struct lazy_percpu_counter *c)
+/**
+ * lazy_percpu_counter_add_noupgrade: Add a value to a lazy_percpu_counter,
+ * without upgrading to percpu mode
+ *
+ * @c: counter to modify
+ * @i: value to add
+ */
+static inline void lazy_percpu_counter_add_noupgrade(struct lazy_percpu_counter *c, s64 i)
 {
-	__lazy_percpu_counter_exit(&c->v);
-}
+	u64 v = atomic64_read(&c->v);
+	u64 __percpu *pcpu_v = lazy_percpu_counter_is_pcpu(v);
 
-static inline void lazy_percpu_counter_add(struct lazy_percpu_counter *c, s64 i)
-{
-	__lazy_percpu_counter_add(&c->v, &c->last_wrap, i);
+	if (likely(pcpu_v))
+		this_cpu_add(*pcpu_v, i);
+	else
+		lazy_percpu_counter_add_slowpath_noupgrade(c, i);
 }
 
 static inline void lazy_percpu_counter_sub(struct lazy_percpu_counter *c, s64 i)
 {
-	__lazy_percpu_counter_sub(&c->v, &c->last_wrap, i);
-}
-
-static inline s64 lazy_percpu_counter_read(struct lazy_percpu_counter *c)
-{
-	return __lazy_percpu_counter_read(&c->v);
+	lazy_percpu_counter_add(c, -i);
 }
 
 #endif /* _LINUX_LAZY_PERCPU_COUNTER_H */
diff --git a/lib/lazy-percpu-counter.c b/lib/lazy-percpu-counter.c
index caddaf246..4f4e32c2d 100644
--- a/lib/lazy-percpu-counter.c
+++ b/lib/lazy-percpu-counter.c
@@ -13,7 +13,7 @@ static inline s64 lazy_percpu_counter_atomic_val(s64 v)
 		(COUNTER_MOD_BITS + COUNTER_IS_PCPU_BIT);
 }
 
-static void lazy_percpu_counter_switch_to_pcpu(struct raw_lazy_percpu_counter *c)
+static void lazy_percpu_counter_switch_to_pcpu(struct lazy_percpu_counter *c)
 {
 	u64 __percpu *pcpu_v = alloc_percpu_gfp(u64, GFP_ATOMIC|__GFP_NOWARN);
 	u64 old, new, v;
@@ -38,23 +38,23 @@ static void lazy_percpu_counter_switch_to_pcpu(struct raw_lazy_percpu_counter *c
 }
 
 /**
- * __lazy_percpu_counter_exit: Free resources associated with a
- * raw_lazy_percpu_counter
+ * lazy_percpu_counter_exit: Free resources associated with a
+ * lazy_percpu_counter
  *
  * @c: counter to exit
  */
-void __lazy_percpu_counter_exit(struct raw_lazy_percpu_counter *c)
+void lazy_percpu_counter_exit(struct lazy_percpu_counter *c)
 {
 	free_percpu(lazy_percpu_counter_is_pcpu(atomic64_read(&c->v)));
 }
-EXPORT_SYMBOL_GPL(__lazy_percpu_counter_exit);
+EXPORT_SYMBOL_GPL(lazy_percpu_counter_exit);
 
 /**
- * __lazy_percpu_counter_read: Read current value of a raw_lazy_percpu_counter
+ * lazy_percpu_counter_read: Read current value of a lazy_percpu_counter
  *
  * @c: counter to read
  */
-s64 __lazy_percpu_counter_read(struct raw_lazy_percpu_counter *c)
+s64 lazy_percpu_counter_read(struct lazy_percpu_counter *c)
 {
 	s64 v = atomic64_read(&c->v);
 	u64 __percpu *pcpu_v = lazy_percpu_counter_is_pcpu(v);
@@ -71,10 +71,9 @@ s64 __lazy_percpu_counter_read(struct raw_lazy_percpu_counter *c)
 
 	return v;
 }
-EXPORT_SYMBOL_GPL(__lazy_percpu_counter_read);
+EXPORT_SYMBOL_GPL(lazy_percpu_counter_read);
 
-void lazy_percpu_counter_add_slowpath(struct raw_lazy_percpu_counter *c,
-				      unsigned long *last_wrap, s64 i)
+void lazy_percpu_counter_add_slowpath(struct lazy_percpu_counter *c, s64 i)
 {
 	u64 atomic_i;
 	u64 old, v = atomic64_read(&c->v);
@@ -97,11 +96,32 @@ void lazy_percpu_counter_add_slowpath(struct raw_lazy_percpu_counter *c,
 	if (unlikely(!(v & COUNTER_MOD_MASK))) {
 		unsigned long now = jiffies;
 
-		if (*last_wrap &&
-		    unlikely(time_after(*last_wrap + HZ, now)))
+		if (c->last_wrap &&
+		    unlikely(time_after(c->last_wrap + HZ, now)))
 			lazy_percpu_counter_switch_to_pcpu(c);
 		else
-			*last_wrap = now;
+			c->last_wrap = now;
 	}
 }
 EXPORT_SYMBOL(lazy_percpu_counter_add_slowpath);
+
+void lazy_percpu_counter_add_slowpath_noupgrade(struct lazy_percpu_counter *c, s64 i)
+{
+	u64 atomic_i;
+	u64 old, v = atomic64_read(&c->v);
+	u64 __percpu *pcpu_v;
+
+	atomic_i  = i << COUNTER_IS_PCPU_BIT;
+	atomic_i &= ~COUNTER_MOD_MASK;
+
+	do {
+		pcpu_v = lazy_percpu_counter_is_pcpu(v);
+		if (pcpu_v) {
+			this_cpu_add(*pcpu_v, i);
+			return;
+		}
+
+		old = v;
+	} while ((v = atomic64_cmpxchg(&c->v, old, old + atomic_i)) != old);
+}
+EXPORT_SYMBOL(lazy_percpu_counter_add_slowpath_noupgrade);
-- 
2.39.0.rc2.1.gbd5df96b79

