From 049f6fe59070392a09c6934af803da0b0249a257 Mon Sep 17 00:00:00 2001
From: Masahito S <firelzrd@gmail.com>
Date: Mon, 13 Mar 2023 21:49:36 +0900
Subject: [PATCH 09/14] sched: Commit to lag based placement

---
 kernel/sched/fair.c     | 60 ++++++++---------------------------------
 kernel/sched/features.h | 15 -----------
 2 files changed, 11 insertions(+), 64 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 285a29f47..058a9c957 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4936,9 +4936,9 @@ static void
 place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)
 {
 	u64 vruntime = avg_vruntime(cfs_rq);
+	s64 lag = 0;
 
-	if (sched_feat(PRESERVE_LAG) && cfs_rq->nr_running > 1) {
-		s64 lag = se->lag;
+	if (cfs_rq->nr_running > 1) {
 
 		/*
 		 * l_i = V - v_i <=> v_i = V - l_i
@@ -4954,57 +4954,19 @@ place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)
 		 *
 		 * l_i = (W + w_i) * l_i' / W
 		 */
-		if (sched_feat(PRESERVE_LAG_SCALE)) {
-			struct sched_entity *curr = cfs_rq->curr;
-			unsigned long load = cfs_rq->avg_load;
-
-			if (curr && curr->on_rq)
-				load += curr->load.weight;
-
-			lag *= load + se->load.weight;
-			if (WARN_ON_ONCE(!load))
-				load = 1;
-			lag = div_s64(lag, load);
-		}
-
-		vruntime -= lag;
-	}
-
-	if (sched_feat(FAIR_SLEEPERS)) {
-		u64 sleep_time;
-
-		/* sleeps up to a single latency don't count. */
-		if (!initial) {
-			unsigned long thresh;
-
-			if (se_is_idle(se))
-				thresh = sysctl_sched_min_granularity;
-			else
-				thresh = sysctl_sched_latency;
-
-			/*
-			 * Halve their sleep time's effect, to allow
-			 * for a gentler effect of sleepers:
-			 */
-			if (sched_feat(GENTLE_FAIR_SLEEPERS))
-				thresh >>= 1;
+		struct sched_entity *curr = cfs_rq->curr;
+		unsigned long load = cfs_rq->avg_load;
 
-			vruntime -= thresh;
-		}
+		if (curr && curr->on_rq)
+			load += curr->load.weight;
 
-		/*
-		 * Pull vruntime of the entity being placed to the base level of
-		 * cfs_rq, to prevent boosting it if placed backwards.  If the entity
-		 * slept for a long time, don't even try to compare its vruntime with
-		 * the base as it may be too far off and the comparison may get
-		 * inversed due to s64 overflow.
-		 */
-		sleep_time = rq_clock_task(rq_of(cfs_rq)) - se->exec_start;
-		if ((s64)sleep_time < 60LL * NSEC_PER_SEC)
-			vruntime = max_vruntime(se->vruntime, vruntime);
+		lag = se->lag * (load + se->load.weight);
+		if (WARN_ON_ONCE(!load))
+			load = 1;
+		lag = div_s64(lag, load);
 	}
 
-	se->vruntime = vruntime;
+	se->vruntime = vruntime - lag;
 	update_deadline(cfs_rq, se, true);
 }
 
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index ebcbd02eb..285e8abdd 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -1,20 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 
-/*
- * Only give sleepers 50% of their service deficit. This allows
- * them to run sooner, but does not allow tons of sleepers to
- * rip the spread apart.
- */
-SCHED_FEAT(FAIR_SLEEPERS, false)
-SCHED_FEAT(GENTLE_FAIR_SLEEPERS, true)
-
-/*
- * Using the avg_vruntime, do the right thing and preserve lag
- * across sleep+wake cycles.
- */
-SCHED_FEAT(PRESERVE_LAG, true)
-SCHED_FEAT(PRESERVE_LAG_SCALE, true)
-
 /*
  * Prefer to schedule the task we woke last (assuming it failed
  * wakeup-preemption), since its likely going to consume data we
-- 
2.39.2.501.gd9d677b2d8

