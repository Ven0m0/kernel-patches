From 46e2f0d5b1988e5cafdc49a5f747b547c9e63cfb Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 20 Apr 2023 18:16:55 +0200
Subject: [PATCH 45/46] Revert "mm: do not increment pgfault stats when page
 fault handler retries"

This reverts commit 4cb4e242b042901e1c6141283a4855e47d14364d.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 mm/memory.c | 45 +++++++++++++++++++--------------------------
 1 file changed, 19 insertions(+), 26 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index 7c8278e8b..0af81ac5d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -5126,30 +5126,24 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
  * updates.  However, note that the handling of PERF_COUNT_SW_PAGE_FAULTS should
  * still be in per-arch page fault handlers at the entry of page fault.
  */
-static inline void mm_account_fault(struct mm_struct *mm, struct pt_regs *regs,
+static inline void mm_account_fault(struct pt_regs *regs,
 				    unsigned long address, unsigned int flags,
 				    vm_fault_t ret)
 {
 	bool major;
 
 	/*
-	 * Do not account for incomplete faults (VM_FAULT_RETRY). They will be
-	 * counted upon completion.
-	 */
-	if (ret & VM_FAULT_RETRY)
-		return;
-
-	/* Register both successful and failed faults in PGFAULT counters. */
-	count_vm_event(PGFAULT);
-	count_memcg_event_mm(mm, PGFAULT);
-
-	/*
-	 * Do not account for unsuccessful faults (e.g. when the address wasn't
-	 * valid).  That includes arch_vma_access_permitted() failing before
-	 * reaching here. So this is not a "this many hardware page faults"
-	 * counter.  We should use the hw profiling for that.
+	 * We don't do accounting for some specific faults:
+	 *
+	 * - Unsuccessful faults (e.g. when the address wasn't valid).  That
+	 *   includes arch_vma_access_permitted() failing before reaching here.
+	 *   So this is not a "this many hardware page faults" counter.  We
+	 *   should use the hw profiling for that.
+	 *
+	 * - Incomplete faults (VM_FAULT_RETRY).  They will only be counted
+	 *   once they're completed.
 	 */
-	if (ret & VM_FAULT_ERROR)
+	if (ret & (VM_FAULT_ERROR | VM_FAULT_RETRY))
 		return;
 
 	/*
@@ -5232,22 +5226,21 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 			   unsigned int flags, struct pt_regs *regs)
 {
-	/* Copy vma->vm_mm in case mmap_lock is dropped and vma becomes unstable. */
-	struct mm_struct *mm = vma->vm_mm;
 	vm_fault_t ret;
 
 	__set_current_state(TASK_RUNNING);
 
+	count_vm_event(PGFAULT);
+	count_memcg_event_mm(vma->vm_mm, PGFAULT);
+
 	ret = sanitize_fault_flags(vma, &flags);
 	if (ret)
-		goto out;
+		return ret;
 
 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
 					    flags & FAULT_FLAG_INSTRUCTION,
-					    flags & FAULT_FLAG_REMOTE)) {
-		ret = VM_FAULT_SIGSEGV;
-		goto out;
-	}
+					    flags & FAULT_FLAG_REMOTE))
+		return VM_FAULT_SIGSEGV;
 
 	/*
 	 * Enable the memcg OOM handling for faults triggered in user
@@ -5276,8 +5269,8 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
 			mem_cgroup_oom_synchronize(false);
 	}
-out:
-	mm_account_fault(mm, regs, address, flags, ret);
+
+	mm_account_fault(regs, address, flags, ret);
 
 	return ret;
 }
-- 
2.40.0.71.g950264636c

