From 6bb46461ae58ac22a680fc9941e5347c54f9ce78 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Tue, 19 Sep 2023 11:34:01 +0200
Subject: [PATCH 3/5] Revert "i915: convert shmem_sg_free_table() to use a
 folio_batch"

This reverts commit 0b62af28f249b9c4036a05acfb053058dc02e2e2.
---
 drivers/gpu/drm/i915/gem/i915_gem_shmem.c | 55 ++++++++++-------------
 1 file changed, 24 insertions(+), 31 deletions(-)

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_shmem.c b/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
index 8f1633c3f..33d5d5178 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_shmem.c
@@ -19,13 +19,13 @@
 #include "i915_trace.h"
 
 /*
- * Move folios to appropriate lru and release the batch, decrementing the
- * ref count of those folios.
+ * Move pages to appropriate lru and release the pagevec, decrementing the
+ * ref count of those pages.
  */
-static void check_release_folio_batch(struct folio_batch *fbatch)
+static void check_release_pagevec(struct pagevec *pvec)
 {
-	check_move_unevictable_folios(fbatch);
-	__folio_batch_release(fbatch);
+	check_move_unevictable_pages(pvec);
+	__pagevec_release(pvec);
 	cond_resched();
 }
 
@@ -33,29 +33,24 @@ void shmem_sg_free_table(struct sg_table *st, struct address_space *mapping,
 			 bool dirty, bool backup)
 {
 	struct sgt_iter sgt_iter;
-	struct folio_batch fbatch;
-	struct folio *last = NULL;
+	struct pagevec pvec;
 	struct page *page;
 
 	mapping_clear_unevictable(mapping);
 
-	folio_batch_init(&fbatch);
+	pagevec_init(&pvec);
 	for_each_sgt_page(page, sgt_iter, st) {
-		struct folio *folio = page_folio(page);
-
-		if (folio == last)
-			continue;
-		last = folio;
 		if (dirty)
-			folio_mark_dirty(folio);
+			set_page_dirty(page);
+
 		if (backup)
-			folio_mark_accessed(folio);
+			mark_page_accessed(page);
 
-		if (!folio_batch_add(&fbatch, folio))
-			check_release_folio_batch(&fbatch);
+		if (!pagevec_add(&pvec, page))
+			check_release_pagevec(&pvec);
 	}
-	if (fbatch.nr)
-		check_release_folio_batch(&fbatch);
+	if (pagevec_count(&pvec))
+		check_release_pagevec(&pvec);
 
 	sg_free_table(st);
 }
@@ -68,7 +63,8 @@ int shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,
 	unsigned int page_count; /* restricted by sg_alloc_table */
 	unsigned long i;
 	struct scatterlist *sg;
-	unsigned long next_pfn = 0;	/* suppress gcc warning */
+	struct page *page;
+	unsigned long last_pfn = 0;	/* suppress gcc warning */
 	gfp_t noreclaim;
 	int ret;
 
@@ -99,7 +95,6 @@ int shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,
 	sg = st->sgl;
 	st->nents = 0;
 	for (i = 0; i < page_count; i++) {
-		struct folio *folio;
 		const unsigned int shrink[] = {
 			I915_SHRINK_BOUND | I915_SHRINK_UNBOUND,
 			0,
@@ -108,12 +103,12 @@ int shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,
 
 		do {
 			cond_resched();
-			folio = shmem_read_folio_gfp(mapping, i, gfp);
-			if (!IS_ERR(folio))
+			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
+			if (!IS_ERR(page))
 				break;
 
 			if (!*s) {
-				ret = PTR_ERR(folio);
+				ret = PTR_ERR(page);
 				goto err_sg;
 			}
 
@@ -152,21 +147,19 @@ int shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,
 
 		if (!i ||
 		    sg->length >= max_segment ||
-		    folio_pfn(folio) != next_pfn) {
+		    page_to_pfn(page) != last_pfn + 1) {
 			if (i)
 				sg = sg_next(sg);
 
 			st->nents++;
-			sg_set_folio(sg, folio, folio_size(folio), 0);
+			sg_set_page(sg, page, PAGE_SIZE, 0);
 		} else {
-			/* XXX: could overflow? */
-			sg->length += folio_size(folio);
+			sg->length += PAGE_SIZE;
 		}
-		next_pfn = folio_pfn(folio) + folio_nr_pages(folio);
-		i += folio_nr_pages(folio) - 1;
+		last_pfn = page_to_pfn(page);
 
 		/* Check that the i965g/gm workaround works. */
-		GEM_BUG_ON(gfp & __GFP_DMA32 && next_pfn >= 0x00100000UL);
+		GEM_BUG_ON(gfp & __GFP_DMA32 && last_pfn >= 0x00100000UL);
 	}
 	if (sg) /* loop terminated early; short sg table */
 		sg_mark_end(sg);
-- 
2.42.0

