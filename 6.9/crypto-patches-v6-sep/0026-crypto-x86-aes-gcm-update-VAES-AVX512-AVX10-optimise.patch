From 8e610440268040df087e1e80a57d64c24fc136b8 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Mon, 3 Jun 2024 14:18:33 +0200
Subject: [PATCH 26/26] crypto: x86/aes-gcm update VAES/AVX512/AVX10-optimised
 AES-GCM patch to v5

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 arch/x86/crypto/aes-gcm-aesni-x86_64.S |  9 ++---
 arch/x86/crypto/aesni-intel_glue.c     | 46 +++++++++++++-------------
 2 files changed, 26 insertions(+), 29 deletions(-)

diff --git a/arch/x86/crypto/aes-gcm-aesni-x86_64.S b/arch/x86/crypto/aes-gcm-aesni-x86_64.S
index c1f8197f0..45940e288 100644
--- a/arch/x86/crypto/aes-gcm-aesni-x86_64.S
+++ b/arch/x86/crypto/aes-gcm-aesni-x86_64.S
@@ -205,7 +205,7 @@
 .endif
 .endm
 
-// Load 1 <= %rcx <= 15 bytes from the pointer \src into the xmm register \dst
+// Load 1 <= %ecx <= 15 bytes from the pointer \src into the xmm register \dst
 // and zeroize any remaining bytes.  Clobbers %rax, %rcx, and \tmp{64,32}.
 .macro	_load_partial_block	src, dst, tmp64, tmp32
 	sub		$8, %ecx		// LEN - 8
@@ -244,7 +244,7 @@
 .Ldone\@:
 .endm
 
-// Store 1 <= %rcx <= 15 bytes from the xmm register \src to the pointer \dst.
+// Store 1 <= %ecx <= 15 bytes from the xmm register \src to the pointer \dst.
 // Clobbers %rax, %rcx, and %rsi.
 .macro	_store_partial_block	src, dst
 	sub		$8, %ecx		// LEN - 8
@@ -592,9 +592,6 @@
 	movq		.Lgfpoly(%rip), GFPOLY
 
 	// Process the AAD one full block at a time.
-	//
-	// Pre-subtracting 16 from AADLEN simplifies the loop.  It also
-	// zero-extends it to 64 bits, which is needed for _load_partial_block.
 	sub		$16, AADLEN
 	jl		.Laad_loop_1x_done\@
 .Laad_loop_1x\@:
@@ -611,7 +608,7 @@
 	jz		.Laad_done\@
 
 	// Process a partial block of length 1 <= AADLEN <= 15.
-	// _load_partial_block assumes that %rcx contains zero-extended AADLEN.
+	// _load_partial_block assumes that %ecx contains AADLEN.
 	_load_partial_block	AAD, %xmm0, %r10, %r10d
 	pshufb		BSWAP_MASK, %xmm0
 	pxor		%xmm0, GHASH_ACC
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index db21aaaa2..cb4bf9895 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -852,24 +852,24 @@ struct aes_gcm_key_aesni {
 	struct aes_gcm_key base;
 
 	/*
-	 * Powers of the hash key H^8 through H^1.  All entries have an extra
-	 * factor of x^-1 and are byte-reversed.  16-byte alignment is required
-	 * by the assembly code.
+	 * Powers of the hash key H^8 through H^1.  These are 128-bit values.
+	 * They all have an extra factor of x^-1 and are byte-reversed.  16-byte
+	 * alignment is required by the assembly code.
 	 */
-	u8 h_powers[8][16] __aligned(16);
+	u64 h_powers[8][2] __aligned(16);
 
 	/*
 	 * h_powers_xored[i] contains the two 64-bit halves of h_powers[i] XOR'd
 	 * together.  It's used for Karatsuba multiplication.  16-byte alignment
 	 * is required by the assembly code.
 	 */
-	u8 h_powers_xored[8][8] __aligned(16);
+	u64 h_powers_xored[8] __aligned(16);
 
 	/*
 	 * H^1 times x^64 (and also the usual extra factor of x^-1).  16-byte
 	 * alignment is required by the assembly code.
 	 */
-	u8 h_times_x64[16] __aligned(16);
+	u64 h_times_x64[2] __aligned(16);
 };
 #define AES_GCM_KEY_AESNI(key)	\
 	container_of((key), struct aes_gcm_key_aesni, base)
@@ -886,16 +886,16 @@ struct aes_gcm_key_avx10 {
 	struct aes_gcm_key base;
 
 	/*
-	 * Powers of the hash key H^16 through H^1.  All entries have an extra
-	 * factor of x^-1 and are byte-reversed.  This is aligned to a 64-byte
-	 * boundary to make it naturally aligned for 512-bit loads, which may
-	 * improve performance on some CPUs.  (The assembly code doesn't *need*
-	 * the alignment; this is just an optimization.)
+	 * Powers of the hash key H^16 through H^1.  These are 128-bit values.
+	 * They all have an extra factor of x^-1 and are byte-reversed.  This
+	 * array is aligned to a 64-byte boundary to make it naturally aligned
+	 * for 512-bit loads, which can improve performance.  (The assembly code
+	 * doesn't *need* the alignment; this is just an optimization.)
 	 */
-	u8 h_powers[16][16] __aligned(64);
+	u64 h_powers[16][2] __aligned(64);
 
 	/* Three padding blocks required by the assembly code */
-	u8 padding[3][16];
+	u64 padding[3][2];
 };
 #define AES_GCM_KEY_AVX10(key)	\
 	container_of((key), struct aes_gcm_key_avx10, base)
@@ -1246,8 +1246,8 @@ static int gcm_setkey(struct crypto_aead *tfm, const u8 *raw_key,
 			struct aes_gcm_key_avx10 *k = AES_GCM_KEY_AVX10(key);
 
 			for (i = ARRAY_SIZE(k->h_powers) - 1; i >= 0; i--) {
-				put_unaligned_be64(h.a, &k->h_powers[i][8]);
-				put_unaligned_be64(h.b, &k->h_powers[i][0]);
+				k->h_powers[i][0] = be64_to_cpu(h.b);
+				k->h_powers[i][1] = be64_to_cpu(h.a);
 				gf128mul_lle(&h, &h1);
 			}
 			memset(k->padding, 0, sizeof(k->padding));
@@ -1255,15 +1255,15 @@ static int gcm_setkey(struct crypto_aead *tfm, const u8 *raw_key,
 			struct aes_gcm_key_aesni *k = AES_GCM_KEY_AESNI(key);
 
 			for (i = ARRAY_SIZE(k->h_powers) - 1; i >= 0; i--) {
-				put_unaligned_be64(h.a, &k->h_powers[i][8]);
-				put_unaligned_be64(h.b, &k->h_powers[i][0]);
-				put_unaligned_be64(h.a ^ h.b,
-						   &k->h_powers_xored[i]);
+				k->h_powers[i][0] = be64_to_cpu(h.b);
+				k->h_powers[i][1] = be64_to_cpu(h.a);
+				k->h_powers_xored[i] = k->h_powers[i][0] ^
+						       k->h_powers[i][1];
 				gf128mul_lle(&h, &h1);
 			}
 			gf128mul_lle(&h1, (const be128 *)x_to_the_63);
-			put_unaligned_be64(h1.a, &k->h_times_x64[8]);
-			put_unaligned_be64(h1.b, &k->h_times_x64[0]);
+			k->h_times_x64[0] = be64_to_cpu(h1.b);
+			k->h_times_x64[1] = be64_to_cpu(h1.a);
 		}
 	}
 	return 0;
@@ -1448,7 +1448,7 @@ gcm_crypt(struct aead_request *req, int flags)
 			ctxsize, priority)				       \
 									       \
 static int gcm_setkey_##suffix(struct crypto_aead *tfm, const u8 *raw_key,     \
-			    unsigned int keylen)			       \
+			       unsigned int keylen)			       \
 {									       \
 	return gcm_setkey(tfm, raw_key, keylen, (flags));		       \
 }									       \
@@ -1464,7 +1464,7 @@ static int gcm_decrypt_##suffix(struct aead_request *req)		       \
 }									       \
 									       \
 static int rfc4106_setkey_##suffix(struct crypto_aead *tfm, const u8 *raw_key, \
-				unsigned int keylen)			       \
+				   unsigned int keylen)			       \
 {									       \
 	return gcm_setkey(tfm, raw_key, keylen, (flags) | FLAG_RFC4106);       \
 }									       \
-- 
2.45.1.145.g83f1add914

