From 96741a79c76ac48359808162306785e3d138ff61 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Thu, 30 May 2024 09:36:16 +0200
Subject: [PATCH 24/25] crypto: x86/aes-gcm update VAES/AVX512/AVX10-optimised
 AES-GCM patch to v4

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 arch/x86/crypto/aes-gcm-avx10-x86_64.S |  59 ++-
 arch/x86/crypto/aesni-intel_glue.c     | 491 ++++++++++++-------------
 2 files changed, 274 insertions(+), 276 deletions(-)

diff --git a/arch/x86/crypto/aes-gcm-avx10-x86_64.S b/arch/x86/crypto/aes-gcm-avx10-x86_64.S
index ca608bda9..97e0ee515 100644
--- a/arch/x86/crypto/aes-gcm-avx10-x86_64.S
+++ b/arch/x86/crypto/aes-gcm-avx10-x86_64.S
@@ -118,23 +118,22 @@
 	.octa	4
 
 // Number of powers of the hash key stored in the key struct.  The powers are
-// stored from highest (H^NUM_KEY_POWERS) to lowest (H^1).
-#define NUM_KEY_POWERS		16
+// stored from highest (H^NUM_H_POWERS) to lowest (H^1).
+#define NUM_H_POWERS		16
 
-// Number of powers of the hash key including zero padding at the end.  Zero
-// padding is appended so that partial vectors can be handled more easily.  E.g.
-// if VL=64 and two blocks remain, we load the 4 values [H^2, H^1, 0, 0].  The
-// most padding blocks needed is 3, which occurs if [H^1, 0, 0, 0] is loaded.
-#define FULL_NUM_KEY_POWERS	(NUM_KEY_POWERS + 3)
+// Offset to AES key length (in bytes) in the key struct
+#define OFFSETOF_AESKEYLEN	480
 
-// Offset to AES key length in the key struct
-#define OFFSETOF_KEYLEN		480
+// Offset to start of hash key powers array in the key struct
+#define OFFSETOF_H_POWERS	512
 
-// Offset to start of key powers array in the key struct
-#define OFFSETOF_KEY_POWERS	512
-
-// Offset to end of key powers array, not including the zero padding
-#define OFFSETOFEND_KEY_POWERS	(OFFSETOF_KEY_POWERS + (NUM_KEY_POWERS * 16))
+// Offset to end of hash key powers array in the key struct.
+//
+// This is immediately followed by three zeroized padding blocks, which are
+// included so that partial vectors can be handled more easily.  E.g. if VL=64
+// and two blocks remain, we load the 4 values [H^2, H^1, 0, 0].  The most
+// padding blocks needed is 3, which occurs if [H^1, 0, 0, 0] is loaded.
+#define OFFSETOFEND_H_POWERS	(OFFSETOF_H_POWERS + (NUM_H_POWERS * 16))
 
 .text
 
@@ -318,9 +317,9 @@
 // Given the expanded AES key |key->aes_key|, this function derives the GHASH
 // subkey and initializes |key->ghash_key_powers| with powers of it.
 //
-// The number of key powers initialized is NUM_KEY_POWERS, and they are stored
-// in the order H^NUM_KEY_POWERS to H^1.  The zeroized padding blocks after the
-// key powers themselves (see FULL_NUM_KEY_POWERS) are also initialized.
+// The number of key powers initialized is NUM_H_POWERS, and they are stored in
+// the order H^NUM_H_POWERS to H^1.  The zeroized padding blocks after the key
+// powers themselves are also initialized.
 //
 // This macro supports both VL=32 and VL=64.  _set_veclen must have been invoked
 // with the desired length.  In the VL=32 case, the function computes twice as
@@ -345,10 +344,10 @@
 	.set	GFPOLY_XMM,	%xmm5
 
 	// Get pointer to lowest set of key powers (located at end of array).
-	lea		OFFSETOFEND_KEY_POWERS-VL(KEY), POWERS_PTR
+	lea		OFFSETOFEND_H_POWERS-VL(KEY), POWERS_PTR
 
 	// Encrypt an all-zeroes block to get the raw hash subkey.
-	movl		OFFSETOF_KEYLEN(KEY), %eax  // AES key length in bytes
+	movl		OFFSETOF_AESKEYLEN(KEY), %eax
 	lea		6*16(KEY,%rax,4), RNDKEYLAST_PTR
 	vmovdqu		(KEY), %xmm0  // Zero-th round key XOR all-zeroes block
 	add		$16, KEY
@@ -420,7 +419,7 @@
 	// multiply [H^(i+1), H^i] by [H^2, H^2] to get [H^(i+3), H^(i+2)].
 	// With VL=64, repeatedly multiply [H^(i+3), H^(i+2), H^(i+1), H^i] by
 	// [H^4, H^4, H^4, H^4] to get [H^(i+7), H^(i+6), H^(i+5), H^(i+4)].
-	mov		$(NUM_KEY_POWERS*16/VL) - 1, %eax
+	mov		$(NUM_H_POWERS*16/VL) - 1, %eax
 .Lprecompute_next\@:
 	sub		$VL, POWERS_PTR
 	_ghash_mul	H_INC, H_CUR, H_CUR, GFPOLY, V0, V1, V2
@@ -687,7 +686,7 @@
 	vbroadcasti32x4	(LE_CTR_PTR), LE_CTR
 
 	// Load the AES key length in bytes.
-	movl		OFFSETOF_KEYLEN(KEY), AESKEYLEN
+	movl		OFFSETOF_AESKEYLEN(KEY), AESKEYLEN
 
 	// Make RNDKEYLAST_PTR point to the last AES round key.  This is the
 	// round key with index 10, 12, or 14 for AES-128, AES-192, or AES-256
@@ -718,10 +717,10 @@
 	jl		.Lcrypt_loop_4x_done\@
 
 	// Load powers of the hash key.
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-4*VL(KEY), H_POW4
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-3*VL(KEY), H_POW3
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-2*VL(KEY), H_POW2
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-1*VL(KEY), H_POW1
+	vmovdqu8	OFFSETOFEND_H_POWERS-4*VL(KEY), H_POW4
+	vmovdqu8	OFFSETOFEND_H_POWERS-3*VL(KEY), H_POW3
+	vmovdqu8	OFFSETOFEND_H_POWERS-2*VL(KEY), H_POW2
+	vmovdqu8	OFFSETOFEND_H_POWERS-1*VL(KEY), H_POW1
 
 	// Main loop: en/decrypt and hash 4 vectors at a time.
 	//
@@ -883,7 +882,7 @@
 	mov		DATALEN, %eax
 	neg		%rax
 	and		$~15, %rax  // -round_up(DATALEN, 16)
-	lea		OFFSETOFEND_KEY_POWERS(KEY,%rax), POWERS_PTR
+	lea		OFFSETOFEND_H_POWERS(KEY,%rax), POWERS_PTR
 
 	// Start collecting the unreduced GHASH intermediate value LO, MI, HI.
 	.set		LO, GHASHDATA0
@@ -1014,7 +1013,7 @@
 	vmovdqa		.Lbswap_mask(%rip), BSWAP_MASK
 
 	// Load the AES key length in bytes.
-	movl		OFFSETOF_KEYLEN(KEY), AESKEYLEN
+	movl		OFFSETOF_AESKEYLEN(KEY), AESKEYLEN
 
 	// Set up a counter block with 1 in the low 32-bit word.  This is the
 	// counter that produces the ciphertext needed to encrypt the auth tag.
@@ -1033,7 +1032,7 @@
 	vpxor		(GHASH_ACC_PTR), %xmm0, GHASH_ACC
 
 	// Load the first hash key power (H^1), which is stored last.
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-16(KEY), H_POW1
+	vmovdqu8	OFFSETOFEND_H_POWERS-16(KEY), H_POW1
 
 .if !\enc
 	// Prepare a mask of TAGLEN one bits.
@@ -1176,7 +1175,7 @@ SYM_FUNC_START(aes_gcm_aad_update_vaes_avx10)
 	// zero-extending it and allowing AADLEN64 to be used later.
 	sub		$32, AADLEN
 	jl		.Laad_loop_1x_done
-	vmovdqu8	OFFSETOFEND_KEY_POWERS-32(KEY), H_POW1	// [H^2, H^1]
+	vmovdqu8	OFFSETOFEND_H_POWERS-32(KEY), H_POW1	// [H^2, H^1]
 .Laad_loop_1x:
 	vmovdqu		(AAD), %ymm0
 	vpshufb		BSWAP_MASK, %ymm0, %ymm0
@@ -1199,7 +1198,7 @@ SYM_FUNC_START(aes_gcm_aad_update_vaes_avx10)
 	vmovdqu8	(AAD), %ymm0{%k1}{z}
 	neg		AADLEN64
 	and		$~15, AADLEN64  // -round_up(AADLEN, 16)
-	vmovdqu8	OFFSETOFEND_KEY_POWERS(KEY,AADLEN64), H_POW1
+	vmovdqu8	OFFSETOFEND_H_POWERS(KEY,AADLEN64), H_POW1
 	vpshufb		BSWAP_MASK, %ymm0, %ymm0
 	vpxor		%ymm0, GHASH_ACC, GHASH_ACC
 	_ghash_mul	H_POW1, GHASH_ACC, GHASH_ACC, GFPOLY, \
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index a64ff4660..4a7789269 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -1217,27 +1217,54 @@ DEFINE_XTS_ALG(vaes_avx2, "xts-aes-vaes-avx2", 600);
 DEFINE_XTS_ALG(vaes_avx10_256, "xts-aes-vaes-avx10_256", 700);
 DEFINE_XTS_ALG(vaes_avx10_512, "xts-aes-vaes-avx10_512", 800);
 
-#define NUM_KEY_POWERS		16 /* excludes zero padding */
-#define FULL_NUM_KEY_POWERS	(NUM_KEY_POWERS + 3) /* includes zero padding */
-
-/*
- * This is the key struct used by the VAES + AVX10 implementations of AES-GCM.
- * It contains the expanded AES key, some precomputed powers of the GHASH key,
- * and the RFC4106 nonce which is only used for the rfc4106 algorithms.  aes_key
- * and ghash_key_powers are aligned to a 64-byte boundary to make them naturally
- * aligned for 512-bit loads, which may improve performance on some CPUs.  (The
- * assembly code doesn't *need* the alignment; this is just an optimization.)
- */
-struct aes_gcm_key_avx10 {
+/* The common part of the x86_64 AES-GCM key struct */
+struct aes_gcm_key {
+	/* Expanded AES key and the AES key length in bytes */
 	struct crypto_aes_ctx aes_key;
+
+	/* RFC4106 nonce (used only by the rfc4106 algorithms) */
 	u32 rfc4106_nonce;
-	u8 ghash_key_powers[FULL_NUM_KEY_POWERS][16] __aligned(64);
 };
+
+/* Key struct used by the VAES + AVX10 implementations of AES-GCM */
+struct aes_gcm_key_avx10 {
+	/*
+	 * Common part of the key.  The assembly code prefers 16-byte alignment
+	 * for the round keys; we get this by them being located at the start of
+	 * the struct and the whole struct being 64-byte aligned.
+	 */
+	struct aes_gcm_key base;
+
+	/*
+	 * Powers of the hash key H^16 through H^1.  All entries have an extra
+	 * factor of x^-1 and are byte-reversed.  This is aligned to a 64-byte
+	 * boundary to make it naturally aligned for 512-bit loads, which may
+	 * improve performance on some CPUs.  (The assembly code doesn't *need*
+	 * the alignment; this is just an optimization.)
+	 */
+	u8 h_powers[16][16] __aligned(64);
+
+	/* Three padding blocks required by the assembly code */
+	u8 padding[3][16];
+};
+#define AES_GCM_KEY_AVX10(key)	\
+	container_of((key), struct aes_gcm_key_avx10, base)
 #define AES_GCM_KEY_AVX10_SIZE	\
 	(sizeof(struct aes_gcm_key_avx10) + (63 & ~(CRYPTO_MINALIGN - 1)))
 
-static inline struct aes_gcm_key_avx10 *
-aes_gcm_key_avx10_get(struct crypto_aead *tfm)
+/*
+ * These flags are passed to the AES-GCM helper functions to specify the
+ * specific version of AES-GCM (RFC4106 or not), whether it's encryption or
+ * decryption, and which assembly functions should be called.  Assembly
+ * functions are selected using flags instead of function pointers to avoid
+ * indirect calls (which are very expensive on x86) regardless of inlining.
+ */
+#define FLAG_RFC4106	BIT(0)
+#define FLAG_ENC	BIT(1)
+#define FLAG_AVX10_512	BIT(2)
+
+static inline struct aes_gcm_key *
+aes_gcm_key_get(struct crypto_aead *tfm, int flags)
 {
 	return PTR_ALIGN(crypto_aead_ctx(tfm), 64);
 }
@@ -1247,10 +1274,33 @@ aes_gcm_precompute_vaes_avx10_256(struct aes_gcm_key_avx10 *key);
 asmlinkage void
 aes_gcm_precompute_vaes_avx10_512(struct aes_gcm_key_avx10 *key);
 
+static void aes_gcm_precompute(struct aes_gcm_key *key, int flags)
+{
+	/*
+	 * To make things a bit easier on the assembly side, the AVX10
+	 * implementations use the same key format.  Therefore, a single
+	 * function using 256-bit vectors would suffice here.  However, it's
+	 * straightforward to provide a 512-bit one because of how the assembly
+	 * code is structured, and it works nicely because the total size of the
+	 * key powers is a multiple of 512 bits.  So we take advantage of that.
+	 */
+	if (flags & FLAG_AVX10_512)
+		aes_gcm_precompute_vaes_avx10_512(AES_GCM_KEY_AVX10(key));
+	else
+		aes_gcm_precompute_vaes_avx10_256(AES_GCM_KEY_AVX10(key));
+}
+
 asmlinkage void
 aes_gcm_aad_update_vaes_avx10(const struct aes_gcm_key_avx10 *key,
 			      u8 ghash_acc[16], const u8 *aad, int aadlen);
 
+static void aes_gcm_aad_update(const struct aes_gcm_key *key, u8 ghash_acc[16],
+			       const u8 *aad, int aadlen, int flags)
+{
+	aes_gcm_aad_update_vaes_avx10(AES_GCM_KEY_AVX10(key), ghash_acc,
+				      aad, aadlen);
+}
+
 asmlinkage void
 aes_gcm_enc_update_vaes_avx10_256(const struct aes_gcm_key_avx10 *key,
 				  const u32 le_ctr[4], u8 ghash_acc[16],
@@ -1269,19 +1319,71 @@ aes_gcm_dec_update_vaes_avx10_512(const struct aes_gcm_key_avx10 *key,
 				  const u32 le_ctr[4], u8 ghash_acc[16],
 				  const u8 *src, u8 *dst, int datalen);
 
+/* __always_inline to optimize out the branches based on @flags */
+static __always_inline void
+aes_gcm_update(const struct aes_gcm_key *key,
+	       const u32 le_ctr[4], u8 ghash_acc[16],
+	       const u8 *src, u8 *dst, int datalen, int flags)
+{
+	if (flags & FLAG_ENC) {
+		if (flags & FLAG_AVX10_512)
+			aes_gcm_enc_update_vaes_avx10_512(AES_GCM_KEY_AVX10(key),
+							  le_ctr, ghash_acc,
+							  src, dst, datalen);
+		else
+			aes_gcm_enc_update_vaes_avx10_256(AES_GCM_KEY_AVX10(key),
+							  le_ctr, ghash_acc,
+							  src, dst, datalen);
+	} else {
+		if (flags & FLAG_AVX10_512)
+			aes_gcm_dec_update_vaes_avx10_512(AES_GCM_KEY_AVX10(key),
+							  le_ctr, ghash_acc,
+							  src, dst, datalen);
+		else
+			aes_gcm_dec_update_vaes_avx10_256(AES_GCM_KEY_AVX10(key),
+							  le_ctr, ghash_acc,
+							  src, dst, datalen);
+	}
+}
+
 asmlinkage void
 aes_gcm_enc_final_vaes_avx10(const struct aes_gcm_key_avx10 *key,
 			     const u32 le_ctr[4], u8 ghash_acc[16],
 			     u64 total_aadlen, u64 total_datalen);
+
+/* __always_inline to optimize out the branches based on @flags */
+static __always_inline void
+aes_gcm_enc_final(const struct aes_gcm_key *key,
+		  const u32 le_ctr[4], u8 ghash_acc[16],
+		  u64 total_aadlen, u64 total_datalen, int flags)
+{
+	aes_gcm_enc_final_vaes_avx10(AES_GCM_KEY_AVX10(key),
+				     le_ctr, ghash_acc,
+				     total_aadlen, total_datalen);
+}
+
 asmlinkage bool __must_check
 aes_gcm_dec_final_vaes_avx10(const struct aes_gcm_key_avx10 *key,
 			     const u32 le_ctr[4], const u8 ghash_acc[16],
 			     u64 total_aadlen, u64 total_datalen,
 			     const u8 tag[16], int taglen);
 
+/* __always_inline to optimize out the branches based on @flags */
+static __always_inline bool __must_check
+aes_gcm_dec_final(const struct aes_gcm_key *key, const u32 le_ctr[4],
+		  u8 ghash_acc[16], u64 total_aadlen, u64 total_datalen,
+		  u8 tag[16], int taglen, int flags)
+{
+	return aes_gcm_dec_final_vaes_avx10(AES_GCM_KEY_AVX10(key),
+					    le_ctr, ghash_acc,
+					    total_aadlen, total_datalen,
+					    tag, taglen);
+}
+
 /*
- * This is the setkey function for the VAES + AVX10 implementations of AES-GCM.
- * It expands the AES key and precomputes powers of the hash key.
+ * This is the setkey function for the x86_64 implementations of AES-GCM.  It
+ * saves the RFC4106 nonce if applicable, expands the AES key, and precomputes
+ * powers of the hash key.
  *
  * To comply with the crypto_aead API, this has to be usable in no-SIMD context.
  * For that reason, this function includes a portable C implementation of the
@@ -1289,24 +1391,26 @@ aes_gcm_dec_final_vaes_avx10(const struct aes_gcm_key_avx10 *key,
  * about the same time as encrypting 37 KB of data.  To be ready for users that
  * may set a key even somewhat frequently, we therefore also include a SIMD
  * assembly implementation, expanding the AES key using AES-NI and precomputing
- * the hash key powers using VPCLMULQDQ.
- *
- * If vl512=true, indicating that the key is being set for the 512-bit
- * vectorized implementation, then we use a 512-bit version of the hash key
- * precomputation function.  The benefit of this is marginal over just reusing
- * the 256-bit one, but it's easy to provide, and it work outs nicely because
- * the total size of the key powers is a multiple of 512 bits.
+ * the hash key powers using PCLMULQDQ or VPCLMULQDQ.
  */
-static int gcm_setkey_vaes_avx10(struct crypto_aead *tfm, const u8 *raw_key,
-				 unsigned int keylen, bool vl512)
+static int gcm_setkey(struct crypto_aead *tfm, const u8 *raw_key,
+		      unsigned int keylen, int flags)
 {
-	struct aes_gcm_key_avx10 *key = aes_gcm_key_avx10_get(tfm);
+	struct aes_gcm_key *key = aes_gcm_key_get(tfm, flags);
 	int err;
 
+	if (flags & FLAG_RFC4106) {
+		if (keylen < 4)
+			return -EINVAL;
+		keylen -= 4;
+		key->rfc4106_nonce = get_unaligned_be32(raw_key + keylen);
+	}
+
 	/* The assembly code assumes the following offsets. */
-	BUILD_BUG_ON(offsetof(typeof(*key), aes_key.key_enc) != 0);
-	BUILD_BUG_ON(offsetof(typeof(*key), aes_key.key_length) != 480);
-	BUILD_BUG_ON(offsetof(typeof(*key), ghash_key_powers) != 512);
+	BUILD_BUG_ON(offsetof(struct aes_gcm_key_avx10, base.aes_key.key_enc) != 0);
+	BUILD_BUG_ON(offsetof(struct aes_gcm_key_avx10, base.aes_key.key_length) != 480);
+	BUILD_BUG_ON(offsetof(struct aes_gcm_key_avx10, h_powers) != 512);
+	BUILD_BUG_ON(offsetof(struct aes_gcm_key_avx10, padding) != 768);
 
 	if (likely(crypto_simd_usable())) {
 		err = aes_check_keylen(keylen);
@@ -1314,15 +1418,13 @@ static int gcm_setkey_vaes_avx10(struct crypto_aead *tfm, const u8 *raw_key,
 			return err;
 		kernel_fpu_begin();
 		aesni_set_key(&key->aes_key, raw_key, keylen);
-		if (vl512)
-			aes_gcm_precompute_vaes_avx10_512(key);
-		else
-			aes_gcm_precompute_vaes_avx10_256(key);
+		aes_gcm_precompute(key, flags);
 		kernel_fpu_end();
 	} else {
 		static const u8 x_to_the_minus1[16] __aligned(__alignof__(be128)) = {
 			[0] = 0xc2, [15] = 1
 		};
+		struct aes_gcm_key_avx10 *k = AES_GCM_KEY_AVX10(key);
 		be128 h1 = {};
 		be128 h;
 		int i;
@@ -1330,74 +1432,33 @@ static int gcm_setkey_vaes_avx10(struct crypto_aead *tfm, const u8 *raw_key,
 		err = aes_expandkey(&key->aes_key, raw_key, keylen);
 		if (err)
 			return err;
-		/*
-		 * Emulate the aes_gcm_precompute assembly function in portable
-		 * C code: Encrypt the all-zeroes block to get the hash key H^1,
-		 * zeroize the padding at the end of ghash_key_powers, and store
-		 * H^1 * x^-1 through H^NUM_KEY_POWERS * x^-1, byte-reversed.
-		 */
+
+		/* Encrypt the all-zeroes block to get the hash key H^1 */
 		aes_encrypt(&key->aes_key, (u8 *)&h1, (u8 *)&h1);
-		memset(key->ghash_key_powers, 0, sizeof(key->ghash_key_powers));
+
+		/* Compute H^1 * x^-1 */
 		h = h1;
 		gf128mul_lle(&h, (const be128 *)x_to_the_minus1);
-		for (i = NUM_KEY_POWERS - 1; i >= 0; i--) {
-			put_unaligned_be64(h.a, &key->ghash_key_powers[i][8]);
-			put_unaligned_be64(h.b, &key->ghash_key_powers[i][0]);
+
+		/* Compute the needed key powers */
+		for (i = ARRAY_SIZE(k->h_powers) - 1; i >= 0; i--) {
+			put_unaligned_be64(h.a, &k->h_powers[i][8]);
+			put_unaligned_be64(h.b, &k->h_powers[i][0]);
 			gf128mul_lle(&h, &h1);
 		}
+		memset(k->padding, 0, sizeof(k->padding));
 	}
 	return 0;
 }
 
-static int gcm_setkey_vaes_avx10_256(struct crypto_aead *tfm, const u8 *raw_key,
-				     unsigned int keylen)
-{
-	return gcm_setkey_vaes_avx10(tfm, raw_key, keylen, /* vl512= */ false);
-}
-
-static int gcm_setkey_vaes_avx10_512(struct crypto_aead *tfm, const u8 *raw_key,
-				     unsigned int keylen)
-{
-	return gcm_setkey_vaes_avx10(tfm, raw_key, keylen, /* vl512= */ true);
-}
-
-static int rfc4106_setkey_vaes_avx10_256(struct crypto_aead *tfm,
-					 const u8 *raw_key, unsigned int keylen)
-{
-	struct aes_gcm_key_avx10 *key = aes_gcm_key_avx10_get(tfm);
-
-	if (keylen < 4)
-		return -EINVAL;
-	keylen -= 4;
-	key->rfc4106_nonce = get_unaligned_be32(raw_key + keylen);
-	return gcm_setkey_vaes_avx10(tfm, raw_key, keylen, /* vl512= */ false);
-}
-
-static int rfc4106_setkey_vaes_avx10_512(struct crypto_aead *tfm,
-					 const u8 *raw_key, unsigned int keylen)
-{
-	struct aes_gcm_key_avx10 *key = aes_gcm_key_avx10_get(tfm);
-
-	if (keylen < 4)
-		return -EINVAL;
-	keylen -= 4;
-	key->rfc4106_nonce = get_unaligned_be32(raw_key + keylen);
-	return gcm_setkey_vaes_avx10(tfm, raw_key, keylen, /* vl512= */ true);
-}
-
-#define FLAG_RFC4106	BIT(0)
-#define FLAG_ENC	BIT(1)
-#define FLAG_VL512	BIT(2)
-
 /*
  * Initialize @ghash_acc, then pass all @assoclen bytes of associated data
  * (a.k.a. additional authenticated data) from @sg_src through the GHASH update
  * assembly function.  kernel_fpu_begin() must have already been called.
  */
-static void gcm_process_assoc_vaes_avx10(const struct aes_gcm_key_avx10 *key,
-					 u8 ghash_acc[16],
-					 struct scatterlist *sg_src,
-					 unsigned int assoclen)
+static void gcm_process_assoc(const struct aes_gcm_key *key, u8 ghash_acc[16],
+			      struct scatterlist *sg_src, unsigned int assoclen,
+			      int flags)
 {
 	struct scatter_walk walk;
 	/*
@@ -1427,13 +1488,13 @@ static void gcm_process_assoc_vaes_avx10(const struct aes_gcm_key_avx10 *key,
 			len_this_page -= len;
 			if (pos < 16)
 				goto next;
-			aes_gcm_aad_update_vaes_avx10(key, ghash_acc, buf, 16);
+			aes_gcm_aad_update(key, ghash_acc, buf, 16, flags);
 			pos = 0;
 		}
 		len = len_this_page;
 		if (unlikely(assoclen)) /* Not the last segment yet? */
 			len = round_down(len, 16);
-		aes_gcm_aad_update_vaes_avx10(key, ghash_acc, src, len);
+		aes_gcm_aad_update(key, ghash_acc, src, len, flags);
 		src += len;
 		len_this_page -= len;
 		if (unlikely(len_this_page)) {
@@ -1449,42 +1510,16 @@ static void gcm_process_assoc_vaes_avx10(const struct aes_gcm_key_avx10 *key,
 		}
 	}
 	if (unlikely(pos))
-		aes_gcm_aad_update_vaes_avx10(key, ghash_acc, buf, pos);
+		aes_gcm_aad_update(key, ghash_acc, buf, pos, flags);
 }
 
-/* __always_inline to optimize out the branches based on @flags */
-static __always_inline void
-aes_gcm_update_vaes_avx10(const struct aes_gcm_key_avx10 *key,
-			  const u32 le_ctr[4], u8 ghash_acc[16],
-			  const u8 *src, u8 *dst, int datalen, int flags)
-{
-	if (flags & FLAG_ENC) {
-		if (flags & FLAG_VL512)
-			aes_gcm_enc_update_vaes_avx10_512(key, le_ctr,
-							  ghash_acc,
-							  src, dst, datalen);
-		else
-			aes_gcm_enc_update_vaes_avx10_256(key, le_ctr,
-							  ghash_acc,
-							  src, dst, datalen);
-	} else {
-		if (flags & FLAG_VL512)
-			aes_gcm_dec_update_vaes_avx10_512(key, le_ctr,
-							  ghash_acc,
-							  src, dst, datalen);
-		else
-			aes_gcm_dec_update_vaes_avx10_256(key, le_ctr,
-							  ghash_acc,
-							  src, dst, datalen);
-	}
-}
 
 /* __always_inline to optimize out the branches based on @flags */
 static __always_inline int
-gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
+gcm_crypt(struct aead_request *req, int flags)
 {
 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
-	const struct aes_gcm_key_avx10 *key = aes_gcm_key_avx10_get(tfm);
+	const struct aes_gcm_key *key = aes_gcm_key_get(tfm, flags);
 	unsigned int assoclen = req->assoclen;
 	struct skcipher_walk walk;
 	unsigned int nbytes;
@@ -1525,7 +1560,7 @@ gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
 	kernel_fpu_begin();
 
 	/* Pass the associated data through GHASH. */
-	gcm_process_assoc_vaes_avx10(key, ghash_acc, req->src, assoclen);
+	gcm_process_assoc(key, ghash_acc, req->src, assoclen, flags);
 
 	/* En/decrypt the data and pass the ciphertext through GHASH. */
 	while ((nbytes = walk.nbytes) != 0) {
@@ -1538,20 +1573,18 @@ gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
 			 * just need to round down to a multiple of 16.
 			 */
 			nbytes = round_down(nbytes, AES_BLOCK_SIZE);
-			aes_gcm_update_vaes_avx10(key, le_ctr, ghash_acc,
-						  walk.src.virt.addr,
-						  walk.dst.virt.addr,
-						  nbytes, flags);
+			aes_gcm_update(key, le_ctr, ghash_acc,
+				       walk.src.virt.addr, walk.dst.virt.addr,
+				       nbytes, flags);
 			le_ctr[0] += nbytes / AES_BLOCK_SIZE;
 			kernel_fpu_end();
 			err = skcipher_walk_done(&walk, walk.nbytes - nbytes);
 			kernel_fpu_begin();
 		} else {
 			/* Last segment: process all remaining data. */
-			aes_gcm_update_vaes_avx10(key, le_ctr, ghash_acc,
-						  walk.src.virt.addr,
-						  walk.dst.virt.addr,
-						  nbytes, flags);
+			aes_gcm_update(key, le_ctr, ghash_acc,
+				       walk.src.virt.addr, walk.dst.virt.addr,
+				       nbytes, flags);
 			err = skcipher_walk_done(&walk, 0);
 			/*
 			 * The low word of the counter isn't used by the
@@ -1566,8 +1599,8 @@ gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
 	taglen = crypto_aead_authsize(tfm);
 	if (flags & FLAG_ENC) {
 		/* Finish computing the auth tag. */
-		aes_gcm_enc_final_vaes_avx10(key, le_ctr, ghash_acc, assoclen,
-					     req->cryptlen);
+		aes_gcm_enc_final(key, le_ctr, ghash_acc, assoclen,
+				  req->cryptlen, flags);
 
 		/* Store the computed auth tag in the dst scatterlist. */
 		scatterwalk_map_and_copy(ghash_acc, req->dst, req->assoclen +
@@ -1584,9 +1617,8 @@ gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
 		 * transmitted one.  The assembly function does the actual tag
 		 * comparison.  Here, just check the boolean result.
 		 */
-		if (!aes_gcm_dec_final_vaes_avx10(key, le_ctr, ghash_acc,
-						  assoclen, datalen,
-						  tag, taglen))
+		if (!aes_gcm_dec_final(key, le_ctr, ghash_acc, assoclen,
+				       datalen, tag, taglen, flags))
 			err = -EBADMSG;
 	}
 out:
@@ -1594,121 +1626,88 @@ gcm_crypt_vaes_avx10(struct aead_request *req, int flags)
 	return err;
 }
 
-static int rfc4106_encrypt_vaes_avx10_256(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_RFC4106|FLAG_ENC);
-}
-
-static int rfc4106_decrypt_vaes_avx10_256(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_RFC4106);
-}
-
-static int gcm_encrypt_vaes_avx10_256(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_ENC);
-}
-
-static int gcm_decrypt_vaes_avx10_256(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, 0);
-}
-
-static int rfc4106_encrypt_vaes_avx10_512(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_RFC4106|FLAG_ENC|FLAG_VL512);
-}
-
-static int rfc4106_decrypt_vaes_avx10_512(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_RFC4106|FLAG_VL512);
-}
-
-static int gcm_encrypt_vaes_avx10_512(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_ENC|FLAG_VL512);
-}
-
-static int gcm_decrypt_vaes_avx10_512(struct aead_request *req)
-{
-	return gcm_crypt_vaes_avx10(req, FLAG_VL512);
-}
-
-static struct aead_alg aes_gcm_algs_vaes_avx10_256[] = { {
-	.setkey			= rfc4106_setkey_vaes_avx10_256,
-	.setauthsize		= common_rfc4106_set_authsize,
-	.encrypt		= rfc4106_encrypt_vaes_avx10_256,
-	.decrypt		= rfc4106_decrypt_vaes_avx10_256,
-	.ivsize			= GCM_RFC4106_IV_SIZE,
-	.chunksize		= AES_BLOCK_SIZE,
-	.maxauthsize		= 16,
-	.base = {
-		.cra_name		= "__rfc4106(gcm(aes))",
-		.cra_driver_name	= "__rfc4106-gcm-vaes-avx10_256",
-		.cra_priority		= 700,
-		.cra_flags		= CRYPTO_ALG_INTERNAL,
-		.cra_blocksize		= 1,
-		.cra_ctxsize		= AES_GCM_KEY_AVX10_SIZE,
-		.cra_module		= THIS_MODULE,
-	},
-}, {
-	.setkey			= gcm_setkey_vaes_avx10_256,
-	.setauthsize		= generic_gcmaes_set_authsize,
-	.encrypt		= gcm_encrypt_vaes_avx10_256,
-	.decrypt		= gcm_decrypt_vaes_avx10_256,
-	.ivsize			= GCM_AES_IV_SIZE,
-	.chunksize		= AES_BLOCK_SIZE,
-	.maxauthsize		= 16,
-	.base = {
-		.cra_name		= "__gcm(aes)",
-		.cra_driver_name	= "__generic-gcm-vaes-avx10_256",
-		.cra_priority		= 700,
-		.cra_flags		= CRYPTO_ALG_INTERNAL,
-		.cra_blocksize		= 1,
-		.cra_ctxsize		= AES_GCM_KEY_AVX10_SIZE,
-		.cra_module		= THIS_MODULE,
-	},
-} };
-
-static struct aead_alg aes_gcm_algs_vaes_avx10_512[] = { {
-	.setkey			= rfc4106_setkey_vaes_avx10_512,
-	.setauthsize		= common_rfc4106_set_authsize,
-	.encrypt		= rfc4106_encrypt_vaes_avx10_512,
-	.decrypt		= rfc4106_decrypt_vaes_avx10_512,
-	.ivsize			= GCM_RFC4106_IV_SIZE,
-	.chunksize		= AES_BLOCK_SIZE,
-	.maxauthsize		= 16,
-	.base = {
-		.cra_name		= "__rfc4106(gcm(aes))",
-		.cra_driver_name	= "__rfc4106-gcm-vaes-avx10_512",
-		.cra_priority		= 800,
-		.cra_flags		= CRYPTO_ALG_INTERNAL,
-		.cra_blocksize		= 1,
-		.cra_ctxsize		= AES_GCM_KEY_AVX10_SIZE,
-		.cra_module		= THIS_MODULE,
-	},
-}, {
-	.setkey			= gcm_setkey_vaes_avx10_512,
-	.setauthsize		= generic_gcmaes_set_authsize,
-	.encrypt		= gcm_encrypt_vaes_avx10_512,
-	.decrypt		= gcm_decrypt_vaes_avx10_512,
-	.ivsize			= GCM_AES_IV_SIZE,
-	.chunksize		= AES_BLOCK_SIZE,
-	.maxauthsize		= 16,
-	.base = {
-		.cra_name		= "__gcm(aes)",
-		.cra_driver_name	= "__generic-gcm-vaes-avx10_512",
-		.cra_priority		= 800,
-		.cra_flags		= CRYPTO_ALG_INTERNAL,
-		.cra_blocksize		= 1,
-		.cra_ctxsize		= AES_GCM_KEY_AVX10_SIZE,
-		.cra_module		= THIS_MODULE,
-	},
-} };
+#define DEFINE_GCM_ALGS(suffix, flags, generic_driver_name, rfc_driver_name,   \
+			ctxsize, priority)				       \
+									       \
+static int gcm_setkey_##suffix(struct crypto_aead *tfm, const u8 *raw_key,     \
+			    unsigned int keylen)			       \
+{									       \
+	return gcm_setkey(tfm, raw_key, keylen, (flags));		       \
+}									       \
+									       \
+static int gcm_encrypt_##suffix(struct aead_request *req)		       \
+{									       \
+	return gcm_crypt(req, (flags) | FLAG_ENC);			       \
+}									       \
+									       \
+static int gcm_decrypt_##suffix(struct aead_request *req)		       \
+{									       \
+	return gcm_crypt(req, (flags));					       \
+}									       \
+									       \
+static int rfc4106_setkey_##suffix(struct crypto_aead *tfm, const u8 *raw_key, \
+				unsigned int keylen)			       \
+{									       \
+	return gcm_setkey(tfm, raw_key, keylen, (flags) | FLAG_RFC4106);       \
+}									       \
+									       \
+static int rfc4106_encrypt_##suffix(struct aead_request *req)		       \
+{									       \
+	return gcm_crypt(req, (flags) | FLAG_RFC4106 | FLAG_ENC);	       \
+}									       \
+									       \
+static int rfc4106_decrypt_##suffix(struct aead_request *req)		       \
+{									       \
+	return gcm_crypt(req, (flags) | FLAG_RFC4106);			       \
+}									       \
+									       \
+static struct aead_alg aes_gcm_algs_##suffix[] = { {			       \
+	.setkey			= gcm_setkey_##suffix,			       \
+	.setauthsize		= generic_gcmaes_set_authsize,		       \
+	.encrypt		= gcm_encrypt_##suffix,			       \
+	.decrypt		= gcm_decrypt_##suffix,			       \
+	.ivsize			= GCM_AES_IV_SIZE,			       \
+	.chunksize		= AES_BLOCK_SIZE,			       \
+	.maxauthsize		= 16,					       \
+	.base = {							       \
+		.cra_name		= "__gcm(aes)",			       \
+		.cra_driver_name	= "__" generic_driver_name,	       \
+		.cra_priority		= (priority),			       \
+		.cra_flags		= CRYPTO_ALG_INTERNAL,		       \
+		.cra_blocksize		= 1,				       \
+		.cra_ctxsize		= (ctxsize),			       \
+		.cra_module		= THIS_MODULE,			       \
+	},								       \
+}, {									       \
+	.setkey			= rfc4106_setkey_##suffix,		       \
+	.setauthsize		= common_rfc4106_set_authsize,		       \
+	.encrypt		= rfc4106_encrypt_##suffix,		       \
+	.decrypt		= rfc4106_decrypt_##suffix,		       \
+	.ivsize			= GCM_RFC4106_IV_SIZE,			       \
+	.chunksize		= AES_BLOCK_SIZE,			       \
+	.maxauthsize		= 16,					       \
+	.base = {							       \
+		.cra_name		= "__rfc4106(gcm(aes))",	       \
+		.cra_driver_name	= "__" rfc_driver_name,		       \
+		.cra_priority		= (priority),			       \
+		.cra_flags		= CRYPTO_ALG_INTERNAL,		       \
+		.cra_blocksize		= 1,				       \
+		.cra_ctxsize		= (ctxsize),			       \
+		.cra_module		= THIS_MODULE,			       \
+	},								       \
+} };									       \
+									       \
+static struct simd_aead_alg *aes_gcm_simdalgs_##suffix[2]		       \
 
-static struct simd_aead_alg *aes_gcm_simdalgs_vaes_avx10_256[2];
-static struct simd_aead_alg *aes_gcm_simdalgs_vaes_avx10_512[2];
+/* aes_gcm_algs_vaes_avx10_256 */
+DEFINE_GCM_ALGS(vaes_avx10_256, 0,
+		"generic-gcm-vaes-avx10_256", "rfc4106-gcm-vaes-avx10_256",
+		AES_GCM_KEY_AVX10_SIZE, 700);
 
+/* aes_gcm_algs_vaes_avx10_512 */
+DEFINE_GCM_ALGS(vaes_avx10_512, FLAG_AVX10_512,
+		"generic-gcm-vaes-avx10_512", "rfc4106-gcm-vaes-avx10_512",
+		AES_GCM_KEY_AVX10_SIZE, 800);
 #endif /* CONFIG_AS_VAES && CONFIG_AS_VPCLMULQDQ */
 
 /*
-- 
2.45.1.145.g83f1add914

