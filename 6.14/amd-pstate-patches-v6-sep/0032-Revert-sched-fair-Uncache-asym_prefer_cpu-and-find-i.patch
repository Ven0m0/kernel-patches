From 326687a4a61a2bd69d6543cdc38f5fdd871c29ea Mon Sep 17 00:00:00 2001
From: Eric Naim <dnaim@cachyos.org>
Date: Thu, 10 Apr 2025 00:48:11 +0800
Subject: [PATCH 32/37] Revert "sched/fair: Uncache asym_prefer_cpu and find it
 during update_sd_lb_stats()"

This reverts commit 34562659110a42276d63f8b5a9d522d2400b2df2.

As per [1], this implementation loses caching benefits and adds some
overhead in common scheduler code.

[1] https://lore.kernel.org/20250409053446.23367-1-kprateek.nayak@amd.com

Signed-off-by: Eric Naim <dnaim@cachyos.org>
---
 kernel/sched/fair.c     | 21 ++-------------------
 kernel/sched/sched.h    |  1 +
 kernel/sched/topology.c | 15 ++++++++++++++-
 3 files changed, 17 insertions(+), 20 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6e1e051e4..89c726010 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -9932,8 +9932,6 @@ struct sg_lb_stats {
 	unsigned int group_weight;
 	enum group_type group_type;
 	unsigned int group_asym_packing;	/* Tasks should be moved to preferred CPU */
-	unsigned int asym_prefer_cpu;		/* Group CPU with highest asym priority */
-	int highest_asym_prio;			/* Asym priority of asym_prefer_cpu */
 	unsigned int group_smt_balance;		/* Task on busy SMT be moved */
 	unsigned long group_misfit_task_load;	/* A CPU has a task too big for its capacity */
 #ifdef CONFIG_NUMA_BALANCING
@@ -10263,7 +10261,7 @@ sched_group_asym(struct lb_env *env, struct sg_lb_stats *sgs, struct sched_group
 	    (sgs->group_weight - sgs->idle_cpus != 1))
 		return false;
 
-	return sched_asym(env->sd, env->dst_cpu, sgs->asym_prefer_cpu);
+	return sched_asym(env->sd, env->dst_cpu, group->asym_prefer_cpu);
 }
 
 /* One group has more than one SMT CPU while the other group does not */
@@ -10344,17 +10342,6 @@ sched_reduced_capacity(struct rq *rq, struct sched_domain *sd)
 	return check_cpu_capacity(rq, sd);
 }
 
-static inline void
-update_sg_pick_asym_prefer(struct sg_lb_stats *sgs, int cpu)
-{
-	int asym_prio = arch_asym_cpu_priority(cpu);
-
-	if (asym_prio > sgs->highest_asym_prio) {
-		sgs->asym_prefer_cpu = cpu;
-		sgs->highest_asym_prio = asym_prio;
-	}
-}
-
 /**
  * update_sg_lb_stats - Update sched_group's statistics for load balancing.
  * @env: The load balancing environment.
@@ -10377,7 +10364,6 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 	memset(sgs, 0, sizeof(*sgs));
 
 	local_group = group == sds->local;
-	sgs->highest_asym_prio = INT_MIN;
 
 	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
 		struct rq *rq = cpu_rq(i);
@@ -10391,9 +10377,6 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 		nr_running = rq->nr_running;
 		sgs->sum_nr_running += nr_running;
 
-		if (sd_flags & SD_ASYM_PACKING)
-			update_sg_pick_asym_prefer(sgs, i);
-
 		if (cpu_overutilized(i))
 			*sg_overutilized = 1;
 
@@ -10515,7 +10498,7 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 
 	case group_asym_packing:
 		/* Prefer to move from lowest priority CPU's work */
-		return sched_asym_prefer(busiest->asym_prefer_cpu, sgs->asym_prefer_cpu);
+		return sched_asym_prefer(sds->busiest->asym_prefer_cpu, sg->asym_prefer_cpu);
 
 	case group_misfit_task:
 		/*
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2dd59d21f..1aa65a0ac 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2086,6 +2086,7 @@ struct sched_group {
 	unsigned int		group_weight;
 	unsigned int		cores;
 	struct sched_group_capacity *sgc;
+	int			asym_prefer_cpu;	/* CPU of highest priority in group */
 	int			flags;
 
 	/*
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 730228279..363ad268a 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -1310,7 +1310,7 @@ static void init_sched_groups_capacity(int cpu, struct sched_domain *sd)
 	WARN_ON(!sg);
 
 	do {
-		int cpu, cores = 0;
+		int cpu, cores = 0, max_cpu = -1;
 
 		sg->group_weight = cpumask_weight(sched_group_span(sg));
 
@@ -1322,6 +1322,19 @@ static void init_sched_groups_capacity(int cpu, struct sched_domain *sd)
 #endif
 		}
 		sg->cores = cores;
+
+		if (!(sd->flags & SD_ASYM_PACKING))
+			goto next;
+
+		for_each_cpu(cpu, sched_group_span(sg)) {
+			if (max_cpu < 0)
+				max_cpu = cpu;
+			else if (sched_asym_prefer(cpu, max_cpu))
+				max_cpu = cpu;
+		}
+		sg->asym_prefer_cpu = max_cpu;
+
+next:
 		sg = sg->next;
 	} while (sg != sd->groups);
 
-- 
2.49.0

