From 534725fd48618569c1f28063587c9bba29ce788a Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Sun, 4 Dec 2022 17:23:37 +0100
Subject: [PATCH 17/19] RFC! sched/numa: Process Adaptive autoNUMA

This patchset implements an adaptive algorithm for calculating the autonuma
scan period. In the existing mechanism of scan period calculation,

- scan period is derived from the per-thread stats.
- static threshold (NUMA_PERIOD_THRESHOLD) is used for changing the
  scan rate.

In this new approach (Process Adaptive autoNUMA or PAN), we gather NUMA
fault stats at per-process level which allows for capturing the application
behaviour better. In addition, the algorithm learns and adjusts the scan
rate based on remote fault rate. By not sticking to a static threshold, the
algorithm can respond better to different workload behaviours.

Since the threads of a processes are already considered as a group,
we add a bunch of metrics to the task's mm to track the various
types of faults and derive the scan rate from them.

The new per-process fault stats contribute only to the per-process
scan period calculation, while the existing per-thread stats continue
to contribute towards the numa_group stats which eventually
determine the thresholds for migrating memory and threads
across nodes.

This patchset has been tested with a bunch of benchmarks on the
following system:

2 socket AMD Milan System
32 cores or 64 threads per socket
256GB memory per socket amounting to 512GB in total
transparent_hugepage=never has been used for all the below tests.
NPS1 NUMA configuration where each socket is a NUMA node

$ numactl -H
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
node 0 size: 257645 MB
node 0 free: 255193 MB
node 1 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
node 1 size: 257984 MB
node 1 free: 257017 MB
node distances:
node   0   1
  0:  10  32
  1:  32  10

While this is an early version that we are experimenting on NPS1
configuration with THP off, we plan to test it on other configurations
as well. However posting it now for some early feedback.

Here are the numbers from some of the benchmarks that we have
tried. A brief description of these benchmarks is also given at
end of this mail. The detailed results are available at
https://drive.google.com/drive/folders/1O7sY-YBsT3F5GHZMiOGQbXkgpp6O0BEo

Plots of how scan period varies in default vs PAN for a couple
of benchmarks are also present in the above folder.

------------------------------------------------------
% gain of PAN vs default (Avg of 3 runs)
------------------------------------------------------
NAS-BT		-0.17
NAS-CG		+9.39
NAS-MG		+8.19
NAS-FT		+2.23
Hashjoin	+0.58
Graph500	+14.93
Pagerank	+0.37
------------------------------------------------------
		Default		PAN		%diff
------------------------------------------------------
		NUMA hint faults(Total of 3 runs)
------------------------------------------------------
NAS-BT		758282358	539850429	+29
NAS-CG		2179458823	1180301361	+46
NAS-MG		517641172	346066391	+33
NAS-FT		297044964	230033861	+23
Hashjoin	201684863	268436275	-33
Graph500	261808733	154338827	+41
Pagerank	217917818	211260310	+03
------------------------------------------------------
		Migrations(Total of 3 runs)
------------------------------------------------------
NAS-BT		106888517	86482076	+19
NAS-CG		81191368	12859924	+84
NAS-MG		83927451	39651254	+53
NAS-FT		61807715	38934618	+37
Hashjoin	45406983	59828843	-32
Graph500	22798837	21560714	+05
Pagerank	59072135	44968673	+24
------------------------------------------------------

And here are some tests from a few microbenchmarks of mmtests suite.
(The results are trimmed a bit here, the complete results can
be viewed in the above mentioned link)

Hackbench
---------
hackbench-process-pipes
                           hackbench              hackbench
                             default                    pan
Min       256     23.5510 (   0.00%)     23.1900 (   1.53%)
Amean     256     24.4604 (   0.00%)     24.0353 *   1.74%*
Stddev    256      0.4420 (   0.00%)      0.7611 ( -72.18%)
CoeffVar  256      1.8072 (   0.00%)      3.1666 ( -75.22%)
Max       256     25.4930 (   0.00%)     30.5450 ( -19.82%)
BAmean-50 256     24.1074 (   0.00%)     23.6616 (   1.85%)
BAmean-95 256     24.4111 (   0.00%)     23.9308 (   1.97%)
BAmean-99 256     24.4499 (   0.00%)     23.9696 (   1.96%)

                   hackbench   hackbench
                     default         pan
Duration User       25810.02    25158.93
Duration System    276322.70   271729.32
Duration Elapsed     2707.75     2671.33

                                      hackbench      hackbench
                                        default            pan
Ops NUMA alloc hit                1082415453.00  1088025994.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local              1082415441.00  1088025974.00
Ops NUMA base-page range updates       33475.00      228900.00
Ops NUMA PTE updates                   33475.00      228900.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                   15758.00      222100.00
Ops NUMA hint local faults %           15371.00      214570.00
Ops NUMA hint local percent               97.54          96.61
Ops NUMA pages migrated                  235.00        4029.00
Ops AutoNUMA cost                         79.03        1112.18

tbench
------
tbench4
                              tbench                 tbench
                             default                    pan
Hmean     1        436.89 (   0.00%)      432.73 *  -0.95%*
Hmean     2        834.27 (   0.00%)      848.11 *   1.66%*
Hmean     4       1629.50 (   0.00%)     1614.22 *  -0.94%*
Hmean     8       2944.06 (   0.00%)     3031.66 *   2.98%*
Hmean     16      5418.25 (   0.00%)     5674.74 *   4.73%*
Hmean     32      9959.60 (   0.00%)     9009.82 *  -9.54%*
Hmean     64     13999.14 (   0.00%)    12160.51 * -13.13%*
Hmean     128    16797.09 (   0.00%)    16506.14 *  -1.73%*
Hmean     256    25344.27 (   0.00%)    25683.66 *   1.34%*
Hmean     512    25289.03 (   0.00%)    25513.77 *   0.89%*
BHmean-50 1        437.13 (   0.00%)      433.01 (  -0.94%)
BHmean-50 2        836.35 (   0.00%)      848.85 (   1.49%)
BHmean-50 4       1631.39 (   0.00%)     1618.43 (  -0.79%)
BHmean-50 8       2948.25 (   0.00%)     3037.86 (   3.04%)
BHmean-50 16      5425.17 (   0.00%)     5684.25 (   4.78%)
BHmean-50 32      9969.17 (   0.00%)     9034.06 (  -9.38%)
BHmean-50 64     14013.93 (   0.00%)    12202.07 ( -12.93%)
BHmean-50 128    16881.94 (   0.00%)    16571.27 (  -1.84%)
BHmean-50 256    25379.59 (   0.00%)    25819.18 (   1.73%)
BHmean-50 512    25435.41 (   0.00%)    25718.02 (   1.11%)
BHmean-95 1        436.92 (   0.00%)      432.81 (  -0.94%)
BHmean-95 2        834.59 (   0.00%)      848.23 (   1.63%)
BHmean-95 4       1629.73 (   0.00%)     1614.83 (  -0.91%)
BHmean-95 8       2945.02 (   0.00%)     3032.19 (   2.96%)
BHmean-95 16      5418.86 (   0.00%)     5675.91 (   4.74%)
BHmean-95 32      9962.57 (   0.00%)     9014.17 (  -9.52%)
BHmean-95 64     14002.44 (   0.00%)    12164.32 ( -13.13%)
BHmean-95 128    16820.56 (   0.00%)    16522.82 (  -1.77%)
BHmean-95 256    25347.34 (   0.00%)    25692.56 (   1.36%)
BHmean-95 512    25302.10 (   0.00%)    25528.52 (   0.89%)
BHmean-99 1        436.90 (   0.00%)      432.75 (  -0.95%)
BHmean-99 2        834.35 (   0.00%)      848.17 (   1.66%)
BHmean-99 4       1629.57 (   0.00%)     1614.38 (  -0.93%)
BHmean-99 8       2944.36 (   0.00%)     3031.77 (   2.97%)
BHmean-99 16      5418.40 (   0.00%)     5675.01 (   4.74%)
BHmean-99 32      9961.01 (   0.00%)     9011.43 (  -9.53%)
BHmean-99 64     14000.68 (   0.00%)    12161.34 ( -13.14%)
BHmean-99 128    16803.44 (   0.00%)    16511.94 (  -1.73%)
BHmean-99 256    25344.93 (   0.00%)    25685.57 (   1.34%)
BHmean-99 512    25291.87 (   0.00%)    25516.94 (   0.89%)

                      tbench      tbench
                     default         pan
Duration User        8482.50     8289.35
Duration System     49462.63    49364.56
Duration Elapsed     2217.10     2217.08

                                         tbench         tbench
                                        default            pan
Ops NUMA alloc hit                 388738400.00   378941469.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local               388738391.00   378941455.00
Ops NUMA base-page range updates      266760.00      266275.00
Ops NUMA PTE updates                  266760.00      266275.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                  241547.00      257790.00
Ops NUMA hint local faults %          145814.00      126410.00
Ops NUMA hint local percent               60.37          49.04
Ops NUMA pages migrated                51535.00       66083.00
Ops AutoNUMA cost                       1210.58        1292.07

dbench
------
dbench4 Latency
                                       dbench                 dbench
                                      default                    pan
Amean     latency-1           2.02 (   0.00%)        2.05 *  -1.52%*
Amean     latency-2           2.60 (   0.00%)        2.55 *   1.64%*
Amean     latency-4           3.52 (   0.00%)        3.56 *  -1.17%*
Amean     latency-8          12.79 (   0.00%)       11.83 *   7.49%*
Amean     latency-16         23.33 (   0.00%)       19.09 *  18.19%*
Amean     latency-32         19.30 (   0.00%)       18.83 *   2.43%*
Amean     latency-64         25.32 (   0.00%)       24.30 *   4.00%*
Amean     latency-128        45.25 (   0.00%)       42.93 *   5.13%*
Amean     latency-1024        0.00 (   0.00%)        0.00 *   0.00%*
BAmean-50 latency-1           1.65 (   0.00%)        1.74 (  -5.16%)
BAmean-50 latency-2           2.10 (   0.00%)        2.10 (  -0.13%)
BAmean-50 latency-4           2.65 (   0.00%)        2.71 (  -2.28%)
BAmean-50 latency-8           6.21 (   0.00%)        4.64 (  25.30%)
BAmean-50 latency-16         17.64 (   0.00%)       14.08 (  20.16%)
BAmean-50 latency-32         15.58 (   0.00%)       15.90 (  -2.07%)
BAmean-50 latency-64         20.76 (   0.00%)       20.31 (   2.15%)
BAmean-50 latency-128        36.22 (   0.00%)       34.85 (   3.80%)
BAmean-50 latency-1024        0.00 (   0.00%)        0.00 (   0.00%)
BAmean-95 latency-1           1.88 (   0.00%)        1.94 (  -3.17%)
BAmean-95 latency-2           2.25 (   0.00%)        2.26 (  -0.26%)
BAmean-95 latency-4           3.00 (   0.00%)        3.08 (  -2.71%)
BAmean-95 latency-8          11.66 (   0.00%)       10.03 (  13.97%)
BAmean-95 latency-16         22.30 (   0.00%)       17.68 (  20.73%)
BAmean-95 latency-32         17.95 (   0.00%)       17.70 (   1.38%)
BAmean-95 latency-64         23.57 (   0.00%)       22.72 (   3.62%)
BAmean-95 latency-128        42.44 (   0.00%)       39.96 (   5.84%)
BAmean-95 latency-1024        0.00 (   0.00%)        0.00 (   0.00%)
BAmean-99 latency-1           1.90 (   0.00%)        1.96 (  -3.30%)
BAmean-99 latency-2           2.38 (   0.00%)        2.37 (   0.48%)
BAmean-99 latency-4           3.24 (   0.00%)        3.34 (  -3.26%)
BAmean-99 latency-8          12.34 (   0.00%)       10.71 (  13.27%)
BAmean-99 latency-16         22.79 (   0.00%)       18.27 (  19.82%)
BAmean-99 latency-32         18.68 (   0.00%)       18.32 (   1.93%)
BAmean-99 latency-64         24.69 (   0.00%)       23.69 (   4.06%)
BAmean-99 latency-128        44.44 (   0.00%)       42.15 (   5.17%)
BAmean-99 latency-1024        0.00 (   0.00%)        0.00 (   0.00%)

dbench4 Throughput (misleading but traditional)
                               dbench                 dbench
                              default                    pan
Hmean     1         505.12 (   0.00%)      492.96 *  -2.41%*
Hmean     2         824.14 (   0.00%)      824.06 *  -0.01%*
Hmean     4        1174.61 (   0.00%)     1207.86 *   2.83%*
Hmean     8        1665.10 (   0.00%)     1667.27 *   0.13%*
Hmean     16       2215.59 (   0.00%)     2160.93 *  -2.47%*
Hmean     32       2727.05 (   0.00%)     2633.26 *  -3.44%*
Hmean     64       3128.64 (   0.00%)     3098.73 *  -0.96%*
Hmean     128      3282.89 (   0.00%)     3340.26 *   1.75%*
Hmean     1024     2551.02 (   0.00%)     2559.41 *   0.33%*
BHmean-50 1         509.87 (   0.00%)      495.10 (  -2.90%)
BHmean-50 2         829.35 (   0.00%)      828.14 (  -0.15%)
BHmean-50 4        1182.38 (   0.00%)     1219.30 (   3.12%)
BHmean-50 8        1678.49 (   0.00%)     1678.83 (   0.02%)
BHmean-50 16       2251.01 (   0.00%)     2194.52 (  -2.51%)
BHmean-50 32       2751.39 (   0.00%)     2678.45 (  -2.65%)
BHmean-50 64       3189.69 (   0.00%)     3154.45 (  -1.10%)
BHmean-50 128      3396.18 (   0.00%)     3451.59 (   1.63%)
BHmean-50 1024     2836.80 (   0.00%)     2836.84 (   0.00%)
BHmean-95 1         506.13 (   0.00%)      493.24 (  -2.55%)
BHmean-95 2         824.84 (   0.00%)      824.30 (  -0.06%)
BHmean-95 4        1175.91 (   0.00%)     1208.57 (   2.78%)
BHmean-95 8        1666.46 (   0.00%)     1668.22 (   0.11%)
BHmean-95 16       2219.59 (   0.00%)     2163.86 (  -2.51%)
BHmean-95 32       2731.26 (   0.00%)     2640.34 (  -3.33%)
BHmean-95 64       3144.73 (   0.00%)     3108.59 (  -1.15%)
BHmean-95 128      3306.51 (   0.00%)     3363.33 (   1.72%)
BHmean-95 1024     2658.37 (   0.00%)     2668.88 (   0.40%)
BHmean-99 1         505.37 (   0.00%)      493.08 (  -2.43%)
BHmean-99 2         824.31 (   0.00%)      824.12 (  -0.02%)
BHmean-99 4        1174.94 (   0.00%)     1208.02 (   2.81%)
BHmean-99 8        1665.40 (   0.00%)     1667.48 (   0.12%)
BHmean-99 16       2216.51 (   0.00%)     2161.60 (  -2.48%)
BHmean-99 32       2728.09 (   0.00%)     2635.09 (  -3.41%)
BHmean-99 64       3135.81 (   0.00%)     3102.12 (  -1.07%)
BHmean-99 128      3291.11 (   0.00%)     3349.16 (   1.76%)
BHmean-99 1024     2645.54 (   0.00%)     2655.67 (   0.38%)

                      dbench      dbench
                     default         pan
Duration User         822.55      827.85
Duration System      8384.99     8164.83
Duration Elapsed     1671.36     1670.74

                                         dbench         dbench
                                        default            pan
Ops NUMA alloc hit                 183324626.00   182350114.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local               183324508.00   182350004.00
Ops NUMA base-page range updates      181531.00      515929.00
Ops NUMA PTE updates                  181531.00      515929.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                  162742.00      510979.00
Ops NUMA hint local faults %          120309.00      426848.00
Ops NUMA hint local percent               73.93          83.54
Ops NUMA pages migrated                37605.00       59519.00
Ops AutoNUMA cost                        815.70        2559.64

Netperf-RR
----------
netperf-udp-rr
                           netperf                netperf
                        rr-default                 rr-pan
Min       1   104915.69 (   0.00%)   104505.71 (  -0.39%)
Hmean     1   105865.46 (   0.00%)   105899.22 *   0.03%*
Stddev    1      528.45 (   0.00%)      881.92 ( -66.89%)
CoeffVar  1        0.50 (   0.00%)        0.83 ( -66.83%)
Max       1   106410.28 (   0.00%)   107196.52 (   0.74%)
BHmean-50 1   106232.53 (   0.00%)   106568.26 (   0.32%)
BHmean-95 1   105972.05 (   0.00%)   106056.35 (   0.08%)
BHmean-99 1   105972.05 (   0.00%)   106056.35 (   0.08%)

                     netperf     netperf
                  rr-default      rr-pan
Duration User          11.20       10.74
Duration System       202.40      201.32
Duration Elapsed      303.09      303.08

                                        netperf        netperf
                                     rr-default         rr-pan
Ops NUMA alloc hit                    183999.00      183853.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local                  183999.00      183853.00
Ops NUMA base-page range updates           0.00       24370.00
Ops NUMA PTE updates                       0.00       24370.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                     539.00       24470.00
Ops NUMA hint local faults %             539.00       24447.00
Ops NUMA hint local percent              100.00          99.91
Ops NUMA pages migrated                    0.00          23.00
Ops AutoNUMA cost                          2.69         122.52

netperf-tcp-rr
                           netperf                netperf
                        rr-default                 rr-pan
Min       1    96156.03 (   0.00%)    96556.87 (   0.42%)
Hmean     1    96627.24 (   0.00%)    97551.38 *   0.96%*
Stddev    1      284.71 (   0.00%)      637.74 (-123.99%)
CoeffVar  1        0.29 (   0.00%)        0.65 (-121.87%)
Max       1    96974.45 (   0.00%)    98554.94 (   1.63%)
BHmean-50 1    96840.81 (   0.00%)    98067.19 (   1.27%)
BHmean-95 1    96679.89 (   0.00%)    97663.14 (   1.02%)
BHmean-99 1    96679.89 (   0.00%)    97663.14 (   1.02%)

                     netperf     netperf
                  rr-default      rr-pan
Duration User          10.21       10.26
Duration System       207.90      208.28
Duration Elapsed      302.99      303.02

                                        netperf        netperf
                                     rr-default         rr-pan
Ops NUMA alloc hit                    183669.00      183695.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local                  183657.00      183695.00
Ops NUMA base-page range updates        3949.00       38561.00
Ops NUMA PTE updates                    3949.00       38561.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                    4186.00       43328.00
Ops NUMA hint local faults %            4100.00       43195.00
Ops NUMA hint local percent               97.95          99.69
Ops NUMA pages migrated                    9.00          73.00
Ops AutoNUMA cost                         20.96         216.91

Autonumabench
-------------
autonumabench
                                           autonumabench          autonumabench
                                                 default                    pan
Amean     syst-NUMA01                11664.40 (   0.00%)    11616.17 *   0.41%*
Amean     syst-NUMA01_THREADLOCAL        0.24 (   0.00%)        0.22 *   7.78%*
Amean     syst-NUMA02                    1.55 (   0.00%)        9.31 *-499.26%*
Amean     syst-NUMA02_SMT                1.14 (   0.00%)        4.04 *-254.39%*
Amean     elsp-NUMA01                  223.52 (   0.00%)      221.43 *   0.93%*
Amean     elsp-NUMA01_THREADLOCAL        0.95 (   0.00%)        0.94 *   0.76%*
Amean     elsp-NUMA02                    6.83 (   0.00%)        5.74 *  15.90%*
Amean     elsp-NUMA02_SMT                6.65 (   0.00%)        6.25 *   5.97%*
BAmean-50 syst-NUMA01                11455.44 (   0.00%)    10985.76 (   4.10%)
BAmean-50 syst-NUMA01_THREADLOCAL        0.22 (   0.00%)        0.21 (   7.46%)
BAmean-50 syst-NUMA02                    1.11 (   0.00%)        8.91 (-703.00%)
BAmean-50 syst-NUMA02_SMT                0.94 (   0.00%)        3.42 (-262.19%)
BAmean-50 elsp-NUMA01                  217.38 (   0.00%)      214.03 (   1.54%)
BAmean-50 elsp-NUMA01_THREADLOCAL        0.94 (   0.00%)        0.94 (   0.35%)
BAmean-50 elsp-NUMA02                    6.66 (   0.00%)        5.45 (  18.08%)
BAmean-50 elsp-NUMA02_SMT                6.50 (   0.00%)        6.09 (   6.31%)
BAmean-95 syst-NUMA01                11611.74 (   0.00%)    11448.30 (   1.41%)
BAmean-95 syst-NUMA01_THREADLOCAL        0.23 (   0.00%)        0.22 (   7.14%)
BAmean-95 syst-NUMA02                    1.27 (   0.00%)        9.21 (-624.93%)
BAmean-95 syst-NUMA02_SMT                0.97 (   0.00%)        3.90 (-300.34%)
BAmean-95 elsp-NUMA01                  221.75 (   0.00%)      218.53 (   1.45%)
BAmean-95 elsp-NUMA01_THREADLOCAL        0.94 (   0.00%)        0.94 (   0.53%)
BAmean-95 elsp-NUMA02                    6.75 (   0.00%)        5.68 (  15.81%)
BAmean-95 elsp-NUMA02_SMT                6.61 (   0.00%)        6.23 (   5.82%)
BAmean-99 syst-NUMA01                11611.74 (   0.00%)    11448.30 (   1.41%)
BAmean-99 syst-NUMA01_THREADLOCAL        0.23 (   0.00%)        0.22 (   7.14%)
BAmean-99 syst-NUMA02                    1.27 (   0.00%)        9.21 (-624.93%)
BAmean-99 syst-NUMA02_SMT                0.97 (   0.00%)        3.90 (-300.34%)
BAmean-99 elsp-NUMA01                  221.75 (   0.00%)      218.53 (   1.45%)
BAmean-99 elsp-NUMA01_THREADLOCAL        0.94 (   0.00%)        0.94 (   0.53%)
BAmean-99 elsp-NUMA02                    6.75 (   0.00%)        5.68 (  15.81%)
BAmean-99 elsp-NUMA02_SMT                6.61 (   0.00%)        6.23 (   5.82%)

                autonumabenchautonumabench
                     default         pan
Duration User       94363.43    94436.71
Duration System     81671.72    81408.53
Duration Elapsed     1676.81     1647.99

                                  autonumabench  autonumabench
                                        default            pan
Ops NUMA alloc hit                 539544115.00   539522029.00
Ops NUMA alloc miss                        0.00           0.00
Ops NUMA interleave hit                    0.00           0.00
Ops NUMA alloc local               279025768.00   281735736.00
Ops NUMA base-page range updates    69695169.00    84767502.00
Ops NUMA PTE updates                69695169.00    84767502.00
Ops NUMA PMD updates                       0.00           0.00
Ops NUMA hint faults                69691818.00    87895044.00
Ops NUMA hint local faults %        56565519.00    65819747.00
Ops NUMA hint local percent               81.17          74.88
Ops NUMA pages migrated              5950362.00     8310169.00
Ops AutoNUMA cost                     349060.01      440226.49

Here is a short description of different benchmarks used:

NAS Parallel benchmarks
-----------------------
Using OpenMP version from https://www.nas.nasa.gov/software/npb.html
Variations tried: BT, CG, MT and FT
Memory Footprint:
BT - 170GB
CG - 196GB
MG - 211GB
FT - 81GB
Results: Operations per second.

Hashjoin
--------
Benchmark from IISc that does SQL join between two tables.
Used the version from https://github.com/mitosis-project/mitosis-asplos20-artifact
It performs a fixed number of join operations
Memory Footprint: 80GB
Results: Time taken

Graph500
--------
Benchmark from https://graph500.org/ that does bread first search
on a graph. The score is reported as TEPS (Traversed Edges Per Second).
It comprises of both search and validation phases, but only the
score phase is contributing to the final score here.
Memory Footprint: 70GB
Results: TEPS

PageRank
--------
This is part of GAP Benchmark Suite from https://github.com/sbeamer/gapbs
It performs basic search operations.
Memory Footprint: 91GB
Results: Time taken

Disha Talreja (3):
  sched/numa: Process based autonuma scan period framework
  sched/numa: Add cumulative history of per-process fault stats
  sched/numa: Add adaptive scan period calculation

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 include/linux/mm_types.h |  14 ++
 kernel/sched/debug.c     |   2 +
 kernel/sched/fair.c      | 344 ++++++++++++++++++++++++++++++++++++++-
 kernel/sched/sched.h     |   2 +
 4 files changed, 358 insertions(+), 4 deletions(-)

diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 500e53679..9824b1b36 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -665,6 +665,20 @@ struct mm_struct {
 
 		/* numa_scan_seq prevents two threads remapping PTEs. */
 		int numa_scan_seq;
+
+		/* Process-based Adaptive NUMA */
+		atomic_long_t faults_locality[2];
+		atomic_long_t faults_shared[2];
+		unsigned long faults_locality_history[2];
+		unsigned long faults_shared_history[2];
+
+		spinlock_t pan_numa_lock;
+		unsigned int numa_scan_period;
+		int remote_fault_rates[2]; /* histogram of remote fault rate */
+		long scanned_pages;
+		bool trend;
+		int slope;
+		u8 hist_trend;
 #endif
 		/*
 		 * An operation with batched TLB flushing is going on. Anything
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 1637b65ba..a8377d0e5 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -334,6 +334,8 @@ static __init int sched_init_debug(void)
 	debugfs_create_u32("scan_period_max_ms", 0644, numa, &sysctl_numa_balancing_scan_period_max);
 	debugfs_create_u32("scan_size_mb", 0644, numa, &sysctl_numa_balancing_scan_size);
 	debugfs_create_u32("hot_threshold_ms", 0644, numa, &sysctl_numa_balancing_hot_threshold);
+	debugfs_create_u32("pan_scan_period_min", 0644, numa, &sysctl_pan_scan_period_min);
+	debugfs_create_u32("pan_scan_period_max", 0644, numa, &sysctl_pan_scan_period_max);
 #endif
 
 	debugfs_create_file("debug", 0444, debugfs_sched, NULL, &sched_debug_fops);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 89871548c..3b95a93e1 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1097,6 +1097,10 @@ unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;
 /* Restrict the NUMA promotion throughput (MB/s) for each target node. */
 unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;
 
+/* Clips of max and min scanning periods */
+unsigned int sysctl_pan_scan_period_min = 50;
+unsigned int sysctl_pan_scan_period_max = 5000;
+
 struct numa_group {
 	refcount_t refcount;
 
@@ -2330,6 +2334,301 @@ static void numa_group_count_active_nodes(struct numa_group *numa_group)
 	numa_group->active_nodes = active_nodes;
 }
 
+/**********************************************/
+/*  Process-based Adaptive NUMA (PAN) Design  */
+/**********************************************/
+#define SLOPE(N, D) ((N)/(D))
+
+static unsigned int pan_scan_max(struct task_struct *p)
+{
+	unsigned long smax, nr_scan_pages;
+	unsigned long rss = 0;
+
+	smax = sysctl_pan_scan_period_max;
+	nr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);
+
+	rss = get_mm_rss(p->mm);
+	if (!rss)
+		rss = nr_scan_pages;
+
+	if (READ_ONCE(p->mm->numa_scan_seq) == 0) {
+		smax = p->mm->scanned_pages * sysctl_pan_scan_period_max;
+		smax = smax / rss;
+		smax = max_t(unsigned long, sysctl_pan_scan_period_min, smax);
+	}
+
+	return smax;
+}
+
+/*
+ * Process-based Adaptive NUMA scan period update alogirthm
+ *
+ * These are the important concepts behind the scan period update:
+ *
+ * - increase trend (of scan period)
+ *   scan period => up, memory coverage => down, overhead => down,
+ *   accuracy => down
+ * - decrease trend
+ *   scan period => down, memory coverage => up, overhead => up,
+ *   accuracy => up
+ * - trend: Reflects the current active trend
+ *   1 means increasing trend, 0 means decreasing trend
+ * - slope
+ *   it controls scan_period: new_scan_period = current_scan_period *
+ *                                              100 / slope
+ * - hist_trend: Reflects the intended trend in the last two
+ *   windows. Uses the last two bits (bit0 and bit1) for the same.
+ *   1 if increasing trend was intended, 0 if decreasing was intended.
+ */
+
+/*
+ * Check if the scan period needs updation when the remote fault
+ * rate has changed (delta > 5)
+ *
+ * Returns TRUE if scan period needs updation, else FALSE.
+ */
+static bool pan_changed_rate_update(struct mm_struct *mm, int ps_ratio,
+				    int oldest_remote_fault_rate,
+				    int fault_rate_diff)
+{
+	u8 value;
+
+	/*
+	 * Set the intended trend for the current window.
+	 * - If the remote fault rate has decreased, set the
+	 *   intended trend to increasing.
+	 * - Otherwise leave the intended trend as decreasing.
+	 */
+	mm->hist_trend = mm->hist_trend << 1;
+	if (fault_rate_diff < 5)
+		mm->hist_trend |= 0x01;
+
+	value = mm->hist_trend & 0x03;
+
+	if (fault_rate_diff < -5 && value == 3) {
+		/*
+		 * The remote fault rate has decreased and the intended
+		 * trend was set to increasing in the previous window.
+		 *
+		 * If on decreasing trend, reverse the trend and change
+		 * the slope using the fault rates from (current-1)
+		 * and (current-2) windows.
+		 *
+		 * If already on increasing trend, change the slope using
+		 * the fault rates from (current) and (current-1) windows.
+		 */
+		if (!mm->trend) {
+			mm->trend = true;
+			mm->slope = SLOPE(mm->remote_fault_rates[0] * 100,
+					  oldest_remote_fault_rate);
+		} else {
+			mm->slope = SLOPE(mm->remote_fault_rates[1] * 100,
+					  mm->remote_fault_rates[0]);
+		}
+	} else if (fault_rate_diff > 5 && value == 0) {
+		/*
+		 * The remote fault rate has increased and the intended
+		 * trend was set to decreasing in the previous window.
+		 *
+		 * If on increasing trend,
+		 *  - If shared fault ratio is more than 30%, don't yet
+		 *  reverse the trend, just mark the intended trend as
+		 *  increasing.
+		 *  - Otherwise reverse the trend. Change the slope using
+		 *  the fault rates from (current-1) and (current-2) windows.
+		 *
+		 *  If on decreasing trend
+		 *  - Continue with a changed slope using the fault
+		 *  rates from (current) and (current-1) windows.
+		 */
+		if (mm->trend) {
+			if (ps_ratio < 7) {
+				mm->hist_trend |= 0x01;
+				return true;
+			}
+
+			mm->trend = false;
+			mm->slope = SLOPE(mm->remote_fault_rates[0] * 100,
+					  oldest_remote_fault_rate);
+		} else {
+			mm->slope = SLOPE(mm->remote_fault_rates[1] * 100,
+					  mm->remote_fault_rates[0]);
+		}
+	} else if (value == 1 || value == 2) {
+		/*
+		 * The intended trend is oscillating
+		 *
+		 * If on decreasing trend and the shared fault ratio
+		 * is more than 30%, reverse the trend and change the slope.
+		 *
+		 * If on increasing trend, continue as is.
+		 */
+		if (!mm->trend && ps_ratio < 7) {
+			mm->hist_trend |= 0x01;
+			mm->trend = true;
+			mm->slope = SLOPE(100 * 100,
+					  100 + ((7 - ps_ratio) * 10));
+		}
+		return false;
+	}
+	return true;
+}
+
+/*
+ * Check if the scan period needs updation when the remote fault
+ * rate has remained more or less the same (delta <= 5)
+ *
+ * Returns TRUE if scan period needs updation, else FALSE.
+ */
+static bool pan_const_rate_update(struct mm_struct *mm, int ps_ratio,
+				  int oldest_remote_fault_rate)
+{
+	int diff1, diff2;
+
+	mm->hist_trend = mm->hist_trend << 1;
+
+	/*
+	 * If we are in the increasing trend, don't change anything
+	 * except the intended trend for this window that was reset
+	 * to decreasing by default.
+	 */
+	if (mm->trend)
+		return false;
+
+	/* We are in the decreasing trend, reverse under some condidtions. */
+	diff1 = oldest_remote_fault_rate - mm->remote_fault_rates[0];
+	diff2 = mm->remote_fault_rates[0] - mm->remote_fault_rates[1];
+
+	if (ps_ratio < 7) {
+		/*
+		 * More than 30% of the pages are shared, so no point in
+		 * further reducing the scan period. If increasing trend
+		 * was intended in the previous window also, then reverse
+		 * the trend to increasing. Else just record the increasing
+		 * intended trend for this window and return.
+		 */
+		mm->hist_trend |= 0x01;
+		if ((mm->hist_trend & 0x03) == 3) {
+			mm->trend = true;
+			mm->slope = SLOPE(100 * 100,
+					  (100 + ((7 - ps_ratio) * 10)));
+		} else
+			return false;
+	} else if (diff1 >= 0 && diff2 >= 0 && mm->numa_scan_seq > 1) {
+		/*
+		 * Remote fault rate has reduced successively in the last
+		 * two windows and address space has been scanned at least
+		 * once. If increasing trend was intended in the previous
+		 * window also, then reverse the trend to increasing. Else
+		 * just record the increasing trend for this window and return.
+		 */
+		mm->hist_trend |= 0x01;
+		if ((mm->hist_trend & 0x03) == 3) {
+			mm->trend = true;
+			mm->slope = SLOPE(100 * 100, 110);
+			mm->hist_trend |= 0x03;
+		} else
+			return false;
+	}
+	return true;
+}
+
+static void pan_calculate_scan_period(struct task_struct *p)
+{
+	int remote_fault_rate, oldest_remote_fault_rate, ps_ratio, i, diff;
+	struct mm_struct *mm = p->mm;
+	unsigned long remote_hist = mm->faults_locality_history[0];
+	unsigned long local_hist = mm->faults_locality_history[1];
+	unsigned long shared_hist = mm->faults_shared_history[0];
+	unsigned long priv_hist = mm->faults_shared_history[1];
+	bool need_update;
+
+	ps_ratio = (priv_hist * 10) / (priv_hist + shared_hist + 1);
+	remote_fault_rate = (remote_hist * 100) / (local_hist + remote_hist + 1);
+
+	/* Keep the remote fault ratio at least 1% */
+	remote_fault_rate = max(remote_fault_rate, 1);
+	for (i = 0; i < 2; i++)
+		if (mm->remote_fault_rates[i] == 0)
+			mm->remote_fault_rates[i] = 1;
+
+	/* Shift right in mm->remote_fault_rates[] to keep track of history */
+	oldest_remote_fault_rate = mm->remote_fault_rates[0];
+	mm->remote_fault_rates[0] = mm->remote_fault_rates[1];
+	mm->remote_fault_rates[1] = remote_fault_rate;
+	diff = remote_fault_rate - oldest_remote_fault_rate;
+
+	if (abs(diff) <= 5)
+		need_update = pan_const_rate_update(mm, ps_ratio,
+						    oldest_remote_fault_rate);
+	else
+		need_update = pan_changed_rate_update(mm, ps_ratio,
+						      oldest_remote_fault_rate,
+						      diff);
+
+	if (need_update) {
+		if (mm->slope == 0)
+			mm->slope = 100;
+		mm->numa_scan_period = (100 * mm->numa_scan_period) / mm->slope;
+	}
+}
+
+/*
+ * Update the cumulative history of local/remote and private/shared
+ * statistics. If the numbers are too small worthy of updating,
+ * return FALSE, otherwise return TRUE.
+ */
+static bool pan_update_history(struct task_struct *p)
+{
+	unsigned long local, remote, shared, private;
+	long diff;
+	int i;
+
+	remote = atomic_long_read(&p->mm->faults_locality[0]);
+	local = atomic_long_read(&p->mm->faults_locality[1]);
+	shared = atomic_long_read(&p->mm->faults_shared[0]);
+	private = atomic_long_read(&p->mm->faults_shared[1]);
+
+	/* skip if the activities in this window are too small */
+	if (local + remote < 100)
+		return false;
+
+	/* decay over the time window by 1/4 */
+	diff = local - (long)(p->mm->faults_locality_history[1] / 4);
+	p->mm->faults_locality_history[1] += diff;
+	diff = remote - (long)(p->mm->faults_locality_history[0] / 4);
+	p->mm->faults_locality_history[0] += diff;
+
+	/* decay over the time window by 1/2 */
+	diff = shared - (long)(p->mm->faults_shared_history[0] / 2);
+	p->mm->faults_shared_history[0] += diff;
+	diff = private - (long)(p->mm->faults_shared_history[1] / 2);
+	p->mm->faults_shared_history[1] += diff;
+
+	/* clear the statistics for the next window */
+	for (i = 0; i < 2; i++) {
+		atomic_long_set(&(p->mm->faults_locality[i]), 0);
+		atomic_long_set(&(p->mm->faults_shared[i]), 0);
+	}
+
+	return true;
+}
+
+/*
+ * Updates mm->numa_scan_period under mm->pan_numa_lock.
+ */
+static unsigned long pan_get_scan_period(struct task_struct *p)
+{
+	if (pan_update_history(p))
+		pan_calculate_scan_period(p);
+
+	p->mm->numa_scan_period = clamp(p->mm->numa_scan_period,
+					READ_ONCE(sysctl_pan_scan_period_min),
+					pan_scan_max(p));
+
+	return p->mm->numa_scan_period;
+}
+
 /*
  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
  * increments. The more local the fault statistics are, the higher the scan
@@ -2871,6 +3170,9 @@ void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)
 			task_numa_group(p, last_cpupid, flags, &priv);
 	}
 
+	atomic_long_add(pages, &(p->mm->faults_locality[local]));
+	atomic_long_add(pages, &(p->mm->faults_shared[priv]));
+
 	/*
 	 * If a workload spans multiple NUMA nodes, a shared fault that
 	 * occurs wholly within the set of nodes that the workload is
@@ -2958,12 +3260,20 @@ static void task_numa_work(struct callback_head *work)
 	if (time_before(now, migrate))
 		return;
 
-	if (p->numa_scan_period == 0) {
+	if (p->mm->numa_scan_period == 0) {
+		p->numa_scan_period_max = task_scan_max(p);
+		p->numa_scan_period = task_scan_start(p);
+		mm->numa_scan_period = p->numa_scan_period;
+	} else if (p->numa_scan_period == 0) {
 		p->numa_scan_period_max = task_scan_max(p);
 		p->numa_scan_period = task_scan_start(p);
 	}
 
-	next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	if (!spin_trylock(&p->mm->pan_numa_lock))
+		return;
+	next_scan = now + msecs_to_jiffies(pan_get_scan_period(p));
+	spin_unlock(&p->mm->pan_numa_lock);
+
 	if (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)
 		return;
 
@@ -3052,6 +3362,7 @@ static void task_numa_work(struct callback_head *work)
 		mm->numa_scan_offset = start;
 	else
 		reset_ptenuma_scan(p);
+	mm->scanned_pages += ((sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT)) - pages);
 	mmap_read_unlock(mm);
 
 	/*
@@ -3066,6 +3377,26 @@ static void task_numa_work(struct callback_head *work)
 	}
 }
 
+/* Init Process-based Adaptive NUMA */
+static void pan_init_numa(struct task_struct *p)
+{
+	struct mm_struct *mm = p->mm;
+	int i;
+
+	spin_lock_init(&mm->pan_numa_lock);
+	mm->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	mm->scanned_pages = 0;
+	mm->trend = false;
+	mm->hist_trend = 0;
+	mm->slope = 100;
+
+	for (i = 0; i < 2; i++) {
+		mm->faults_locality_history[i] = 0;
+		mm->faults_shared_history[i] = 0;
+		mm->remote_fault_rates[i] = 1;
+	}
+}
+
 void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 {
 	int mm_users = 0;
@@ -3076,6 +3407,7 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 		if (mm_users == 1) {
 			mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
 			mm->numa_scan_seq = 0;
+			pan_init_numa(p);
 		}
 	}
 	p->node_stamp			= 0;
@@ -3127,6 +3459,9 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)
 		return;
 
+	if (!spin_trylock(&curr->mm->pan_numa_lock))
+		return;
+
 	/*
 	 * Using runtime rather than walltime has the dual advantage that
 	 * we (mostly) drive the selection from busy threads and that the
@@ -3134,16 +3469,17 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	 * NUMA placement.
 	 */
 	now = curr->se.sum_exec_runtime;
-	period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	period = (u64)curr->mm->numa_scan_period * NSEC_PER_MSEC;
 
 	if (now > curr->node_stamp + period) {
 		if (!curr->node_stamp)
-			curr->numa_scan_period = task_scan_start(curr);
+			curr->mm->numa_scan_period = task_scan_start(curr);
 		curr->node_stamp += period;
 
 		if (!time_before(jiffies, curr->mm->numa_next_scan))
 			task_work_add(curr, work, TWA_RESUME);
 	}
+	spin_unlock(&curr->mm->pan_numa_lock);
 }
 
 static void update_scan_period(struct task_struct *p, int new_cpu)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0fb95c98b..2c89aaa92 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2469,6 +2469,8 @@ extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
 extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_hot_threshold;
+extern unsigned int sysctl_pan_scan_period_min;
+extern unsigned int sysctl_pan_scan_period_max;
 #endif
 
 #ifdef CONFIG_SCHED_HRTICK
-- 
2.39.0.rc2.1.gbd5df96b79

