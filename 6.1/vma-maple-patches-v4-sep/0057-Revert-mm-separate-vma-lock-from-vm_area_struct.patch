From 0a6ab1ed8bd17b3f21ff4dc930884c3ce6fc5e7e Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Fri, 30 Dec 2022 13:29:59 +0100
Subject: [PATCH 57/59] Revert "mm: separate vma->lock from vm_area_struct"

This reverts commit 0b13f9486268d9d4b66d79669d7d337518e08d92.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/mm.h       | 27 +++++++------
 include/linux/mm_types.h |  2 +-
 kernel/fork.c            | 82 ++++++++++++----------------------------
 3 files changed, 40 insertions(+), 71 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 494fed736..bc1fe62c0 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -614,6 +614,11 @@ struct vm_operations_struct {
 };
 
 #ifdef CONFIG_PER_VMA_LOCK
+static inline void vma_init_lock(struct vm_area_struct *vma)
+{
+	init_rwsem(&vma->lock);
+	vma->vm_lock_seq = -1;
+}
 
 static inline void vma_write_lock(struct vm_area_struct *vma)
 {
@@ -629,9 +634,9 @@ static inline void vma_write_lock(struct vm_area_struct *vma)
 	if (vma->vm_lock_seq == mm_lock_seq)
 		return;
 
-	down_write(vma->lock);
+	down_write(&vma->lock);
 	vma->vm_lock_seq = mm_lock_seq;
-	up_write(vma->lock);
+	up_write(&vma->lock);
 }
 
 /*
@@ -645,7 +650,7 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 	if (vma->vm_lock_seq == READ_ONCE(vma->vm_mm->mm_lock_seq))
 		return false;
 
-	if (unlikely(down_read_trylock(vma->lock) == 0))
+	if (unlikely(down_read_trylock(&vma->lock) == 0))
 		return false;
 
 	/*
@@ -655,7 +660,7 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 	 * modification invalidates all existing locks.
 	 */
 	if (unlikely(vma->vm_lock_seq == READ_ONCE(vma->vm_mm->mm_lock_seq))) {
-		up_read(vma->lock);
+		up_read(&vma->lock);
 		return false;
 	}
 	return true;
@@ -663,13 +668,13 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 
 static inline void vma_read_unlock(struct vm_area_struct *vma)
 {
-	up_read(vma->lock);
+	up_read(&vma->lock);
 }
 
 static inline void vma_assert_locked(struct vm_area_struct *vma)
 {
-	lockdep_assert_held(vma->lock);
-	VM_BUG_ON_VMA(!rwsem_is_locked(vma->lock), vma);
+	lockdep_assert_held(&vma->lock);
+	VM_BUG_ON_VMA(!rwsem_is_locked(&vma->lock), vma);
 }
 
 static inline void vma_assert_write_locked(struct vm_area_struct *vma)
@@ -684,7 +689,7 @@ static inline void vma_assert_write_locked(struct vm_area_struct *vma)
 
 static inline void vma_assert_no_reader(struct vm_area_struct *vma)
 {
-	VM_BUG_ON_VMA(rwsem_is_locked(vma->lock) &&
+	VM_BUG_ON_VMA(rwsem_is_locked(&vma->lock) &&
 		      vma->vm_lock_seq != READ_ONCE(vma->vm_mm->mm_lock_seq),
 		      vma);
 }
@@ -694,6 +699,7 @@ struct vm_area_struct *lock_vma_under_rcu(struct mm_struct *mm,
 
 #else /* CONFIG_PER_VMA_LOCK */
 
+static inline void vma_init_lock(struct vm_area_struct *vma) {}
 static inline void vma_write_lock(struct vm_area_struct *vma) {}
 static inline bool vma_read_trylock(struct vm_area_struct *vma)
 		{ return false; }
@@ -704,10 +710,6 @@ static inline void vma_assert_no_reader(struct vm_area_struct *vma) {}
 
 #endif /* CONFIG_PER_VMA_LOCK */
 
-/*
- * WARNING: vma_init does not initialize vma->lock.
- * Use vm_area_alloc()/vm_area_free() if vma needs locking.
- */
 static inline void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)
 {
 	static const struct vm_operations_struct dummy_vm_ops = {};
@@ -716,6 +718,7 @@ static inline void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)
 	vma->vm_mm = mm;
 	vma->vm_ops = &dummy_vm_ops;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
+	vma_init_lock(vma);
 }
 
 /* Use when VMA is not part of the VMA tree and needs no locking */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 4119489ef..bda8fa1e8 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -472,7 +472,7 @@ struct vm_area_struct {
 
 #ifdef CONFIG_PER_VMA_LOCK
 	int vm_lock_seq;
-	struct rw_semaphore *lock;
+	struct rw_semaphore lock;
 #endif
 
 	/*
diff --git a/kernel/fork.c b/kernel/fork.c
index 45f20db2b..99029c161 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -451,28 +451,40 @@ static struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
-#ifdef CONFIG_PER_VMA_LOCK
+struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
+{
+	struct vm_area_struct *vma;
 
-/* SLAB cache for vm_area_struct.lock */
-static struct kmem_cache *vma_lock_cachep;
+	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	if (vma)
+		vma_init(vma, mm);
+	return vma;
+}
 
-static bool vma_init_lock(struct vm_area_struct *vma)
+struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 {
-	vma->lock = kmem_cache_alloc(vma_lock_cachep, GFP_KERNEL);
-	if (!vma->lock)
-		return false;
+	struct vm_area_struct *new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 
-	init_rwsem(vma->lock);
-	vma->vm_lock_seq = -1;
-
-	return true;
+	if (new) {
+		ASSERT_EXCLUSIVE_WRITER(orig->vm_flags);
+		ASSERT_EXCLUSIVE_WRITER(orig->vm_file);
+		/*
+		 * orig->shared.rb may be modified concurrently, but the clone
+		 * will be reinitialized.
+		 */
+		*new = data_race(*orig);
+		INIT_LIST_HEAD(&new->anon_vma_chain);
+		vma_init_lock(new);
+		dup_anon_vma_name(orig, new);
+	}
+	return new;
 }
 
+#ifdef CONFIG_PER_VMA_LOCK
 static inline void __vm_area_free(struct vm_area_struct *vma)
 {
 	/* The vma should either have no lock holders or be write-locked. */
 	vma_assert_no_reader(vma);
-	kmem_cache_free(vma_lock_cachep, vma->lock);
 	kmem_cache_free(vm_area_cachep, vma);
 }
 
@@ -528,7 +540,6 @@ void vm_area_free(struct vm_area_struct *vma)
 
 #else /* CONFIG_PER_VMA_LOCK */
 
-static bool vma_init_lock(struct vm_area_struct *vma) { return true; }
 void drain_free_vmas(struct mm_struct *mm) {}
 
 void vm_area_free(struct vm_area_struct *vma)
@@ -539,48 +550,6 @@ void vm_area_free(struct vm_area_struct *vma)
 
 #endif /* CONFIG_PER_VMA_LOCK */
 
-struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
-{
-	struct vm_area_struct *vma;
-
-	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
-	if (!vma)
-		return NULL;
-
-	vma_init(vma, mm);
-	if (!vma_init_lock(vma)) {
-		kmem_cache_free(vm_area_cachep, vma);
-		return NULL;
-	}
-
-	return vma;
-}
-
-struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
-{
-	struct vm_area_struct *new;
-
-	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
-	if (!new)
-		return NULL;
-
-	ASSERT_EXCLUSIVE_WRITER(orig->vm_flags);
-	ASSERT_EXCLUSIVE_WRITER(orig->vm_file);
-	/*
-	 * orig->shared.rb may be modified concurrently, but the clone
-	 * will be reinitialized.
-	 */
-	*new = data_race(*orig);
-	if (!vma_init_lock(new)) {
-		kmem_cache_free(vm_area_cachep, new);
-		return NULL;
-	}
-	INIT_LIST_HEAD(&new->anon_vma_chain);
-	dup_anon_vma_name(orig, new);
-
-	return new;
-}
-
 static void account_kernel_stack(struct task_struct *tsk, int account)
 {
 	if (IS_ENABLED(CONFIG_VMAP_STACK)) {
@@ -3160,9 +3129,6 @@ void __init proc_caches_init(void)
 			sizeof_field(struct mm_struct, saved_auxv),
 			NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
-#ifdef CONFIG_PER_VMA_LOCK
-	vma_lock_cachep = KMEM_CACHE(rw_semaphore, SLAB_PANIC|SLAB_ACCOUNT);
-#endif
 	mmap_init();
 	nsproxy_cache_init();
 }
-- 
2.39.0.rc2.1.gbd5df96b79

